# 集合框架

## 1. ArrayList

### 1.1 ArrayList底层实现

ArrayList是基于数组实现的，是一个动态数组。它实现了List接口，底层使用数组保存所有元素；其操作基本上是对数组的操作。

1）私有属性：

```java
// ArrayList只定义类的两个私有属性
// elementData存储ArrayList内的元素
private transient Object[] elementData;
// size表示它包含的元素的数量
private int size;
```

2）构造方法：

- 构造一个默认初始容量为10的空列表
- 构造一个指定初始容量的空列表
- 构造一个包含指定collection的元素的列表

```java
// ArrayList带容量大小的构造函数。 
public ArrayList(int initiaCapacity){
    super();
    if(initiaCapacity < 0)
       throw new IllegalArgumentException("Illegal Capacity: "+    
                                               initialCapacity);
    //新建一个数组
    this.elementData = new Object[initialCapacity];
}
//ArrayList无参构造方法，默认容量是10。
public ArrayList(){
    this(10);
}
// 创建一个包含collection的ArrayList    
public ArrayList(Collection<? extends E> c){
    elementData = c.toArray();
    size = elementData.length;
    if (elementData.getClass() != Object[].class)    
            elementData = Arrays.copyOf(elementData, size, Object[].class);  
}
```

3）元素存储

`set(int index, E element)、add(E e)、add(int index, E element)、addAll(Collection<? extends E> c)、addAll(int index, Collection<? extends E> c)`

```java
// 用指定的元素替代此列表中指定位置上的元素，并返回以前位于该位置上的元素。  
public E set(int index, E element){
    RangeCheck(index);
    
    E oldValue = (E)elementData[index];
    elementData[index] = element;
    return oldValue;
}

//将指定的元素添加到此列表的尾部
public boolean add(E e){
    ensureCapacity(size + 1);
    elementData[size++] = e;
    return true;
}

//将指定的元素插入此列表中的指定位置
//如果当前位置有元素，则向右移动当前位于该位置的元素以及所有后续元素
public void add(int index, E element){
    rangeCheckForAdd(index);
    //如果数组长度不足，将进行扩容
    ensureCapacity(size + 1); //自增+1
    System.arraycopy(elementData, index, elementData, index + 1, size - index);
    elementData[index] = element;
}
//按照指定collection的迭代器锁返回的元素顺序，将该collection中的所有元素添加到此列表的尾部
public boolean addAll(Collection<? extends E> c){
    Object[] a = c.toArray();
    int numNew = a.length;
    ensureCapacityInternal(size + numNew);  //自增+1
    System.arraycopy(a, 0, elementData, size, numNew);
    size += numNew;
    return numNew != 0;
}
```

4）元素读取

```java
public E get(int index){
    RangeCheck(index);
    return (E) elementData[index];
}
```

5）元素删除

```java
//两种方式
/*移除指定位置的元素
*/
public E remove(int index){
    RangeCheck(index);
    
    modCount++;
    E oldValue = (E)elementData[index];
    
    int numMoved = size - index - 1;
    if (numMoved > 0)
        System.arraycopy(elementData, index + 1, elementData, index, numMoved);
    elementData[--size] = null;
    
    return oldValue;
}
/*移除列表种首次出现的指定元素（如果存在）
*/
public boolean remove(Object o){
    //由于Array List种允许存放null，因此有两种情况
    // 1.
    if (o == null){
        for (int index = 0; index < size; index++)
            if(elementData[index] == null){
                //类似remove(int index)，移除列表中指定位置的元素
                fastRemove(index);
                return true;
            }
    }
    else{
        for(int index = 0; index < size; index++)
            if(o.equals(elementData[index])){
                fastRemove(index);
                return true;
            }
    }
    return false;
}
```

**fastRemove不会判断边界，因为找到元素就相当于确定了index不会超过边界，而且fastRemove并不返回被移除的元素。**



### 1.2. ArrayList底层是怎么排序的

重写sort方法

```java
@Override
public void sort(Comparator<? super E> c){
    final int expectedCount = modCount;
    Arrays.sort((E[]) elementData, 0 , size, c);
    if(modCount != expectedModCount){
        throw new ConcurrentModificationException();
    }
    modCount++;
}
//使用

arrayList.sort(Comparator.naturalOrder());
arrayList.sort(Comparator.reverseOrder());
```

### 1.3. ArrayList扩容机制

```java
private void grow(int minCapacity){
    //考虑溢出
    int oldCapacity = elementData.length;
    int newCapacity = oldCapacity + (oldCapacity >> 1); //扩大1.5倍
    if(newCapacity - minCapacity < 0)
        newCapacity = minCapacity;
    if(newCapacity - MAX_ARRAY_SIZE > 0)
        newCapacity = hugeCapacity(minCapacity);
    elementData = Arrays.copyOf(elementData, newCapacity);
}
```

## 2. LinkedList

### 2.1LinkedList的底层实现

**Node类**

```java
private static class Node<E>{
    E item;
    Node<E> next;
    Node<E> prev;
    
    Node(Node<E> prev, E element, Node<E> next){
        this.item = element;
        this.next = next;
        this.prev = prev;
    }
}
```

**属性**

```java
//节点数  transient关键字：不参与对象的序列化
transient int size = 0;
//指向第一个节点的引用
transient Node<E> first;
//指向最后一个节点的引用
transient Node<E> last;
```

**构造方法**

```java
//无参构造
public LinkedList(){
}
//集合构造
public LinkedList(Collection<? extends E> c){
    this();
    addAll(c);
}
```

**核心方法**

```java
//返回列表中指定位置的元素
public E get(int index){
    checkElementIndex(index);
    return node(index).item;
}
```

```java
//返回指定元素索引处的节点
Node<E> node(int index){
    //二分法，选择从前向后遍历还是从后向前遍历
    if (index < (size >> 1)){
        Node<E> x = first;
        for(int i = 0; i < index; i++)
            x = x.next;
    }
    else{
        Node<E> x = last;
            for (int i = size - 1; i > index; i--)
                x = x.prev;
            return x;
    }
}
```

```java
//向指定位置添加元素
public void add(int index, E element){
    checkPostionIndex(index);
    
    if(index == size)
        linkLast(element);
    else
        linkBefore(element, node(index));
}

//添加的元素是在末尾
void linkLast(E e){
    final Node<E> l = last;
    final Node<E> newNode = new Node<>(l, e, null);
    last = newNode;
    if(l == null)
        first = newNode;
    else
        l.next = newNode;
    size++;
    modCount++;
}

//正常添加
void linkBefore(E e, Node<E> succ){
    final Node<E> pred = succ.prev;
        final Node<E> newNode = new Node<>(pred, e, succ);
        succ.prev = newNode;
        if (pred == null)
            first = newNode;
        else
            pred.next = newNode;
        size++;
        modCount++;
}
```

```java
//删除元素
public E remove(int index){
    checkElementIndex(index);
    return unlink(node(index));
}

//真正的删除方法
E unlink(Node<E> x){
    final E element = x.item;
        final Node<E> next = x.next;
        final Node<E> prev = x.prev;

        if (prev == null) {
            first = next;
        } else {
            prev.next = next;
            x.prev = null;
        }

        if (next == null) {
            last = prev;
        } else {
            next.prev = prev;
            x.next = null;
        }

        x.item = null;
        size--;
        modCount++;
        return element;
}
```

## 3. HashMap

### 3.1 HashMap底层实现

**关键变量**

```java
//初始容量大小  必须是2的幂
static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; //  = 16
//最大容量
static final int MAXIMUM_CAPACITY = 1 << 30;

//负载因子，因为统计学中hash冲突符合泊松分布，7-8之间冲突最小
static final float DEFAULT_LOAD_FACTOR = 0.75f;

//链表大于这个值就会树化
static final int TREEIFY_THRESHOLD = 8;

//小于这个值就会反树化
static final int UNTREEIFY_THRESHOLD = 6;
```

**构造方法**

- 指定初始容量大小，负载因子

```java
public HashMap(int initialCapacity, float loadFactor){
    if(initialCapacity < 0)
        throw new IllegalArgumentException("Illegal initial capacity: " +
                                               initialCapacity);
    if (initialCapacity > MAXIMUM_CAPACITY) //1<<30 最大容量是 Integer.MAX_VALUE;
            initialCapacity = MAXIMUM_CAPACITY;
        if (loadFactor <= 0 || Float.isNaN(loadFactor))
            throw new IllegalArgumentException("Illegal load factor: " +
                                               loadFactor);
	this.loadFactor = loadFactor;
    //tableSizeFor 可以找到最小的2的幂
    this.threshold = tableSizeFor(initialCapacity);  
}
```

- *负载因子给的默认值0.75*

```java
public HashMap(int initialCapacity) {
        this(initialCapacity, DEFAULT_LOAD_FACTOR);
    }
```

- 空参构造，均使用默认值

```java
public HashMap() {
        this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted
    }
```

- 初始化

```java
public HashMap<Map<? extends K, ? extends V> m){
    this.loadFactor = DEFAULT_LOAD_FACTOR;
    putMapEntries(m, false);
}
```

### 3.2 HashMap计算Hash值

**hash方法**

```java
static final int hash(Object key){
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

`h = key.hashCode()`表示`h`是key对象的hashCode返回值；

`h >>> 16`是h右移16位，因为int是4字节，32位，所以右移16位后变成：左边16个0 + 右边原h的高16位；

**异或**：二进制位运算。如果一样返回 0，不一样则返回 1。

例：两个二进制 110 和 100 进行异或

​        	  110

​          ^ 100

结果     010

**putVal（）中寻址部分**

`tab[i = (n - 1) & hash]`

**为什么不直接用hashCode（）%length?**

**寻址为什么不用取模**

HashMap中规定了哈希表长度为2的幂，而2 ^k - 1转为二进制就是k个连续的1，那么`hash & (k个连续的1)返回的就是hash的低k个位，该计算结果就是0到2^k-1。

为什么不直接用 hashCode() 而是用它的高 16 位进行异或计算新 hash 值？

int 类型占 32 位，可以表示 2^32 种数（范围：-2^31 到 2^31-1），而哈希表长度一般不大，在 HashMap 中哈希表的初始化长度是 16（HashMap 中的 DEFAULT_INITIAL_CAPACITY），如果直接用 hashCode 来寻址，那么相当于只有低 4 位有效，其他高位不会有影响。这样假如几个 hashCode 分别是 210、220、2^30，那么寻址结果 index 就会一样而发生冲突，所以哈希表就不均匀分布了。

为了减少这种冲突，HashMap 中让 hashCode 的高位也参与了寻址计算（进行扰动），即把 hashCode 高 16 位与 hashCode 进行异或算出 hash，然后根据 hash 来做寻址。

### 3.3 HashMap的put操作

![image-20211027210936780](E:\实习\Java_docs\pics\image-20211027210936780.png)

```java
public V put(K key, V value){
    //把key先hash得到hash值
    return putVal(hash(key), key, value, false, true);
}

final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict){
    //数组+链表+红黑树，链表型（Node泛型)数组，每一个元素代表一条链表，则每个元素称为桶
    //HashMap 的每一个元素，都是链表的一个节点（Entry<K,V>）这里也就是Node<K,V>
    //tab:桶 p:桶 n:哈希表数组大小 i:数组下标(桶的位置)
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    //1.判断当前是否为0，空就调用resize()方法（resize中会判断是否进行初始化）
    if((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    //2.判断是否有hash冲突，根据入参key和key的hash值找到具体的桶并且判空，
    if((p = tab[i = (n - 1) & hash]) == null)
		tab[i] = newNode(hash, key, value, null);
    //3.以下表示有冲突，处理hash冲突
    else{
        Node<K,V> e; K k; //均为临时变量
        //4.判断当前桶的key是否与入参key一致，一致则存在，把当前桶p赋值给e,覆盖原 value 在步骤10进行
        if(p.hash == hash && ((k = p.key) == key ||(key != null && key.equals(k))))
            e = p;
        //5.如果当前的桶为红黑树，用putTreeVal方法写入
        else if(p instanceof TreeNode)
            e = ((TreeNode<K,V>) p).putTreeVal(this, tab, hash, key, value);
        //当前的桶是链表 遍历链表
        else{
            for(int binCount = 0; ; ++binCount){
                if((e = p.next) == null){
                    //7.尾插法 链表下一个节点是null
                    p.next = newNode(hash, key, value, null);
                    //8.判断是否大于阈值，是否需要转成红黑树
                    if(binCount >= TreeIFY_THRESHOLD - 1)
                        treeifyBin(tab, hash);
                    break;
                }
                //9. 如果链表中key存在，则直接跳出
                if(e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        //10. 存在相同的key的Node节点，覆盖原来的value
        if(e != null){
            V oldValue = e.value;
            if(!onlyIfAbsent || oldValue == null)
                e.value = value;
            //LinkedHashMap用到的回调方法
                    afterNodeAccess(e);
                    return oldValue;
        }
    }
    ++modCount;
    if(++size > threshold)
        resize();
    //LinkedHashMap用到的回调方法
            afterNodeInsertion(evict);
            return null;
}

/**
1、开始，入参key、value
2、判断当前table是否为空或者length=0？
    是，去扩容，（resize()方法中有判断是否初始化）
    否，根据key算出hash值并得到插入的数组的索引
        判断找到的这个table[i]是否为空？
            是，直接插入，再到步骤
            否，判断key是否存在？
                是，直接覆盖对应的value,再到步骤3
                否，去判断当前这个table[i]是不是treeNode？
                    是，使用红黑树的方式插入key、value
                    否，开始遍历链表准备插入
                        判断链表长度是不是大于8？
                            是，链表转红黑树插入key、value
                            否，以链表的方式插入key、value如果key存在就直接覆盖对应value

3、判断map的size()是否大于阈值？
	    是 就去扩容resize()
4、结束
**/

```

### 3.4 HashMap的get操作

```java
public V get(Object key){
    Node<K,V> e;
    return (e = getNode(hash(key), key)) == null ? null : e.value;
}


final Node<K,V> getNode(int hash, Object key){
    Node<K,V>[] tab; Node<K,V> first, e, int n; K k;
    //1.判断当前数组不为空并且长度大于0 && 由key的hash值找到对应数组下的桶
    if((tab = table) != null &&  (n = table.length) > 0 &&
      (first = tab[(n - 1)& hash]) != null){
        //2.先判断桶的第一个节点 如果key一致 返回
        if(first.hash == hash &&
          ((k = first,key) == key || (key != null && key.equals(k))))
            return first;
        //3.再判空下一个节点不为空 && 判断是红黑树还是链表
        if((e = first.next) != null){
            //4.如果是红黑树 则按红黑树方式取值
            if(first instanceof TreeNode)
                return ((TreeNode<K,V> first).getTreeNode(hash,key));
            //否则就是链表
            do{
                if(e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k))))
                    return e;
            }
            while((e = e.next) != null);
        }
        
    }
    return null;
}
```

```java
/**1、开始，入参key
2、判断当前的数组长度不为空&&length>0
    是 return null;
    否，去判断第一个节点，如果key符合，返回
        再去判断下一个节点是否为空
            是，return null
            否，判断否是红黑树？
                是，按红黑树的方式取值
                否，遍历链表取值

3、结束
**/
```

### 3.5 HashMap的resize（）

```java
final Node<K,V>[] resize(){
    Node<K,V>[] oldtab = table;
    int oldCap = (oldTab == null) ? 0 : oldTab.length;
    int OldThr = threshold;
    int newCap, newThr = 0;
    //1.原数组扩容
    if(oldCap > 0){
        //如果原数组长度大于最大容量，把阈值调最大，return
        if(oldCap >= MAXIMUM_CAPACITY){
            threshold = Integer.MAX_VALUE;
            return oldTab;
        }
        //把原数组大小、阈值都扩大一倍
        else if((newCap = oldCap << 1) < MAXIMUM_CAPACITY && 
               oldCap >= DEFAULT_INITIAL_CAPACITY)
            newThr = oldThr << 1;   // 现阈值 = 原阈值的两倍
    }
    //使用指定initialCapacity的构造方法，则用原阈值作为新容量
    else if(oldThr > 0)
        newCap = oldThr;
    //使用空参构造，用默认值
    else{
        newCap = DEFAULT_INITIAL_CAPACITY; //16
        newThr = (int)(DEFAULT_INITIAL_CAPACITY * DEFAULT_INITIAL_LOAD_FACTOR); //0.75*16 = 12
    }
    //使用指定了initialcapacity的构造方法，新阈值为0，则计算新的阈值
    if(newThr == 0){
        float ft = (float)newCap * loadFactor;
        newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE); 
    }
    threshold = newThr;
    //2.用新的数组容量大小初始化数组
    Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
    //如果仅仅是初始化过程，到此结束
    table = newTab;
    //3.开始扩容的主要工作，数据迁移
    if(oldTab != null){
        //遍历原数组开始复制旧数据
        for(int j = 0; j < oldCap; j++){
            Node<K,V> e;
            if((e = oldTab[j]) != null){
                oldTab[j] = null;
                //原数组中单个元素，直接复制到新表
                if(e.next == null)
                    newTab[e.hash & (newCap - 1)] = e;
                //如果该元素类型是红黑树，则按红黑树处理
                else if(e instanceof TreeNode)
                    ((TreeNode<K,V>)e.split(this, newTab,j, oldCap));
                else{
                    //先定义了两种类型的链表，以及头尾节点，高位链表和低位链表
                    Node<K,V> loHead = null, loTail = null;
                    Node<K,V> hiHead = null, hiTail = null;
                    Node<K,V> next;
                    //按顺序遍历原链表的节点
                    do{
                        next = e.next;
                        //核心判断条件，
                        // = 0 则放到低位链表
                        if(e.hash & oldCap) == 0){
                            if(loTail == null)
                                loHead = e;
                            else
                                loTail.next = e;
                            loTail = e;
                        }
                        else {
                                if (hiTail == null)
                                    hiHead = e;
                                else
                                    hiTail.next = e;
                                hiTail = e;
                            }
                    }
                    while((e = next) != null);
                    //把整个低位链表放到新数组的j位置
                        if (loTail != null) {
                            loTail.next = null;
                            newTab[j] = loHead;
                        }
                        //把整个高位链表放到新数组的j+oldCap位置上
                        if (hiTail != null) {
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead;
                }
            }
        }
    }
        return newTab;
}
```

```java
/**1、开始，拿到原数组
2、对原数组扩容
	  2.1如果原数组中的容量到最大，不再扩容，return原数组
	  2.2把原数组容量大小与阈值都扩大一倍
3、如初始化用的指定initialCapacity的构造方法，则用原阈值作为新容量
4、如初始化时候用的空参构造，用默认容量与默认阈值
5、如初始化用的指定initialCapacity的构造方法，阈值=0，计算新的阈值
6、用新的容量初始化数组，如果是初始化，结束返回新数组
7、开始扩容，做数据迁移
	  7.1遍历原数组copy数据到新数组
		    7.1.1如数组中只有一个元素，则直接复制
		    7.1.2如元素是红黑树数类型，则按红黑树的方式处理
		    7.1.3对原数组的链表进行处理
				      定义一个高位链表、一个低位链表（对原链表拆分）
				      开启一个循环，遍历原链表
					        判断条件e.hash & oldCap == 0?
						          是，把这些链表节点放到低位链表
						          否，放到高位链表
				      循环结束，遍历链表完成
				      把整个低位链表放到新数组j位置
				      把整个高位链表放到新数组j+oldCap位置
	  7.2循环结束，遍历旧数组完成
8、返回新数组
**/
```



### 3.6HashMap扩容时(e.hash & oldCap) == 0推导

[(1条消息) HashMap 在扩容时为什么通过位运算 (e.hash & oldCap) 得到新数组下标_Luke.Du的博客-CSDN博客](https://blog.csdn.net/qq_45369827/article/details/114960370)

1. HashMap计算key所对应数组下标的公式是`(length - 1) & hash`，这个公式等价于`hash % length`(当length是2的n次幂)
2. 如下图，`hash % length`的结果只取决于小于数组长度的部分，这个key的hash值得低四位就是当前所在数组的下标。扩容后 新数组长度 = 旧数组长度 * 2，也就是左移 1 位，而此时 hash % length 的结果只取决于 hash 值的低五位，前后两者之间的**差别就差在了第五位**上。

![image-20211123183145042](E:\实习\Java_docs\pics\image-20211123183145042.png)

3. 如果第五位是 0，那么只要看低四位 (也就是当前下标)；如果第五位是 1，只要把二进制数 1 0 0 0 0 + 1 1 1 0 ，就可以得到新数组下标。
4. 那为什么根据 (e.hash & oldCap) == 0 来做判断条件呢？是因为旧数组的长度 length 的二进制数的第五位刚好是 1 其它位全为 0，而 & 运算相同为 1 不同为 0，==因此 hash & length 的目的就是为了计算 hash 值的第五位是 0 还是 1。==

### 3.7 HashMap为什么要使用红黑树而不使用AVL树

**红黑树**

- 根节点是黑色的
- 每个叶子节点都是黑色的空节点（NIL），叶子节点不存储数据
- 任何相邻的节点都不能同时为红色
- 每个节点，从该节点到其可达叶子节点的所有路径，都包含相同数目的黑色节点

AVL树和红黑树有几点比较和区别：
（1）AVL树是更加严格的平衡，因此可以提供更快的查找速度，一般读取查找密集型任务，适用AVL树。
（2）红黑树更适合于插入修改密集型任务。
（3）通常，AVL树的旋转比红黑树的旋转更加难以平衡和调试。

在CurrentHashMap中是加锁了的，实际上是读写锁，如果写冲突就会等待，如果插入时间过长必然等待时间更长，而红黑树相对AVL树他的插入更快！在AVL树中，从根到任何叶子的最短路径和最长路径之间的差异最多为1。在红黑树中，差异可以是2倍。

在AVL树中查找通常更快，但这是以更多旋转操作导致更慢的插入和删除为代价的，红黑树在添加，删除，查找相对较好。

### 3.8 HashMap 的 tableSizeFor

```java
static final int tableSizeFor(int cap){
    //part1
    int n = cap - 1;
    //part2
    n |= n >>> 1;
    n |= n >>> 2;
    n |= n >>> 4;
    n |= n >>> 8;
    n |= n >>> 16;
    //part3
    return (n < 0) ? 1: (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
}
```

| **操作** | **原始值**           | **结果值** |
| -------- | -------------------- | ---------- |
| n >>> 1  | 1xxxxxxx             | 01xxxxxx   |
| 位或操作 | 1xxxxxxx \| 01xxxxxx | 11xxxxxx   |
| n >>> 2  | 11xxxxxx             | 0011xxxx   |
| 位或操作 | 11xxxxxx \| 0011xxxx | 1111xxxx   |
| n >>> 4  | 1111xxxx             | 00001111   |
| 位或操作 | 1111xxxx \| 00001111 | 11111111   |
| n >>> 8  | 11111111             | 00000000   |
| 位或操作 | 11111111 \| 00000000 | 11111111   |
| n >>> 16 | 11111111             | 00000000   |
| 位或操作 | 11111111 \| 00000000 | 11111111   |

## 4. ConcurrentHashMap

### 4.1 ConcurrentHashMap底层实现

数组 + 链表 + 红黑树

- table： 默认为null，初始化发生在第一次插入操作，默认为16的数组。
- nextTable：默认为null，扩容时新生成的数组，其大小为原数组的两倍。
- sizeCtl： 默认为0，用来控制table的初始化和扩容操作
  - **-1** 代表table正在初始化
  - **-N** 表示有N - 1个线程正在进行扩容操作
- Node：保存key，value及key的hash值

其中value和next都用volatile修饰，保证并发的可见性

```java
static class Node<K,V> implements Map.Entry<K,V>{
    final int hash;
    final K key;
    volatile V val;
    volatile Node<K,V> next;
}
```

- ForwardingNode: 一个特殊的Node节点，hash值为-1，其中存储nextTable的引用。

只有table发生扩容的时候，ForwardingNode才会发挥作用，作为一个占位符放在table中表示当前节点为null或则已经被移动。

```java
final class ForwardingNode<K,V> extends Node<K,V>{
    final Node<K,V>[] nextTable;
    ForwardingNode(Node<K,V>[] tab){
        super(MOVED, null, null, null);
        this.nextTable = tab;
    }
}
```

**实例初始化**

实例化ConcurrentHashMap时倘若声明了table的容量，在初始化时会根据参数调整table大小，==确保table的大小总是2的幂次方==。默认的table大小为16.

table的初始化操作回延缓到第一次put操作再进行，并且初始化只会执行一次。

```java
private final Node<K,V>[] initTable(){
    Node<K,V>[] tab; int sc;
    while((tab = table) == null || tab.length == 0){
         // 别的线程已经初始化好了或者正在初始化 sizeCtl 为 -1
        if((sc = sizeCtl) < 0)
            Thread.yield   // 让出线程的执行权
		// CAS 一下，将 sizeCtl 设置为 -1，代表抢到了锁 基本在这就相当于同步代码块
        else if(U.compareAndSwapInt(this, SIZECTL, sc, -1)){
            try{
                if((tab = table) == null || tab.length == 0){
                    // DEFAULT_CAPACITY 默认初始容量是 16
                    int n = (sc > 0) ? sc : DEFAULT_CAPACTIY;
                    // 初始化数组，长度为16或者初始化时提供的长度
                    Node<K,V>[] nt = (Node<K,V>[]) new Node<?, ?>[n];
                    //将这个数组赋值给table，table时volatile的，它的写发生在别人的读之前
                    table = tab = nt;
                    //如果n为16的化，那么这里sc其实就是0.75 * n
                    sc = n -(n >>> 2);
                }
            }
            finally{
                设置下次扩容时的阈值
                sizeCtl = sc;
            }
            break;
        }
    }
    return tab;
}
```

### 4.2 table扩容

什么时候会触发扩容

- 如果新增节点后，所在的链表的元素个数大于等于8，则会调用treeifyBin把链表转换为红黑树。在转换结构时，若tab的长度小于==MIN_TREEIFY_CAPACITY==，默认值为64，则会将数组长度扩大到原来的两倍，并触发`transfer`,重新调整节点位置。（只有当`tab.length >= 64, ConcurrentHashMap`才会使用红黑树。）

- 新增节点后，`addCount`统计tab中的节点个数大于阈值（sizeCtl），会触发`transfer`，重新调整节点位置。



### 4.3 如何在扩容时，并发地复制与插入？

1. 遍历整个table，当前节点为空，则采用CAS的方式在当前位置放入fwd
2. 当前节点已经为fwd(with hash field “MOVED”)，则已经有有线程处理完了了，直接跳过 ，这里是控制并发扩容的核心
3. 当前节点为链表节点或红黑树，重新计算链表节点的hash值，移动到nextTable相应的位置（构建了一个反序链表和顺序链表，分别放置在i和i+n的位置上）。移动完成后，用`Unsafe.putObjectVolatile`在tab的原位置赋为为fwd, 表示当前节点已经完成扩容。

### 4.4 transfer

1. 如果当前的 `nextTab` 是空，也就是说需要进行扩容的数组还没有初始化，那么初始化一个大小是原来两倍的数组出来，作为扩容后的新数组。 
2. 我们分配几个变量，来把原来的数组拆分成几个完全相同的段，你可以把他们想象成一个个大小相同的短数组，每个短数组的长度是 `stride` 。 
3. 我们先取最后一个短数组，用 `i` 表示一个可变的指针，可指向短数组的任意一个位置，最开始指向的是短数组的结尾。`bound` 表示短数组的下界，也就是开始的位置。也就是我们在短数组选择的时候是采用从后往前进行的。 
4. 然后使用了一个全局的属性 `transferIndex`（线程共享），来记录当前已经选择过的短数组和还没有被选择的短数组之间的分隔。
5. 那么当前的线程选择的这个短数组其实就是当前线程应该进行的数据迁移任务，也就是说当前线程就负责完成这一个小数组的迁移任务就行了。那么很显然在 `transferIndex` 之前的，没有被线程处理过的短数组就需要其他线程来帮忙进行数据迁移，其他线程来的时候看到的是 `transferIndex` 那么他们就会从 `transferIndex` 往前数 `stride` 个元素作为一个小数组当做自己的迁移任务。

### 4.5 ConcurrentHashMap怎么保证写数据安全

通过自旋锁 + CAS + sychronized + 分段锁

Ⅰ. 先判断散链表是否已经初始化，如果没有初始化则先初始化数组

```java
if(tab == null || (n = tab.length) == 0)
    tab = initTable();
```

Ⅱ. 向桶中加入数据，需要先判断桶中是否为空，如果为空就通过CAS算法将新增数据加入到桶中。

如果写入失败，说明其他线程已经在当前桶位写入了数据，当前线程竞争失败，回到自旋位置，进行等待。

```java
// 如果对应key的哈希值对应table数组下标的位置没有node，则通过cas操作创建一个node放入table
else if((f = tabAt(tab, i = (n - 1) & hash) == null)){
    if(casTabAt(tab,i, null, new Node<K,V>(hash, key, value,null)))
        break;
}
```

Ⅲ. 如果桶中不为空，就需要判断当前桶中头节点的类型：如果桶中头节点值为- 1则表示当前桶位的头节点为fed节点，目前散链表正处于扩容状态，这时候当前线程需要协助扩容。

```java
// 如果table正在扩容，则得到扩容后的table，然后再重新开始一个循环
else if((fh == f.hash) == MOVED) //MOVED = -1
    tab = helpTransfer(tab, f)
```

### ***很难 4.6 ConcurrentHashMap怎么获取size

[ConcurrentHashMap 1.8 计算 size 的方式 - 简书 (jianshu.com)](https://www.jianshu.com/p/971ee45597ac)

```java
//方法一 但不推荐
public int size(){
    long n = sumCount();
    return ((n < 0L) ? 0 : (n > (long)Integer.MAX_VALUE)? Integer.MAX_VALUE):
    (int)n);
}
//更推荐使用这个方法计算
public long mappingCount() {
        long n = sumCount();
        return (n < 0L) ? 0L : n; // ignore transient negative values
    }


//核心方法 sumCount
final long sumCount() {
        CounterCell[] as = counterCells; CounterCell a;
        long sum = baseCount;
        if (as != null) {
            for (int i = 0; i < as.length; ++i) {
                if ((a = as[i]) != null)
                    sum += a.value;
            }
        }
        return sum;
    }
```

==baseCount就是记录容器数量，为什么sumCount()还要遍历counterCells数组，累加对象的值呢==

```java
//counterCells是个全局的变量，表示的是CounterCell类数组。CounterCell是ConcurrentHashmap的内部类，它就是存储一个值。
private transient volatile CounterCell[] counterCells;
```

JDK1.8中使用一个volatile类型的变量baseCount记录元素的个数，当插入新数据put()或则删除数据remove()时，**会通过addCount()方法更新baseCount:**

```java
```

### 4.7 为什么 ConcurrentHashMap 的读操作不需要加锁？

volatile关键字来保证可见性、有序性。但不保证原子性。

```java
static class Node<K,V> implements Map.Entry<K,V> {
        final int hash;
        final K key;
    
    //Node的值 用了volatile修饰
        volatile V val;
        volatile Node<K,V> next;
```

- 数组用volatile修饰主要是保证在数组扩容的时候保证可见性。

# Java并发

## 5.1 volatile

Java线程安全（volatile & synchronized）

- volatile **不能保证线程安全**而 synchronized 可以保证线程安全。

- volatile 只能保证被其修 饰变量的**内存可见性**，但如果对该变量执行的是非原子操作线程依旧是不安全的。
- synchronized 既可以保证其修饰范围内存可见性和操作的原子性，所以 synchronized 是线程安全的

保证了共享变量的可见性，可见性 就是在一个线程修改一个共享变量的时候，另一个线程可以看到修改后的值

线程对 volatile 变量的修改会立刻被其他 线程所感知，即不会出现数据脏读的现象，从而保证数据的“可见性”

## 5.2 进程、线程的区别

**进程** 

一个在内存中运行的应用程序。每个进程都有自己独立的一块内存空间，一个进程可以有多个线程，比如在Windows系统中，一个运行的xx.exe就是一个进程。

**线程**

进程中的一个执行任务（控制单元），负责当前进程中程序的执行。一个进程至少有一个线程，一个进程可以运行多个线程，多个线程可共享数据。

与进程不同的是同类的多个线程共享进程的**堆**和**方法区**资源，但每个线程有自己的**程序计数器**、**虚拟机栈**和**本地方法栈**，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。



==根本区别==：进程是操作系统资源分配的基本单位，而线程是处理器任务调度和执行的基本单位

![image-20211028223308797](E:\实习\Java_docs\pics\image-20211028223308797.png)

## 5.3 死锁出现的原因和如何避免

是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象。

==产生的四个必要条件==

**互斥条件：**资源是独占的且排他使用，进程互斥使用资源，即任意时刻一个资源只能给 一个进程使用，其他进程若申请一个资源，而该资源被另一进程占有时，则申请者等待 直到资源被占有者释放。

**不可剥夺条件：**进程所获得的资源在未使用完毕之前，不被其他进程强行剥夺，而只能 由获得该资源的进程资源释放

**请求和保持条件：**进程每次申请它所需要的一部分资源，在申请新的资源的同时，继续 占用已分配到的资源

**环路等待条件：**在发生死锁时必然存在一个进程等待队列{P1,P2,…,Pn},其中 P1 等待 P2 占有的资源，P2 等待 P3 占有的资源，…，Pn 等待 P1 占有的资源，形成一个进程等待 环路，环路中每一个进程所占有的资源同时被另一个申请，也就是前一个进程占有后一 个进程所深情地资源

==如何解决死锁==

*预防死锁*

*避免死锁*

*检测死锁*

*解除死锁*

## 5.4 sychronized

synchronized 是 Java 的关键字，是一种同步锁，==被 synchronized 修饰的代码块及 方法，在同一时间，只能被单个线程访问==。

三种使用场景

### 5.4.1 修饰实例方法

 synchronized修饰实例方法只需要在方法上加上synchronized关键字即可。

```java
public synchronized void add(){
       i++;
}
```

此时，synchronized加锁的对象就是这个方法所在实例的本身。

### 5.4.2 修饰静态方法

synchronized修饰静态方法的使用与实例方法并无差别，在静态方法上加上synchronized关键字即可

```java
public static synchronized void add(){
       i++;
}
```

此时，synchronized加锁的对象为当前静态方法所在类的Class对象。

### 5.4.3 修饰代码块

synchronized修饰代码块需要传入一个对象。

```java
public void add(f) {
    synchronized (this) {
        i++;
    }
}
```

此时synchronized加锁对象即为传入的这个对象实例。

### 5.4.4 Java对象头和Monitor对象

- **实例对象** 存放类的属性数据信息，包括父类的属性信息，这部分内存按4字节对齐。
- **填充数据** 由于虚拟机要求对象起始位置必须是8字节的整数倍。填充数据不是必须存在的，只是为了字节对齐。
- **对象头** 对象头被分为两部分，分别为： Mark Word（标记字段）、Class Pointer（类型指针）。如果是数组，那么还会有数组长度。

#### 对象头

在对象头的Mark Word中主要存储了对象自身的**运行时数据**，例如哈希码、GC分代年龄、锁状态、线程持有的锁、偏向线程ID以及偏向时间戳等。

![image-20211126203020789](E:\实习\Java_docs\pics\image-20211126203020789.png)

==当为重量级锁时，对象头的MarkWord中存储了指向Monitor对象的指针。==

#### Monitor对象

Monitor对象被称为管程或者监视器锁。在Java中，每一个对象实例都会关联一个Monitor对象。这个Monitor对象既可以与对象一起创建销毁，也可以在线程试图获取对象锁时自动生成。当这个Monitor对象被线程持有后，它便处于锁定状态。

**锁升级**

==偏向锁==:认为锁不存在多线程竞争，总是由同一线程获得。。所以当一个线程获取锁的时候，会在对象头和栈帧中的锁记录里存放锁偏向的线程 ID，之后该线程进入和退出该同步 代码块不需要通过 cas 操作获得锁和释放锁，只需要比较对象头内存储的线程 ID 是不是当 前线程，是的话就获得了锁。

**当多个线程竞争锁时，偏向锁就会撤销，偏向锁撤销之后会升级为轻量级锁**

Monitor是由[ObjectMonitor](https://link.juejin.cn/?target=https%3A%2F%2Fhg.openjdk.java.net%2Fjdk8u%2Fjdk8u%2Fhotspot%2Ffile%2F782f3b88b5ba%2Fsrc%2Fshare%2Fvm%2Fruntime%2FobjectMonitor.hpp)实现的,它是一个使用C++实现的类

ObjectMonitor中有五个重要部分，分别为_ower,_WaitSet,_cxq,_EntryList和count。

- **_ower** 用来指向持有monitor的线程
- **_WaitSet** 调用了锁对象的wait方法后的线程会被加入到这个队列中
- **_cxq** 是一个阻塞队列，线程被唤醒后根据决策判断是放入cxq还是EntryList
- **_EntryList** 没有抢到锁的线程会被放到这个队列
- **coun**t 用于记录线程获取锁的次数，成功获取到锁后count加1，释放锁时count减1

**_WaitSet存放的是处于WAITING状态等待被唤醒的线程。而_EntryList队列中存放的是等待锁的BLOCKED状态。_cxq队列仅仅是临时存放，最终还是会被转移到_EntryList中等待获取锁。**

### synchronized底层实现原理

#### 同步代码块

通过javap -v来反汇编下面的一段代码。

```java
public void add() {
    synchronized (this) {
        i++;
    }
}
```

可以得到如下的字节码指令：

```java
public class com.zhangpan.text.TestSync {
  public com.zhangpan.text.TestSync();
    Code:
       0: aload_0
       1: invokespecial #1                  // Method java/lang/Object."<init>":()V
       4: return

  public void add();
    Code:
       0: aload_0
       1: dup
       2: astore_1
       3: monitorenter    // synchronized关键字的入口
       4: getstatic     #2                  // Field i:I
       7: iconst_1
       8: iadd
       9: putstatic     #2                  // Field i:I
      12: aload_1
      13: monitorexit  // synchronized关键字的出口
      14: goto          22
      17: astore_2
      18: aload_1
      19: monitorexit // synchronized关键字的出口
      20: aload_2
      21: athrow
      22: return
    Exception table:
       from    to  target type
           4    14    17   any
          17    20    17   any
}
```

由此可以得出在字节码中会在同步代码块的入口和出口加上monitorenter和moniterexit指令。当执行到monitorenter指令时，线程就会去尝试获取该对象对应的Monitor的所有权，即尝试获得该对象的锁。

当该对象的 monitor 的计数器count为0时，那线程可以成功取得 monitor，并将计数器值设置为 1，取锁成功。如果当前线程已经拥有该对象monitor的持有权，那它可以重入这个 monitor ，计数器的值也会加 1。而当执行monitorexit指令时，锁的计数器会减1。

### 偏向锁

经研究发现，**在大多数情况下锁不仅不存在多线程竞争关系，而且大多数情况都是被同一线程多次获得**。因此，为了减少同一线程获取锁的代价而引入了偏向锁的概念。

偏向锁的核心思想是，如果一个线程获得了锁，那么锁就进入偏向模式，此时Mark Word的结构也变为偏向锁结构，即将对象头中Mark Word的第30bit的值改为1，并且在Mark Word中记录该线程的ID。当这个线程再次请求锁时，无需再做任何同步操作，即可获取锁的过程。

### 轻量级锁

轻量级锁优化性能的依据是**对于大部分的锁，在整个同步生命周期内都不存在竞争。** 当升级为轻量级锁之后，MarkWord的结构也会随之变为轻量级锁结构。JVM会利用CAS尝试把对象原本的Mark Word 更新为Lock Record的指针，成功就说明加锁成功，改变锁标志位为00，然后执行相关同步操作。

### 自旋锁

自旋锁是基于**在大多数情况下，线程持有锁的时间都不会太长**。因此自旋锁会假设在不久将来，当前的线程可以获得锁，因此虚拟机会让当前想要获取锁的线程做几个空循环(这也是称为自旋的原因)，不断的尝试获取锁。如果还不能获得锁，那就会将线程在操作系统层面挂起，即进入到重量级锁。

### synchronized锁升级过程

1. 当没有被当成锁时，就是一个普通的对象，Mark Word记录对象的HashCode。锁标志位时01，是否偏向锁那一位是0；
2. 当对象被当做同步锁并有一个线程A抢到锁时，锁标志位还是01，但是`是否偏向锁`那一位改成1，前23bit记录抢到锁的线程id，表示进入偏向锁状态。
3.  当线程A再次试图来获得锁时，JVM发现同步锁对象的标志位是01，是否偏向锁是1，也就是偏向状态，Mark Word中记录的线程id就是线程A自己的id，表示线程A已经获得了这个偏向锁，可以执行同步中的代码;
4. 当线程B试图获得这个锁时，JVM发现同步锁处于偏向状态，但是Mark Word中的线程id记录的不是B，那么线程B会先用*CAS操作试图获得锁*。如果抢锁成功，就把Mark Word里的线程id改为线程B的id，代表线程B获得了这个偏向锁，可以执行同步代码。如果抢锁失败，则继续执行步骤5;
5. 偏向锁状态抢锁失败，代表当前所有一定的竞争，**偏向锁将升级为轻量级锁**。JVM会在当前线程的线程栈中开辟一块单独的空间，里面保存指向对象锁Mark Word的指针，同时会在对象锁Mark Word中保存指向这片空间的指针。上述两个保存操作都是CAS操作，如果保存成功，代表线程抢到了同步锁，就把Mark Word中的锁标志位改成00，可以执行同步代码。如果保存失败，表示抢锁失败，竞争太激烈，继续执行步骤6;
6. 轻量级锁抢锁失败，JVM会使用自旋锁，不断地重试，尝试抢锁。
7. 自旋锁重试之后如果抢锁依然失败，同步锁会升级到重量级锁，锁标志位改为10。在这个状态下，未抢到锁的线程都会被阻塞。

## 5.5 AQS源码

[AQS源码详细解读 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/65349219)

```
AbstractQueuedSynchronizer  //队列同步器
```

使用锁时，线程获取锁是一种**悲观锁策略**，即假设每一次执行临界区代码都会产生冲突，所以当前线程获取到锁的时候同时也会阻塞其他线程获取该锁。 而CAS操作（又称为无锁操作）是一种**乐观锁策略**，它假设所有线程访问共享资源的时候不会出现冲突。

###  5.5.1 成员变量

```java
private volatile int state;

//设置期望值，想修改的值，通过CAS操作实现
protected final boolean compareAndSetState(int expect, int update){
    return unsafe.compareAndSwapInt(this, stateOffset,expect, update);
}
//维护了等待队列（也叫CHL队列，同步队列）的头节点和尾节点
private transient volatile Node head;
private transient volatile Node tail;
//CHL队列由链表实现，以自旋的方式获取资源，是可阻塞的先进先出的双向队列，通过自旋和CAS操作保证节点插入和移除的原子性，当有线程获取锁失败，就被添加到队列==末尾==
```

### 5.5.2 内部类Node

AQS的工作模式分为独占模式和共享模式，记录在节点的信息中。

一般地，它的实现类只实现一种模式，ReentrantLock就实现了独占模式；但也有例外，ReentrantReadAndWriteLock实现了独占模式和共享模式。

```java
//当前节点处于共享模式的标记
static final Node SHARED = new Node();
//当前节点处于独占模式的标记
static final Node EXCLUSIVE  = null;
//线程被取消了
static final int CANCELLED =  1;
//释放资源后需唤醒后继节点
static final int SIGNAL    = -1;
//等待condition唤醒
static final int CONDITION = -2;
//工作于共享锁状态，需要向后传播，
//比如根据资源是否剩余，唤醒后继节点
static final int PROPAGATE = -3;

//等待状态，有1,0,-1,-2,-3五个值。分别对应上面的值
volatile int waitStatus;

//前驱节点
volatile Node prev;

//后继节点
volatile Node next;

//等待锁的线程
volatile Thread thread;

//等待条件的下一个节点，ConditonObject中用到
Node nextWaiter;
```

### 5.5.3 获取资源（锁）

获取释放资源是对state变量的修改，

获取锁的方法有**acquire(),acquiredShared()**.

- acquire()独占模式获取资源，忽略中断

```java
public final void acquire(int arg){
    if(!tryAcquire(arg) && 
      //让线程处于一种自旋状态
      //尝试让该线程重新获取锁！ 当条件满足获取到了锁可以从自旋过程中退出，否则继续
      acquireQueued(addWait(Node.EXCULSIVE), arg))
      selfInterrrupt();
}
```

addWaiter()：将当前线程插入至队尾，返回在等待队列中的节点（就是处理了它的前驱后继）

```java
  private Node addWaiter(Node mode) {
        //把当前线程封装为node,指定资源访问模式
        Node node = new Node(Thread.currentThread(), mode);
        // Try the fast path of enq; backup to full enq on failure
        Node pred = tail;
        //如果tail不为空,把node插入末尾
        if (pred != null) {
            node.prev = pred;
            //此时可能有其他线程插入,所以使用CAS重新判断tail
            if (compareAndSetTail(pred, node)) {
                pred.next = node;
                return node;
            }
        }
        //如果tail为空，说明队列还没有初始化，执行enq()
        enq(node);
        return node;
    }
```

### CAS的操作过程

**V 内存地址存放的实际值； O 预期的值（旧值）；N 更新的新值**

当V和O相同时，也就是说旧值和内存中实际的值相同表明该值没有被其他线程更改过，即该旧值O就是目前来说最新的值了，自然而然可以将新值N赋值给V。

### 存在的问题

#### ABA问题

解决方案可以宴席数据库中常用的乐观锁方式，添加一个版本号

#### 自旋时间过长

使用CAS时非阻塞同步，也就是说不会将线程挂起，会自旋进行下一次尝试，如果自旋时间过长对性能有很大的消耗。

#### 只能保证一个共享变量的原子操作

## 5.6 Reentrantlock

### 5.6.1 ReentrantLock使用

ReentrantLock是一种显式锁，需要我们**手动编写加锁和释放锁**的代码。

```java
public class ReentrantLockDemo{
    // 实例化一个非公平锁，构造方法的参数为true表示公平锁，false为非公平锁。
    private final ReentrantLock lock = new ReentrantLock(false);
    private int i;
    
    public void testLock(){
        //获取锁，如果获取不到就一直等待
        lock.lock();
        try{
            // 再次尝试获取锁（可重入），拿锁最多等待100ms
            if(lock.trylock(100, TimeUnit.MILLISECONDS))
                i++;
        }	catch(InterruptedException e){
            e.printStackTrace();
        }finally{
            // 释放锁
            lock.unlock();
            lock.unlock();
        }
    }
}
```

### 5.6.2 源码解析

```java
public class ReentrantLock implements Lock, java.io.Serializable {

    private final Sync sync;
    
    public ReentrantLock() {
        sync = new NonfairSync();
    }

    public ReentrantLock(boolean fair) {
        sync = fair ? new FairSync() : new NonfairSync();
    }
    
    // ...省略其它代码
}
```

它实现了Lock和Serializable两个接口，同时有两个构造方法，在无参构造方法中初始化了一个非公平锁，在有参构造方法中根据参数决定是初始化公平锁还是非公平锁。

```java
public interface Lock(){
    //获取锁
    void lock();
    void lockInterruptibly() throws InterruptedException;
    // 尝试获取锁，成功返回true，失败返回false
    boolean tryLock();
    // 在给定时间内尝试获取锁，成功返回true，失败返回false
    boolean tryLock(long time, TimeUnit unit) throws InterruptedException;
    // 释放锁
    void unlock();
    // 等待与唤醒机制
    Condition newCondition();
}
```

**NonfairSync非公平锁的实现：**

```java
static final class NonfairSync extends Sync {
        private static final long serialVersionUID = 7316153563782823691L;
        protected final boolean tryAcquire(int acquires) {
            return nonfairTryAcquire(acquires);
        }
    }
```

#### lock方法

```java
// ReentrantLock
public void lock(){
    sync.acquire(1);
}
   // AbstractQueuedSynchronizer
    
public final void acquire(int arg) {
	if (!tryAcquire(arg) &&acquireQueued(addWaiter(Node.EXCLUSIVE), arg))
            selfInterrupt();
    }
```



## 5.7 Lock接口

实现类： ReentrantLock，ReentrantReadWriteLock

简单使用：

```java
Lock Lock = new ReentrantLock();
lock.lock();
try{
    
}finally{
    lock.unlock();
}
//最好不要把获取锁的过程写在try语句块中，因为如果在获取锁时发生了异常，异常抛出的同时也会导致锁无法被释放
```

Condition newCondition()

==Condition接口==

synchronized关键字与wait()和notify/notifyAll()方法相结合可以实现等待/通知机制

ReentrantLock需要借助于Condition接口与newCondition（）方法，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。

```java
//使用Condition实现等待、通知机制
//使用单个Condition实例实现等待/通知机制

```

## 6. 线程池

### 6.1 线程池的几个主要参数

```java
public ThreadPoolExecutor(int corePoolSize,
                         int maxmumPoolsize,
                         long keepAliveTime,
                         TimeUnit unit,
                         BlockingQueue<Runnable workQueue,
                         ThreadFactory threadFactory,
                         RejectedExecutionHandler handler) 
```

**corePoolSize**: 线程池中核心线程的数量。当有任务提交到线程池时，如果线程池中的线程数小于corePoolSize,那么则直接创建新的线程来执行任务。

**maximumPoolSize**: 线程池中最大线程数量。当一个任务提交到线程池时，线程池中的线程数大于corePoolSize,并且workQueue已满，那么则会创建新的线程执行任务，但是线程数要小于等于maximumPoolSize。

**keepAliveTime**：非核心线程的存活时间 

**TimeUnit unit**：存活时间单位 

**workQueue**：任务队列。它是一个阻塞队列，用于存储来不及执行的任务的队列。当有任务提交到线程池的时候，如果线程池中的线程数大于等于corePoolSize，那么这个任务则会先被放到这个队列中，等待执行。

**threadFactory**：线程工厂，用于创建线程，一般用默认的即可 

**handler**：拒绝策略，当队列满了并且工作线程大于等于线程池的最大线程数

### 6.2 线程池的生命周期

线程池从诞生到死亡，中间会经历RUNNING、SHUTDOWN、STOP、TIDYING、TERMINATED五个生命周期状态。

- **RUNNING** 表示线程池处于运行状态，能够接收新提交的任务且能对已添加的任务进行处理。RUNNING状态是线程池的初始化状态，线程池一旦被创建就处于RUNNING状态。

- **SHUTDOWN** 线程处于关闭状态，不接受新任务，但可以处理已添加的任务。RUNNING状态的线程池调用shutdown后会进入SHUTDOWN状态。

- **STOP** 线程池处于停止状态，不接受任务，不处理已添加的任务，且会中断正在执行任务的线程，RUNNING状态的线程池调用了shutdownNow后会进入STOP状态。

- **TIDYING**  当所有任务已终止，且任务数量为0时，线程池会进入TIDYING。当线程池处于SHUTDOWN状态时，阻塞队列中的任务被执行完了，且线程池中没有正在执行的任务了，状态会由SHUTDOWN变为TIDYING。当线程处于STOP状态时，线程池中没有正在执行的任务时则会由STOP变为TIDYING。

- **TERMINATED** 线程终止状态。处于TIDYING状态线程执行terminated（）后进入该状态。

  ![image-20211127145041171](E:\实习\Java_docs\pics\image-20211127145041171.png)

  

### 6.2 线程池的执行流程



![image-20211108220501033](E:\实习\Java_docs\pics\image-20211108220501033.png)

1） 在创建线程池后，等待提交过来的任务请求。

2） 当调用execute()方法添加一个请求任务时，线程池会做如下判断：

- 如果正在运行的线程数量小于corePoolSize，那么马上创建核心线程运行这个线程。
- 如果正在运行的线程数量大于或者等于corePoolSize，且此时没有空闲的线程，那么则会将任务存储到workQueue中。
- 如果任务队列满了且正在运行的线程数量小于maximumPoolSize(最大线程数)，那么创建一个非核心线程立刻运行这个任务；

- 如果任务队列满了且正在运行的线程数量大于或等于maximumPoolSize，线程池会执行拒绝策略；

3） 当一个线程完成任务时，会在队列中取下一个任务来执行；

### 6.3 任务拒绝策略

线程池有一个最大的容量，当线程池的任务缓存队列已满，并且线程池中的线程数目达到maximumPoolSize时，就需要拒绝该任务。

拒绝策略是一个接口：

```java
public interface RejectedExecutionHandler{
    void rejectedExecution(Runnable r, ThreadPoolExecutor, executor);
}
//可以通过实现这个接口去定制拒绝策略，也可以选择JDK提供的4种已有策略

```

![image-20211109195944246](E:\实习\Java_docs\pics\image-20211109195944246.png)

### 6.4 线程池的创建方式

```java
ThreadPoolExecutor//：最原始的创建线程池的方式，它包含了 7 个参数可供设置
//1. 创建一个固定大小的线程池
newFixedThreadPool(int nThreads)
//2. 创建单个线程的线程池
newSingleThreadExecutor()
//3. 创建一个可缓存的线程池，若线程数超过处理所需，缓存一段时间后会回收，若线程数不够，则新建线程。
newCachedThreadPool()
//4. 创建一个可以执行延迟任务的线程池。
newScheduledThreadPool(int corePoolSize)
```

## 7. 垃圾回收GC

### 7.1 收集对象

**识别垃圾**

- 引用计数法（reference counting）：对每个对象的引用进行计数，每当有一个地方引用它时计数器 +1、引用失效则 -1，引用的计数放到对象头中，大于 0 的对象被认为是存活对象。
- **可达性分析，又称引用链法（Tracing GC）：** 从 GC Root 开始进行对象搜索，可以被搜索到的对象即为可达对象，此时还不足以判断对象是否存活/死亡，需要经过多次标记才能更加准确地确定，整个连通图之外的对象便可以作为垃圾被回收掉。



**收集算法**

- **Mark-Sweep（标记-清除）：** 回收过程主要分为两个阶段，第一阶段为追踪（Tracing）阶段，即从 GC Root 开始遍历对象图，并标记（Mark）所遇到的每个对象，第二阶段为清除（Sweep）阶段，即回收器检查堆中每一个对象，并将所有未被标记的对象进行回收，整个过程不会发生对象移动。
- **Mark-Compact （标记-整理）：** 这个算法的主要目的就是解决在非移动式回收器中都会存在的碎片化问题，也分为两个阶段，第一阶段与 Mark-Sweep 类似，第二阶段则会对存活对象按照整理顺序（Compaction Order）进行整理。
- **Copying（复制）：** 将空间分为两个大小相同的 From 和 To 两个半区，同一时间只会使用其中一个，每次进行回收时将一个半区的存活对象通过复制的方式转移到另一个半区。

### 7.2 收集器

![image-20211109204207001](E:\实习\Java_docs\pics\image-20211109204207001.png)

#### 7.2.1 ParNew

新生代垃圾回收器

#### 7.2.2 CMS

以获取最短回收停顿时间为目标，采用**“标记-清除”**算法，分 4 大步进行垃圾收集，其中初始标记和重新标记会 STW ，多数应用于互联网站或者 B/S 系统的服务器端上，JDK9 被标记弃用，JDK14 被删除。



1. 初始标记（initial mark)

2) 并发标记（concurrent mark）
3) 重新标记（remark）
4) 并发清除（concurrent sweep）



**初始标记、重新标记**这两个步骤仍然需要“Stop the World"，初始标记仅仅只是标记一下GC Roots能直接关联到的对象。

**并发标记**阶段就是从GC Roots的直接关联对象开始遍历整个对象图的过程，可以并发进行。

**重新标记**阶段则是为了修正并发标记期间，产生变动的那一部分对象的标记记录。

**并发清除**，清理删除掉标记阶段判断的已经死亡的对象，由于不需要移动存放对象，也是可以并发的。

==CMS存在的问题==

1. CMS收集器对处理器资源非常敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程而导致应用程序变慢，降低总吞吐量。
2. **CMS无法处理浮动垃圾**。有可能出现”Con-current Mode Failure“而导致Full GC的产生。
3. 由于是基于”标记-清除“算法实现的收集器，可能会有大量的空间碎片产生。



```java
//参数
//并发执行初始标记
-XX:+CMSParallelInitialMarkEnabled
//当此阶段耗时较长的时候，可以加入参数,在重新标记之前，先执行一次young GC，回收掉年轻代的对象无用的对象。
-XX:CMSScavengeBeforeRemark
```



==使用时的注意点==

- **减少remark阶段的停顿**

一般CMS的GC耗时80%都在remark阶段，如果发现remark阶段停顿事件很长，可以先进行一下Young GC，目的在于减少年轻代对老年代的无效引用，降低remark时的开销。

- **内存碎片问题**

CMS是基于标记-清除算法的，所以会产生内存碎片，这时候需要参数 `-XX:CMSFullGCsBeforeCompaction=n`意思是说上一次CMS并发GC执行过后，还要再执行多少次Full GC才会做压缩。

- **concurrent mode failure**

```java
-XX:+UseCMSInitiatingOccupancyOnly
-XX:CMSInitiatingOccupancyFraction=70 //是指设定CMS在对内存占用率达到70%的时候开始GC
```





#### 7.2.3 G1收集器

一种服务器端的垃圾收集器，应用在多处理器和大容量内存环境中，在实现高吞吐量的同时，尽可能地满足垃圾收集暂停时间的要求。

G1抛弃了之前的分代收集的方式，面向整个堆内存进行回收，把内存划分为多个大小相等的独立区域Region

一共有4种Region：

1. 自由分区 Free Region
2. 年轻代分区 Young Region
3. 老年代分区 Old Region
4. 大对象分区 Humongous Region

每个Region大小通过`-XX:G1HeapRegionSize`来设置，大小为1~32MB，默认最多有2048个Region。

对于大对象的存储，存在Humongous概念，对G1来说，超过一个Region一半大小的对象都被认为大对象，将会被放入Humongous Region，而对于超过整个Region的大对象，则用几个连续的Humongous来存储。

==G1的最大优势是可预测的停顿时间模型==，可以通过参数`-XX:MaxGCPauseMillis`来设置允许的停顿时间（默认为200ms）G1会收集每个Region的回收之后的空间大小、回收需要的时间，根据评估得到的价值，在后台维护一个优先级列表，然后基于我们设置的停顿时间优先回收**价值收益最大**的Region。

##### 回收过程

- 初始标记：标记GC ROOT能关联到的对象，需要STW

- 并发标记：从GCRoots的直接关联对象开始遍历整个对象图的过程，扫描完成后还会重新处理并发标记过程中产生变动的对象

- 最终标记：短暂暂停用户线程，再处理一次，需要STW

- 筛选回收：更新Region的统计数据，对每个Region的回收价值和成本排序，根据用户设置的停顿时间制定回收计划。再把需要回收的Region中存活对象复制到空的Region，同时清理旧的Region。需要STW。



### 7.3 新生代和老年代

新生代又被称为Eden、from survivor、to survior

JVM每次只会使用Eden和其中一块survivor来为对象服务，所以无论什么时候，都会有一块survivor空间，因此新生代实际可用空间为90%。

**新生代GC**（minor gc）：指发生在新生代的垃圾回收动作，因为Java对象大多数都是“朝生夕死”的特性，所以minor GC非常频繁，使用复制算法快速的回收。

## 8. Lock的Condition

相比synchronize的监视器锁，Condition提供了更加灵活和精确的线程控制。它的最大特点是可以为不同的线程建立多个Condition，从而达到精确控制某一些线程的休眠与唤醒。

```java
public interface Condition{
    // 使当前线程进入等待状态,可以相应中断请求
    void await() throws InterruptedException;
    // 使当前线程进入等待状态，不响应中断请求
    void awaitUninterruptibly();
    // 使当前线程进入等待状态，直到被唤醒或中断，或者经过指定的等待时间。nanosTimeout单位纳秒
    long awaitNanos(long nanosTimeout) throws InterruptedException;
    // 同awaitNanos方法，可以指定时间单位
    boolean await(long time, TimeUnit unit) throws InterruptedException;
    // 使线程进入等待状态，直到被被唤醒或者中断，或者到截止的时间
    boolean awaitUntil(Date deadline) throws InterruptedException;
    // 唤醒一个等待在Condition上的线程，与notify功能类似 
    void signal();
    // 唤醒所有等待在Condition上的线程，与notifyAll类似
    void signalAll();
}
```

**Condition的实现类是在AQS中的ConditionObject**

### 8.1 Condition实现”生产者-消费者“模式

[深入理解Java线程的等待与唤醒机制（二） - 掘金 (juejin.cn)](https://juejin.cn/post/6980655421497278495)

### 8.2 Condition实现原理

```java
// ReentrantLock#Sync
final ConditionObject newCondition() {
    return new ConditionObject();
}
```

==BreadContainer中的providerCondition与consumerCondition是两个不同的ConditionObject实例。==

```java
public class ConditionObject implements Condition, java.io.Serializable {
    // 指向等待队列的头结点
    private transient Node firstWaiter;
    // 指向等待队列的尾结点
    private transient Node lastWaiter;

    public ConditionObject() { }
}
```

它内部维护了一个Node类型**等待队列**。其中firstWaiter指向队列的头结点，而lastWaiter指向队列的尾结点。在线程中调用了Condition的await方法后，线程就会被封装成一个Node节点，并将Node的waitStatus设置成CONDITION状态，然后插入到这个Condition的等待队列中。等到收到singal或者被中断、超时就会被从等待队列中移除。

![image-20211127002254423](E:\实习\Java_docs\pics\image-20211127002254423.png)

## 9. ThreadLocal

### **基本使用**

```java
public static void main(String[] args) {
    ThreadLocal<Integer> threadLocal = new ThreadLocal<>();

    new Thread(() -> {
        threadLocal.set(10);
        try {
            Thread.sleep(1000);
            System.out.println(Thread.currentThread().getName() + " value = " + threadLocal.get());
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }).start();

    try {
        Thread.sleep(1000);
        System.out.println(Thread.currentThread().getName() + " value = " + threadLocal.get());
    } catch (InterruptedException e) {
        e.printStackTrace();
    }
}
```

除此之外，ThreadLocal可以设置全局的初始值，代码如下：

```java
ThreadLocal<Integer> threadLocal = ThreadLocal.withInitial(() -> 10);
```

### 9.1 ThreadLocal的实现原理

#### 9.1.1 set过程

```java
public void set(T value){
    //获得当前线程
    Thread t = Thread.currentThread();
    //获取线程中的ThreadLocalMap
    ThreadLocalMap map = getMap(t);
    if(map != null){
        //将值存储到ThreadLocalMap中
        map.set(this, value);
    } else{
        //创建map
        create(t, value);
    }
    
    
    void createMap(Thread t, T firstValue){
        //实例化当前线程中的map
        t.threadLocals = new ThreadLocalMap(this, firstValue);
    }
}
```

#### 9.1.2 get过程

```java
public T get(){
    //获取当前线程
    Thread t = Thread.currentThread();
    // 获取当前线程对应的ThreadLocalMap
    ThreadLocalMap map = getMap(t);
    if(map != null){
        ThreadLocalMap.Entry e = map.getEntry(this);
        if (e != null) {
            @SuppressWarnings("unchecked")
            T result = (T)e.value;
            return result;
        }
    }
}
```

get方法依然是先获取到当前线程，然后拿到当前线程的ThreadLocalMap，并通过ThreadLocalMap的getEntry方法将这个ThreadLocal作为key来取值。如果ThreadLocalMap为null，则会通过setInitialValue方法返回了一个null值。

#### 9.1.3 ThreadLocalMap

ThreadLocalMap是一个存储K-V类型的数据结构，并且Thread类中维护了一个ThreadLocalMap的成员变量。

```java
public class Thread implements Runnable {

    ThreadLocal.ThreadLocalMap threadLocals = null;
    ...
}
```

**ThreadLocalMap是ThreadLocal的内部类**

```java
static class ThreadLocalMap {

    private Entry[] table;
    private int size = 0;
    
}
```

ThreadLocalMap内部维护了一个Entry数组，和一个int类型的size。Entry是ThreadLocalMap的内部类，它就是对我们设置的value的封装

```java
static class Entry extends WeakReference<ThreadLocal<?>>{
    Object value;
    Entry(ThreadLocal<?> k, Object v){
        super(k);
        value = v;
    }
}
```

==也就是说Entry中维护了一个ThreadLocal作为key和一个Object的value作为value。==

#### 9.1.4 内存泄漏的原因

为什么ThreadLocalMap中的Entry要继承WeakReference，使ThreadLocal作为一个弱引用呢？我们知道，弱引用在发生GC时这个对象一定会被回收。通常来说使用弱引用是为了避免内存泄漏。

如果将ThreadLocal声明为强引用，一旦ThreadLocal不再使用，就需要被回收。但是此时由于ThreadLocalMap中的Entry数组持有了ThreadLocal。导致ThreadLocal不能够被回收而出现内存泄漏。那么，如果将ThreadLocal声明为弱引用就可以避免这一问题的出现。



#### 使用场景

- ThreadLocal **用作保存每个线程独享的对象**，为每个线程都创建一个**副本**，这样每个线程都可以修改自己所拥有的副本, 而不会影响其他线程的副本，确保了线程安全。

- **用作每个线程内需要独立保存信息**，以便供其他方法更方便地获取该信息的场景。每个线程获取到的信息可能都是不一样的，前面执行的方法保存了信息后，后续方法可以通过 ThreadLocal 直接获取到，**避免了传参，类似于全局变量的概念**。

代码演示：

```java
public class LocalContext{
    //初始化ThreadLocal 将用户信息装进Local
    private static final ThreadLocal<Node> LOCAL = new InheritbaleThreadLocal<Node>(){
        @Override
        protected Node initialValue(){
            return new Node();
        }
    };
    /**
    将userID装进本地缓存
    **/
    public static void setUserId(Integer userId){
        LOCAL.get().setUserId(userId);
    }
    
    /**获取用户
    **/
    public static Integer getUserId(){
        return LOCAL.get().getUserId();
    }
    //清除本地变量
    public static void remove(){
        LOCAL.remove();
    }
    
    //定义用户
    @Data
    public static class Node{
        private Integer userId;
    	private String openId;
    	private String mobile;
    	private String appId;

    	public Node(){
      		this.userId = 0;
     		this.openId = "";
      		this.mobile = "";
      		this.appId = "";
    }    
}
```





# 8. Mysql

## 索引

InnoDB的数据文件本身就是索引文件，在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。**这种索引叫做聚集索引。**

![image-20211123202107258](E:\实习\Java_docs\pics\image-20211123202107258.png)



## 连接查询

| 类型       | 解释       | 说明                                                         |
| ---------- | ---------- | ------------------------------------------------------------ |
| INNER JOIN | 内连接     | 关键字在表中存在至少一个匹配时返回行(交集)                   |
| LEFT JOIN  | (外)左连接 | 关键字从左表（table1）返回所有的行，即使右表（table2）中没有匹配。如果右表中没有匹配，则结果为 NULL |
| RIGHT JOIN | (外)右连接 | 关键字从右表（table2）返回所有的行，即使左表（table1）中没有匹配。如果左表中没有匹配，则结果为 NULL |
| FULL JOIN  | (外)全连接 | 关键字只要左表（table1）和右表（table2）其中一个表中存在匹配，则返回行.(并集) |

### InoDB和MyISAM的区别

#### 存储结构

MyISAM：每张表被存放在三个文件：frm-表格定义、MYD（MYData）-数据文件、MYI（MYIndex）-索引文件

InnoDB：所有的表都保存在同一个数据文件中，InnoDB表的大小只受限于操作系统文件的大小，一般为2GB

#### 存储空间

MyISAM：可以被压缩，存储空间较小

InnoDB： InnoDB的表需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。

#### 事务

MyISAM：不支持。

Innodb：支持。

### 8.1 四大隔离级别

**事务**： 由一个有限的数据库操作序列构成，这些操作要么全部执行，要么全部不执行，是一个不可分割的工作单位。

**事务的特性**

- **原子性**： 事务作为一个整体被执行，包含在其中的对数据库的操作要么全部都执行，要么都不执行。
- **一致性**： 指在事务开始之前和事务结束以后，数据不会被破坏，假如A账户给B账户转10块钱，不管成功与否，A和B的总金额是不变的。
- **隔离性**：多个事务并发访问时，事务之间是相互隔离的，一个事务不应该被其他事务干扰，多个并发事务之间要相互隔离。
- **持久性**：表示事务完成提交后，该事务对数据库所作的操作更改，将持久地保存在数据库中。

### 8.2 存在的问题

==**脏读 （dirty read）**==

假设现在有两个事务A、B：

- 假设现在A的余额是100，事务A正在准备查询Jay的余额
- 这时候，事务B先扣减Jay的余额，扣了10
- 最后A 读到的是扣减后的余额



==**不可重复读（unrepeatable read）**==

假设现在有两个事务A和B：

- 事务A先查询Jay的余额，查到结果是100
- 这时候事务B 对Jay的账户余额进行扣减，扣去10后，提交事务
- 事务A再去查询Jay的账户余额发现变成了90

==**幻读**==

假设现在有两个事务A、B：

- 事务A先查询id大于2的账户记录，得到记录id=2和id=3的两条记录
- 这时候，事务B开启，插入一条id=4的记录，并且提交了
- 事务A再去执行相同的查询，却得到了id=2,3,4的3条记录了。

### 8.3 事务的隔离级别

- 读未提交（Read Uncommitted）
- 读已提交（Read Committed）
- 可重复读（Repeatable Read）
- 串行化（Serializable）

[一文彻底读懂MySQL事务的四大隔离级别 - 掘金 (juejin.cn)](https://juejin.cn/post/6844904115353436174)

#### 8.3.1 读未提交

```sql
set session transaction isolation level read uncommitted;
//开启事务A
begin;
select * from account where id = 1;
//开启事务B
begin;
update account set balance = labance + 20 where id = 1;
//回到事务A
select * from account where id = 1;
//会看到 balance + 20
```

不能解决脏读问题。

#### 8.3.2 已提交读

```sql
set session transaction isolation level read committed;
//开启事务A
begin;
select * from account where id = 1;
//开启事务B
begin;
update account set balance = balance + 20 where id = 1;
//回到事务A，数据没有改变
select * from account where id = 1;
//到事务B执行事务
commit;
// 再回到事务A查询，发现数据变成balance + 20
```

可以解决脏读问题，但是不能解决**不可重复读**

#### 8.3.3 可重复读

![image-20211111203331561](E:\实习\Java_docs\pics\image-20211111203331561.png)

#### 8.3.4 串行化（Serializable）

## 8.4 隔离级别的实现原理

- 读写锁
- 一致性快照， MVCC

MySql使用不同的锁策略(Locking Strategy)/MVCC来实现四种不同的隔离级别。

Repeatable Read、Read Committed的实现与MVCC有关， Read Uncommitted、串行化与锁有关。



**读未提交**

读未提交，采取的是读不加锁原理。

- 事务读不加锁，不阻塞其他事务的读和写
- 事务写阻塞其他事务写，但不阻塞其他事务读；

**串行化（Serializable)**

- 所有SELECT语句会隐式转化为`SELECT ... FOR SHARE`，即加共享锁。
- 读加共享锁，写加排他锁，读写互斥。如果有未提交的事务正在修改某些行，所有select这些行的语句都会阻塞。

##### MVCC的实现原理

MVCC，中文叫**多版本并发控制**，它是通过读取历史版本的数据，来降低并发事务冲突，从而提高并发性能的一种机制。它的实现依赖于**隐式字段、undo日志、快照读&当前读、Read View**

###### **隐式字段**

对于InnoDB存储引擎，每一行记录都有两个隐藏列**DB_TRX_ID、DB_ROLL_PTR**，如果表中没有主键和非NULL唯一键时，则还会有第三个隐藏的主键列**DB_ROW_ID**。

- DB_TRX_ID，记录每一行最近一次修改（修改/更新）它的事务ID，大小为6字节；
- DB_ROLL_PTR，这个隐藏列就相当于一个指针，指向回滚段的undo日志，大小为7字节；
- DB_ROW_ID，单调递增的行ID，大小为6字节；

###### undo日志

- 事务未提交的时候，修改数据的镜像（修改前的旧版本），存到undo日志里。以便事务回滚时，恢复旧版本数据，撤销未提交事务数据对数据库的影响。

- undo日志是逻辑日志。可以这样认为，当delete一条记录时，undo log中会记录一条对应的insert记录，当update一条记录时，它记录一条对应相反的update记录。

- 存储undo日志的地方，就是**回滚段**。

多个事务并行操作某一行数据时，不同事务对该行数据的修改会产生多个版本，然后通过回滚指针（DB_ROLL_PTR）连一条**Undo日志链**。

==举例==

- 假设表accout现在只有一条记录，插入该该记录的事务Id为100
- 如果事务B（事务Id为200），对id=1的该行记录进行更新，把balance值修改为90

![image-20211111223840302](E:\实习\Java_docs\pics\image-20211111223840302.png)

###### 快照读&当前读

**快照读：**

读取的是记录数据的可见版本（有旧的版本），不加锁,普通的select语句都是快照读,如：

```sql
select * from account where id > 2;
```

**当前读：**

读取的是记录数据的最新版本，显示加锁的都是当前读

```sql
select * from account where id>2 lock in share mode;
select * from  account where id>2 for update;
```

###### Read View

- Read View就是事务执行**快照读**时，产生的读视图。
- 事务执行快照读时，会生成数据库系统当前的一个快照，记录当前系统中还有哪些活跃的读写事务，把它们放到一个列表里。
- Read View主要是用来做可见性判断的，即判断当前事务可见哪个版本的数据



- **m_ids:当前系统中那些活跃的读写事务ID,它数据结构为一个List。**
- **min_limit_id:m_ids事务列表中，最小的事务ID**
- **max_limit_id:m_ids事务列表中，最大的事务ID**

如果DB_TRX_ID < min_limit_id，表明生成该版本的事务在生成ReadView前已经提交(因为事务ID是递增的)，所以该版本可以被当前事务访问。

如果DB_TRX_ID > m_ids列表中最大的事务id，表明生成该版本的事务在生成ReadView后才生成，所以该版本不可以被当前事务访问。

如果 min_limit_id =<DB_TRX_ID<= max_limit_id,需要判断m_ids.contains(DB_TRX_ID)，如果在，则代表Read View生成时刻，这个事务还在活跃，还没有Commit，你修改的数据，当前事务也是看不见的；如果不在，则说明，你这个事务在Read View生成之前就已经Commit了，修改的结果，当前事务是能看见的。

##### 锁相关

###### 共享锁与排他锁

InnoDB 实现了标准的行级锁，包括两种：共享锁（简称 s 锁）、排它锁（简称 x 锁）。

- 共享锁（S锁）：允许持锁事务读取一行。
- 排他锁（X锁）：允许持锁事务更新或者删除一行。

如果事务 T1 持有行 r 的 s 锁，那么另一个事务 T2 请求 r 的锁时，会做如下处理：

- T2 请求 s 锁立即被允许，结果 T1 T2 都持有 r 行的 s 锁
- T2 请求 x 锁不能被立即允许

如果 T1 持有 r 的 x 锁，那么 T2 请求 r 的 x、s 锁都不能被立即允许，T2 必须等待T1释放 x 锁才可以，因为X锁与任何的锁都不兼容。

###### 记录锁（Record Locks）

- 记录锁是最简单的行锁，**仅仅锁住一行**。如：`SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE`

c1 为 10 的记录行会被锁住。

==需要注意的是：`id` 列必须为`唯一索引列`或`主键列`，否则上述语句加的锁就会变成`临键锁`。==

==同时查询语句必须为`精准匹配`（`=`），不能为 `>`、`<`、`like`等，否则也会退化成`临键锁`==

- 记录锁**永远都是加在索引上**的，即使一个表没有索引，InnoDB也会隐式的创建一个索引，并使用这个索引实施记录锁。
- 会阻塞其他事务对其插入、更新、删除

在通过 `主键索引` 与 `唯一索引` 对数据行进行 UPDATE 操作时，也会对该行数据加`记录锁`：

```sql
-- id 列为主键列或唯一索引列
update set age = 50 where id = 1;
```



###### 间隙锁（Gap Locks）

- 间隙锁是一种**加在两个索引之间的锁，或者加在第一个索引之前，或最后一个索引之后的间隙**。

- 使用**间隙锁锁住的是一个区间**，而不仅仅是这个区间中的每一条数据。

- 间隙锁只阻止其他事务插入到间隙中，他们不阻止其他事务在同一个间隙上获得间隙锁，所以 gap x lock 和 gap s lock 有相同的作用。

```sql
select * from table where id between 1 and 10 for update;
```

即所有在`（1，10）`区间内的记录行都会被锁住，所有id 为 2、3、4、5、6、7、8、9 的数据行的插入会被阻塞，但是 1 和 10 两条记录行并不会被锁住。



###### 临键锁（Next-Key Locks）

 每个数据行上的`非唯一索引列`上都会存在一把**临键锁**，当某个事务持有该数据行的**临键锁**时，会锁住一段**左开右闭区间**的数据。

`InnoDB` 中`行级锁`是基于索引实现的，**临键锁**只与`非唯一索引列`有关，在`唯一索引列`（包括`主键列`）上不存在**临键锁**。

| id   | age  |  name  |
| ---- | ---- | :----: |
| 1    | 10   |  Lee   |
| 3    | 24   | Soraka |
| 5    | 32   |  Zed   |
| 7    | 45   | Talon  |

该表中 `age` 列潜在的`临键锁`有：

| (-∞, 10] |
| :------: |
| (10, 24] |
| (24, 32] |
| (32, 45] |
| (45, +∞] |

在事务A中执行：

```sql
-- 根据非唯一索引列update某条记录
update table set name = Vladimir where age = 24;
-- 根据非唯一索引列 锁住某条记录
select * from table where age = 24 for update;
```

事务B中执行：

```sql
-- 命令会被阻塞
insert into table values(100,26, 'Ezreal');
```

## 8.5 主从复制

主从复制是指将主数据库的DDL和DML操作通过二进制日志传到从数据库上，然后再从数据库上对这些日志进行重新执行，从而使从数据库和主数据库的数据保持一致。

#### 8.5.1 主从复制的原理

- MySql主库在事务提交时会把数据变更作为事件记录在二进制日志Binlog中；
- 主库推送二进制日志文件Binlog中的事件到从库的中继日志Relay Log中，之后从库根据中继日志重做数据变更操作，通过逻辑复制来达到主库和从库数据的一致性。
- MySql通过三个线程完成主从库间的数据复制，其中BinLog Dump线程跑在主库上，I/O线程和SQL线程

跑在从库上。

- 当在从库上启动复制时，首先创建I/O线程连接主库，主库随后创建Binlog Dump线程读取数据库事件并发送给I/O线程，I/O线程获取到事件数据后更新到从库的中继日志Relay Log中去，之后从库上的SQL线程读取中继日志Relay Log中更新的数据库事件并应用。

![image-20211112202602539](E:\实习\Java_docs\pics\image-20211112202602539.png)

## 8.6 临时表

MySql在执行SQL语句的时候会临时创建一些存储中间结果集的表，这种表被称为**临时表**，临时表只对当前连接可见，在连接关闭后，临时表会被删除并释放空间。

临时表主要分为内存临时表和磁盘临时表两种。内存临时表使用的是MEMORY存储引擎，磁盘临时表使用的是MyISAM存储引擎。

会产生临时表的情况：

- From中的子查询
- Distinct 查询并加上Order by
- Order by 和 Group by 的子句不一样时
- 使用Union查询

## 8.7 慢查询

#### 8.7.1 慢查询日志

1. MySql的慢查询日志是MySql提供的一种日志记录，它用来记录MySql中查询时间超过设置阈值（long_query_time)的语句，记录到慢查询日志中。
2. long_query_time的默认值是10

#### 8.7.2 开启慢查询日志

**默认情况下，MySql没有开启慢查询日志**。需要手动开启。

```sql
-- 查看慢查询日志是否开启
show variables like '%slow_query_log%'
-- 开启慢查询日志，只对当前数据库生效，并且重启数据库后失效
set global slow_query_log = 1;
-- 查看慢查询日志的阈值，默认为10s
shown variables like '%long_query_time%';
-- 设置阈值
set long_query_time = 3;

```



#### 8.7.3 对慢查询优化

- 分析语句的执行情况，查询SQL语句的索引是否命中
- 优化数据库的结构，将字段很多的表分解成多个表，或者考虑建立中间表
- 优化LIMIT 分页

## 8.8 SQL的执行顺序

```sql
SELECT DISTINCT     select_list FROM     left_table LEFT JOIN     right_table ON join_condition WHERE     where_condition GROUP BY     group_by_list HAVING     having_condition ORDER BY     order_by_condition
```

![image-20211113202004093](E:\实习\Java_docs\pics\image-20211113202004093.png)

## 索引失效

### 使用！=或者<>

```sql
SELECT * FROM `user` WHERE `name` != '冰峰';
```

我们给name字段建立了索引，但是如果!= 或者 <> 这种都会导致索引失效，进行全表扫描

### 类型不一致

```sql
SELECT * FROM `user` WHERE height= 175;
```

height 类型为 varchar，但是这里使用int类型，类型不一致会导致索引失效。

### 函数导致的索引失效

```sql
select * from 'user' where date(create_time) = '2020-09-03';
```

### 运算符

```sql
SELECT * FROM 'user' where age - 1 = 20;
```

### OR

```sql
SELECT * FROM 'user' WHERE 'name' = 'zhangsan' OR height = '175';
```

### 模糊搜索

```sql
SELECT * FROM 'user' WHERE 'name' LIKE '%kk';
```

### NOT IN、NOT EXISTS

```sql
SELECT s.* FROM `user` s WHERE NOT EXISTS (SELECT * FROM `user` u WHERE u.name = s.`name` AND u.`name` = '冰峰')
```

### IS NULL不走索引，IS NOT NULL走索引

### ThreadLocal

**基本使用**

```java
//创建一个ThreadLocal对象
private ThreadLocal<Integer> localInt = new ThreadLocal<>();
// 上述代码创建一个localInt变量，由于ThreadLocal是一个泛型类，这里指定了localInt的类型为整数。

//如何设置和获取这个变量的值
public int setAndGet(){
    localInt.set(8);
    return localInt.get();
}
/***
上述代码设置变量的值为8，接着取得这个值。
由于ThreadLocal里设置的值，只有当前线程自己看得见，这意味着你不可能通过其他线程为它初始化值。
ThreadLocal提供了一个withInitial()方法统一初始化所有线程的ThreadLocal的值
***/
private ThreadLocal<Integer> localInt = ThreadLocal.withInitial(() -> 6);
//上述代码将ThreadLocal的初始值设置为6，这对全体线程都是可见的。
```

#### 实现原理

```java
public T get(){
    //获取当前线程
    Thread t = Thread.currentThread();
    //每个线程 都有一个自己的ThreadLocalMap
    //ThreadLocalMap里就保存着所有的ThreadLocal变量
    ThreadLocalMap map = getMap(t);
    if(map != null){
        //ThreadLocalMap的key就是当前ThreadLocal对象实例，
        //多个ThreadLocal变量都是放在这个map中
        ThreadLocalMap.Entry e = map.getEntry(this);
        if(e != null){
            //从map 中取出来的值就是我们需要的这个ThreadLocal变量
            T result = (T)e.value;
            return result;
        }
    }
    //如果map没有初始化
    return setInitialValue();
}
```

可以看到，所谓的ThreadLocal变量就是保存在每个线程的map中的。这个map就是Thread对象中的threadLocals字段。如下：

```java
ThreadLocal.ThreadLocalMap threadLocals = null;
```

ThreadLocal.ThreadLocalMap是一个比较特殊的Map，它的每个Entry的key都是一个弱引用：

```java
static class Entry extends WeakReference<ThreadLocal<?>> {
    /** The value associated with this ThreadLocal. */
    Object value;
    //key就是一个弱引用
    Entry(ThreadLocal<?> k, Object v) {
        super(k);
        value = v;
    }
}
```

#### 内存泄漏的原因

严格来说，ThreadLocal没有内存泄漏问题。有的话，那就是你忘记执行remove方法。这是不正确使用引起的。

如果你不调用remove方法的话，ThreadLocal所对应的值，就会存在，一直到当前线程的销毁。

众所周知，线程的生命周期都比较长，加上现在普遍使用的线程池，会让线程的生命更加长。不remove，当然不会释放。这和Key，到底是不是弱引用，关系不大。

#### 使用场景

- ThreadLocal **用作保存每个线程独享的对象**，为每个线程都创建一个**副本**，这样每个线程都可以修改自己所拥有的副本, 而不会影响其他线程的副本，确保了线程安全。

- **用作每个线程内需要独立保存信息**，以便供其他方法更方便地获取该信息的场景。每个线程获取到的信息可能都是不一样的，前面执行的方法保存了信息后，后续方法可以通过 ThreadLocal 直接获取到，**避免了传参，类似于全局变量的概念**。

代码演示：

```java
public class LocalContext{
    //初始化ThreadLocal 将用户信息装进Local
    private static final ThreadLocal<Node> LOCAL = new InheritbaleThreadLocal<Node>(){
        @Override
        protected Node initialValue(){
            return new Node();
        }
    };
    /**
    将userID装进本地缓存
    **/
    public static void setUserId(Integer userId){
        LOCAL.get().setUserId(userId);
    }
    
    /**获取用户
    **/
    public static Integer getUserId(){
        return LOCAL.get().getUserId();
    }
    //清除本地变量
    public static void remove(){
        LOCAL.remove();
    }
    
    //定义用户
    @Data
    public static class Node{
        private Integer userId;
    	private String openId;
    	private String mobile;
    	private String appId;

    	public Node(){
      		this.userId = 0;
     		this.openId = "";
      		this.mobile = "";
      		this.appId = "";
    }    
}
```

# 9. Redis

## 9.1 五种基本数据类型

### 9.1.1 字符串对象

```
set number 520
get number
```

- 应用场景： 共享session、分布式锁、计数器、限流
- 内部编码：`int（8字节长整型）/embstr（小于等于39字节字符串）/raw（大于39个字节字符串）`

### 9.1.2 散列对象

内部实现结构上与JDK1.7的HashMap一致，底层通过数据+链表实现。

- 哈希类型是指v（值）本身又是一个键值对（k-v)结构
- 举例： `hset key field value`,`hget key field`
- 内部编码： `ziplist(压缩列表)`,`hashtable(哈希表)`
- 应用场景：
- - 记录整个博客的访问人数（数据量大会考虑HyperLogLog，但是这个数据结构存在很小的误差，如果不能接受误差，可以考虑别的方案）
  - 记录博客中某个博主的主页访问量、博主的姓名、联系方式、住址

### 9.1.3 列表对象

简介：列表（list）类型是用来存储多个有序的字符串，一个列表最多可以存储2^32-1个元素。

简单实用举例：` lpush  key  value [value ...]` 、`lrange key start end`

内部编码：ziplist（压缩列表）、linkedlist（链表）

应用场景： 消息队列，文章列表

- lpush+lpop=Stack（栈）
- lpush+rpop=Queue（队列）
- lpsh+ltrim=Capped Collection（有限集合）
- lpush+brpop=Message Queue（消息队列）

```c++
typedef struct listNode{
    //前置节点
    struct listNode *prev;
    //后置节点
    struct listNode *next;
    void *value;
}listNode;
```

![image-20211115191406712](E:\实习\Java_docs\pics\image-20211115191406712.png)

- 链表被用于实现Redis的各种功能，比如列表键、发布与订阅、慢查询、监视器等。

### 9.1.4 集合对象

**Redis的set集合相当于Java的HashSet。**

![image-20211117201357931](E:\实习\Java_docs\pics\image-20211117201357931.png)

相当于

- 简介：集合（set）类型也是用来保存多个的字符串元素，但是不允许重复元素
- 简单使用举例：`sadd key element [element ...]`、`smembers key`
- 内部编码：`intset（整数集合）`、`hashtable（哈希表）`
- 应用场景： 用户标签、生成随机数抽奖、社交需求。

### 9.1.5 有序集合

zset为有序（优先score排序，score相同则元素字典序），自动去重的集合数据类型，其底层实现为字典和跳跃表，当数据比较少的时候用压缩列表（ziplist）编码结构存储。

**同时满足**以下两个条件采用ziplist存储：

- 有序集合保存的元素数量小于默认值128个
- 有序集合保存的所有元素的长度小于默认值64字节

#### ziplist存储方式

当ziplist作为zset的底层存储结构时候，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，**第一个节点保存元素的成员，第二个元素保存元素的分值**

![image-20211122160546544](E:\实习\Java_docs\pics\image-20211122160546544.png)

#### 字典+跳跃表



## 9.2 数据结构

### 9.2.1 SDS(simple dynamic string)

```c++
struct sdshdr{
    //记录buf数组已使用字节的数量
    int len;
    //记录buf数组未使用字节的数量
    int free;
    //字节数组，用于保存字符串
    char buf[];
}
```

![image-20211115190014649](E:\实习\Java_docs\pics\image-20211115190014649.png)

**SDS的好处**

###### **常数复杂度获取字符串长度**

通过`STRLEN`命令得到一个字符串的长度，在`SDS`结构中`len`属性记录了字符串的长度。

###### **杜绝缓冲区（数据）溢出**

当需要修改数据时，首先会检查当前SDS空间len是否满足，不满足则自动扩容空间至修改所需的大小，然后再实行修改

==内存重分配策略==

SDS通过两种内存重分配策略，很好的解决了字符串在增长和缩短时的内存分配问题。

1. 空间预分配

当修改字符串对SDS空间进行扩展时，不仅会为SDS分配修改所必要的空间，还会再为SDS分配额外的未使用空间`free`。**其大小一般为修改后的长度**。

2. 惰性空间释放

用于优化SDS字符串缩短操作，当缩短SDS字符串后，并不会立即执行内存重分配回收多余的空间，而是用`free`属性将这些空间记录下来。

### 9.2.2 链表

#### 链表节点

```c++
typedef struct listNode{
    //前置节点
    struct listNode *prev;
    //后置节点
    struct listNode *next;
    void *value;
}listNode;
```

```c++
typedef struct list {
    // 链表头节点
    listNode *head;
    // 链表尾节点
    listNode *tail;
    // 节点值复制函数
    void *(*dup)(void *ptr);
    // 节点值释放函数
    void (*free)(void *ptr);
    // 节点值对比函数
    int (*match)(void *ptr, void *key);
    // 链表所包含的节点数量
    unsigned long len;
} list;
```

#### 链表特性

- 双端链表： 带有指向前置节点和后置节点的指针
- 无环： 表头节点的prev和表尾节点的next都指向null
- 链表长度计数器：带有len属性

### 9.2.3 字典

用于保存键值对（key-value pair)

```sql
set msg 'hello world'
# 在数据库中创建一个键为"msg",值为"hello world"
```

当一个哈希键包含的键值对比较多，又或者键值对中的元素都是比较长的字符串时，Redis就会使用字典作为哈希键的底层实现。

==字典使用哈希表作为底层实现，一个哈希表里面又多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。==

**哈希表**

```c++
typedef struct dictht{
    //哈希表数组
    dictEntry **table;
    //哈希表大小
    unsigned long size;
    //哈希表大小掩码，用于计算索引值
    //总是等于size - 1
    unsigned long sizemask;
    //已有节点数量
    unsigned long used;
}dictht;
```

**哈希表节点**

```c++
typedef struct dictEntry{
    //键
    void *key;
    //值
    union{
        void *val;
        uint64_tu64;
        int64_ts64;
    }v;
    //指向下一个哈希表节点，形成链表
    struct dictEntry *next;
}dictEntry;
```

![image-20211115192436628](E:\实习\Java_docs\pics\image-20211115192436628.png)

**字典实现**

```c++
typedef struct dict{
    //类型指定
    dictType * type;
    //私有属性
    void * privdata;
    //哈希表
    dictht ht[2];
    //rehash索引
    //当不在rehash时，值为-1
    int rehashidx;
}dict;
```

- ht属性是一个包含两个项的数组，数组中的每个项都是一个dictht哈希表，一般情况下，字典只使用ht [0]哈希表,ht [1]哈希表只会在对ht[0]哈希表进行rehash时使用。
- 除了ht[1]之外，另一个和rehash有关的属性就是rehashidx，它记录了rehash目前的进度，如果目前没有在进行rehash，那么它的值为-1。

#### Rehash过程

1. 为字典的ht[1]分配空间，取决于ht[0].used属性的值

- 如果执行的是扩展操作，那么ht[1]的大小为第一个大于等于ht[0].used * 2的 2^n
- 如果执行的收缩操作，那么ht[1]的大小为第一个大于等于ht[0].used 的2^n

2. 将保存在ht[0]中的所有键值对rehash到ht[1]上面：rehash是指重新计算哈希值和索引值。
3. 当ht[0]包含的所有键值对都迁移到了ht[1]之后（ht[0]为空表时), 释放ht[0]， 将ht[1]设置为ht[0]，并且ht[1]新创建一个空白哈希表。

==扩展和收缩的条件==

当以下条件中的任意一个被满足时，程序会自动开始对哈希表执行扩展操作：

1. 服务器目前没有执行BGSAVE命令或者BGREWRITEAOF命令，并且哈希表的**负载因子大于等于1**
2. 服务器目前执行BGSAVE命令或者BGREWRITEAOF命令，并且哈希表的**负载因子大于等于5.**

收缩： **负载因子小于0.1**



### 9.2.4 跳跃表

跳跃表是有序集合（Sorted Set）的底层实现之一，如果有序集合包含的元素比较多，或者元素的成员是比较长的字符串时，Redis会使用跳跃表做有序集合的底层实现

#### 跳跃表的定义

==多层的链表==

- **多层**的结构组成，每层是一个有序的链表
- 最底层（level 1）的链表包含所有的元素
- 跳跃表的查找次数近似于层数
- 跳跃表是一种随机化的数据结构（通过抛硬币来决定层数）

![image-20211122153123500](E:\实习\Java_docs\pics\image-20211122153123500.png)

### 9.2.5 整数集合

```c++
typedef struct intset{
    //编码方式
    uint32_t encoding;
    //集合包含的元素数量
    uint32_t length;
    //保存元素的数组
    int8_t contents[];
}intset;
```

### 9.2.6 压缩列表

是由一系列特殊编码的连续内存块组成的顺序性（sequential）数据结构，一个压缩列表可以包含多个节点，每个节点可以保存一个字节数组或者一个整数值。

压缩列表是列表（List）和散列（Hash）的底层实现实现之一，一个列表只包含少量列表项，并且每个列表项是小整数值或比较短的字符串，会使用压缩列表作为底层实现

`zlbytes`：记录整个压缩列表占用的内存字节数，在压缩列表内存重分配，或者计算`zlend`的位置时使用

`zltail`：记录压缩列表表尾节点距离压缩列表的起始地址有多少字节，通过该偏移量，可以不用遍历整个压缩列表就可以确定表尾节点的地址

`zllen`：记录压缩列表包含的节点数量，但该属性值小于UINT16_MAX（65535）时，该值就是压缩列表的节点数量，否则需要遍历整个压缩列表才能计算出真实的节点数量

`entryX`：压缩列表的节点

`zlend`：特殊值0xFF（十进制255），用于标记压缩列表的末端

#### 压缩列表的节点构成

![image-20211122154047229](E:\实习\Java_docs\pics\image-20211122154047229.png)

- `previous_entry_ength`：记录压缩列表前一个字节的长度
- `encoding`：节点的encoding保存的是节点的content的内容类型
- `content`：content区域用于保存节点的内容，节点内容类型和长度由encoding决定

## RDB快照持久化

RDB持久化是通过**快照**的方式，即在指定的时间间隔内将内存中的数据集快照写入磁盘。在创建快照之后，用户可以备份该快照，可以将快照复制到其他服务器以创建相同数据的服务器副本，或者在重启服务器后恢复数据。RDB是Redis**默认的持久化方式**

RDB文件默认为当前工作目录下的`dump.rdb`，可以根据配置文件中的`dbfilename`和`dir`设置RDB的文件名和文件位置

==触发快照的时机==

- 执行save和bgsave命令
- 配置文件设置`save<seconds><changes>`规则，自动间隔性执行bgsave命令
- 主从复制时，从库全量复制同步主库数据，主库会执行bgsave
- 执行flushall命令清空服务器数据
- 执行shutdown命令关闭Redis时，会执行save命令



#### save和bgsave命令

执行save和bgsave命令，可以手动触发快照，生成RDB文件

使用save命令会**阻塞Redis服务器进程**，服务器进程在RDB文件创建完成之前是不能处理任何请求

而bgsave命令，会**fork**一个子进程，然后该子进程会负责创建RDB文件。

```c++
127.0.0.1:6379>bgsave
Background saving started
```

`fork`一个子进程，子进程会把数据集先写入临时文件，写入成功之后，再替换之前的RDB文件，用二进制压缩存储，这样可以保证RDB文件始终存储的是完整的持久化内容

#### AOF持久化

AOF持久化会把被执行的写命令写到AOF文件的末尾，记录数据的变化。默认情况下，Redis是没有开启AOF持久化的，开启后，每执行一条更改Redis数据的命令，都会把该命令追加到AOF文件中，

==AOF需要记录Redis的每个写命令，步骤为：命令追加（append）、文件写入（write）和文件同步（sync）==

**命令追加**：

开启AOF持久化功能后，服务器每执行一个写命令，都会把该命令以协议格式先追加到`aof_buf`缓存区的末尾，而不是直接写入文件，减少硬盘IO次数。

**文件写入(write)和文件同步(sync)**

对于何时把`aof_buf`缓冲区的内容写入保存在AOF文件中，Redis提供了多种策略

- `appendsync always`: 将`aof_buf`缓冲区的所有内容写入并同步到AOF文件，每个写命令同步写入磁盘
- `appendfsync everysec`: 将`aof_buf`缓冲区的内容写入AOF文件，每秒同步一次，该操作由一个线程单独完成。
- `appendfsync no`：将`aof_buf`缓存区的内容写入AOF文件，什么时候同步由操作系统来决定

`appendfsync`选项的默认配置为`everysec`，即每秒执行一次同步

#### AOF重写（rewrite）

```sql
127.0.0.1:6379> set s1 hello
OK
127.0.0.1:6379> set s2 world
OK
```

==appendonly.aof==

```
*3
$3
set
$2
s1
$5
hello
*3
$3
set
$2
s2
$5
world
```

该命令格式为Redis的序列化协议（RESP）。`*3`代表这个命令有三个参数，`$3`表示该参数长度为3

AOF重写的目的就是减小AOF文件的体积，不过值得注意的是：**AOF文件重写并不需要对现有的AOF文件进行任何读取、分享和写入操作，而是通过读取服务器当前的数据库状态来实现的**

文件重写可分为手动触发和自动触发，手动触发执行`bgrewriteaof`命令，该命令的执行跟`bgsave`触发快照时类似的，都是先`fork`一个子进程做具体的工作

自动触发会根据`auto-aof-rewrite-percentage`和`auto-aof-rewrite-min-size 64mb`配置来自动执行`bgrewriteaof`命令

- 重写会有大量的写入操作，所以服务器进程会`fork`一个子进程来创建一个新的AOF文件

- 在重写期间，服务器进程继续处理命令请求，如果有写入的命令，追加到`aof_buf`的同时，还会追加到`aof_rewrite_buf`AOF重写缓冲区

- 当子进程完成重写之后，会给父进程一个信号，然后父进程会把AOF重写缓冲区的内容写进新的AOF临时文件中，再对新的AOF文件改名完成替换，这样可以保证新的AOF文件与当前数据库数据的一致性

#### RDB和AOF优缺点

**RDB**

优点：

- RDB快照是一个压缩过的非常紧凑的文件，保存着某个时间点的数据集，适合做数据的备份，灾难恢复
- 可以最大化Redis性能，在保存RDB文件，服务器进程只需fork一个子进程来完成RDB文件的创建，父进程不需要做IO操作
- 与AOF相比，恢复大数据集的时候会更快

缺点：

- RDB的数据安全性不如AOF，保存整个数据集的过程比较繁重，如果服务器宕机，可能丢失几分钟的数据
- Redis数据集比较大时，fork的子进程会比较消耗CPU、耗时



**AOF**

优点：

- 数据更完整，安全性更高，（取决于fsync策略）

- AOF文件只是一个追加的日志文件，内容是可读的，适合误删紧急恢复

缺点：

- 对于相同的数据集，AOF文件体积要大于RDB文件

- 根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB。 不过在一般情况下， 每秒 fsync 的性能依然非常高

### 9.4 内存回收

1. 在Redis中，set指令可以指定key的过期时间，当过期时间到达后，key就失效了
2. Redis是基于内存操作的，所有数据都是保存在内存中的。

#### 9.4.1 内存回收机制

Redis的内存回收主要分为过期删除策略和内存淘汰策略两部分。

##### 过期删除策略

删除达到过期时间的key

**1.定时删除**

对于每一个设置了过期时间的key都会创建一个定时器，一旦到达过期时间就会立即删除。该策略可以立即清除过期的数据，对内存较友好，但是缺点是占用了大量的CPU资源去处理过期的数据，会影响Redis的吞吐量和响应时间。

**2.惰性删除**

当访问一个key时，才判断该key是否过期，过期则删除。该策略能最大限度地节省CPU资源。有一种极端的情况是可能出现大量的过期key没有被再次访问，因为不会被清除，导致占用了大量的内存。

**3.定期删除**

每隔一段时间，扫描Redis中过期key字典，并且清除部分过期的key。这是**折中方案。**

*redisDb结构体定义*

```c++
typedef struct redisDb{
    dict *dict; 	//键空间，保存所有键值对
    dict *expires;	//保存所有过期的key
    dict * blocking_keys;	
    dict *ready_keys;
    dict *watched_keys;
    int id; 	//数据库ID字段，代表不同的数据库
    long long avg_ttl; 
}
```

**expires属性**

它的类型也是字典，Redis会把所有的过期键值对加入到expires，之后再通过定期删除清理expires里面的值，加入expires的场景有：

1. set指定过期时间expire

如果设置key的时候指定了过期时间，Redis会将这个key直接加入到expires字典中并将超时时间设置到该字典元素。

2. 调用expire命令

显式指定某个key的过期时间

3. 恢复或者修改数据

从Redis持久化文件中恢复文件或者修改key，如果数据中的key已经设置过期时间，那么就将他加入expires

**Redis清理过期key的时机**

1. Redis在启动的时候，会注册两种事件，一种是时间事件，另一种是文件事件。

在时间事件中，redis注册的回调函数是`serverCron`，在定时任务回调函数中，通过调用databasesCron清理部分过期key（*定期删除的实现*）

```c++
int serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData)
{
    databasesCron();
}
```

2. 每次访问key的时候，都会调用`expireIfNeeded`函数判断key是否过期，如果是，清理key*（惰性删除的实现）*

```c++
robj *lookupKeyRead(redisDb *db, robj *key){
    robj *val;
    expireIfNeeded(db, key);
    val = lookupKey(db, key);
    
    return val;
}
```

**删除key**

Redis4.0以前，使用`del`指令删除，del会直接释放对象的内存。

Redis4.0版本引入了`unlink`指令，能对删除操作进行‘懒’处理，将删除操作丢给后台线程，由后台线程来异步回收内存。

==实际上，在判断key需要过期之后，真正删除key 的过程是先广播expire事件到从库和AOF文件中，然后根据Redis的配置决定立即删除还是异步删除。==

如果是立即删除，Redis会立即释放key和value占用的的内存空间，否则，Redis会在另一个bio线程中释放需要延迟删除的空间。

##### 内存淘汰策略

是指内存达到**maxmemory**极限时，使用某种算法来决定清理掉哪些数据，以保证新数据的存入。

- noeviction：当内存不足以容纳新写入数据时，新写入操作会报错
- allkeys-lru：当内存不足以容纳新写入数据时，在键空间`(server.db[i].dict)`中，移除最近最少使用的key**（最常用）**

- allkeys-random：当内存不足以容纳新写入数据时，在键空间（`server.db[i].dict`）中，随机移除某个 key。

- volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（`server.db[i].expires`）中，移除最近最少使用的 key。

- volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（`server.db[i].expires`）中，随机移除某个 key。

- volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（`server.db[i].expires`）中，有更早过期时间的 key 优先移除。

==在配置文件中，通过maxmemory-policy可以配置要使用哪一个淘汰机制。==

```c++
int processCommand(client *c)
{
    ...
    if (server.maxmemory) {
        int retval = freeMemoryIfNeeded();  
    }
    ...
}
```

###### LRU实现原理

在淘汰key时，Redis默认最常用的是LRU算法（Latest Recently Used）。Redis通过在每一个redisObject保存lru属性来保存key最近的访问时间，在实现LRU算法时直接读取key的lru属性。

具体实现时，Redis遍历每一个db，从每一个db中随机抽取一批样本key，默认是3个key，再从这3个key中，删除最近最少使用的key。

### 9.5 三大缓存问题

#### 9.5.1 缓存穿透

缓存穿透是指查询一条数据库和缓存都没有的一条数据，就会一直查询数据库，对数据库的访问压力就会增大。

解决方案：

1. 缓存空对象：代码维护比较简单，但是效果不好
2. 布隆过滤器：代码维护复杂，但是效果很好

##### 缓存空对象

缓存空对象是指当一个请求过来缓存和数据库中都不存在该请求的数据，第一次请求就会掉过缓存进行数据库的访问，并且访问数据库后返回为空，此时也将该对象进行缓存。

若是再次进行访问该空对象的时候，就会直接**击中缓存**，而不是再次**数据库**，缓存空对象实现的原理图如下：

![image-20211116213555543](E:\实习\Java_docs\pics\image-20211116213555543.png)

缓存空对象的实现代码很简单，但是缓存空对象会带来比较大的问题，就是缓存中会存在很多空对象，占用**内存的空间**，浪费资源，一个解决的办法就是设置空对象的**较短的过期时间**

##### 布隆过滤器

布隆过滤器是一种基于**概率**的**数据结构**，主要用来判断某个元素是否在集合内，它具有**运行速度快**（时间效率），**占用内存小**的优点（空间效率），但是有一定的**误识别率**和**删除困难**的问题。它只能告诉你某个元素一定不在集合内或可能在集合内。

在实际项目中会启动一个**系统任务**或者**定时任务**，来初始化布隆过滤器，将热点查询数据的id放进布隆过滤器里面，当用户再次请求的时候，使用布隆过滤器进行判断，该订单的id是否在布隆过滤器中存在，不存在直接返回null

#### 9.5.2 缓存击穿

缓存击穿是指一个key非常热点，在不停的扛着大并发，**大并发**集中对这一个点进行访问，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，瞬间对数据库的访问压力增大。

缓存击穿这里强调的是**并发**，造成缓存击穿的原因有两个：

1. 该数据没有人查询过，第一次就大并发的访问
2. 添加到了缓存，但是**失效**了，大并发访问

对于缓存击穿的解决方案是加锁：

当用户出现**大并发**访问的时候，在查询缓存的时候和查询数据库的过程加锁，只能第一个进来的请求进行执行，当第一个请求把该数据放进缓存中，接下来的访问就会直接集中缓存，防止了**缓存击穿**。

业界比较普遍的一种做法，即根据key获取value值为空时，锁上，从数据库中`load`数据后再释放锁。若其它线程获取锁失败，则等待一段时间后重试。

以一个获取商品库存的案例进行代码的演示，**单机版**的锁实现具体实现的代码如下：

```java
//获取库存数量
public String getProduceNum(String key){
    try{
        synchronized(this){
            //加锁
            //缓存中取数据，并存入缓存中
            int num = Integer.parseInt(redisTemplate.opsForValue().get(key));
            
            if(num &gt; 0){
                //每查一次库存 - 1
                redisTemplate.opsForValue().set(key, (num - 1)+"");
                System.out.println("剩余库存为" + (num - 1));
            }
            else{
                System.out.println("库存为0");
            }
            
        }
    }
}
```

#### 9.5.3 缓存雪崩

缓存雪崩是指在某一个时间段，缓存集中过期失效，此刻无数的请求直接绕开缓存，直接请求数据库。

造成缓存雪崩的原因，有两点：

1. redis宕机
2. 大部分数据失效

比如天猫双11，马上就要到双11零点，很快就会迎来一波抢购，这波商品在23点集中的放入了缓存，假设缓存一个小时，那么到了凌晨24点的时候，这批商品的缓存就都过期了。

而对这批商品的访问查询，都落到了数据库上，对于数据库而言，就会产生周期性的压力波峰，对数据库造成压力，甚至压垮数据库。

**对于缓存雪崩的解决方案有以下两种**：

1. 搭建高可用的集群，防止单机的redis宕机。
2. 设置不同的过期时间，防止同意之间内大量的key失效。

### 9.6 数据一致性

#### KV、DB读写模式

缓存+数据库读写模式，就是**Cache Aside Pattern**

- 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回相应。
- 更新的时候，先**更新数据库，然后再删除缓存。**

==为什么是删除缓存，而不是更新缓存==

很多时候，在复杂的缓存场景，缓存不单单是数据库中直接取出来的值。

比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。

### 9.7 Redis线程模型

Redis内部使用文件事件处理器`file event handler`，这个文件事件处理器是单线程的，所以Redis才叫做单线程的模型。它采用IO多路复用机制同时监听多个**Socket**，根据Socket上的事件来选择对应的事件处理器进行处理。

文件事件处理器的结构包含4个部分：

- 多个Socket
- IO多路复用程序
- 文件事件派发器
- 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）

### 9.8 什么是热Key问题，如何解决热Key问题

在Redis中，我们把访问频率高的Key，称为热点key

如果某一热点key的请求到服务器主机时，由于请求量特别大，可能会导致主机资源不足，甚至宕机，从而影响正常的服务。

**热点key的产生**

- 用户消费的数据远大于生产的数据，如秒杀、热点新闻等读多写少的场景。
- 请求分片集中，超过单Redis服务器的性能，比如固定名称key，Hash落入同一台服务器。

**如何解决热Key问题**

- Redis集群扩容：增加分片副本，均衡读流程。

- 将热key分散到不同的服务器中；
- 使用二级缓存，即JVM本地缓存,减少Redis的读请求。

### 9.9 集群方案

#### 9.9.1 主从复制模式

![image-20211118230939850](E:\实习\Java_docs\pics\image-20211118230939850.png)

`Redis`主从复制，主从库模式一个`Master`主节点多`Slave`从节点的模式，将一份数据保存在多`Slave`个实例中，**增加副本冗余量**，当某些出现宕机后，`Redis`服务还可以使用。

但是这会存在数据不一致问题，那redis的副本集是如何数据一致性？

==`Redis`为了保证数据副本的一致，主从库之间采用读写分离的方式==：

- **读操作：主库、从库都可以执行处理；**
- **写操作：先在主库执行，再由主库将写操作同步给从库。**

使用读写分离方式的好处，可以避免当主从库都可以处理写操作时，主从库处理写操作加锁等一系列巨额的开销。

主从库是同步数据方式有两种：

##### **全量同步**

- 当一次从库启动时，从库给主库发送`psync`命令进行数据同步（`psync` 命令包含：主库的 `runID` 和复制进度 `offset` 两个参数）
- 当主库接收到ysync命令后将会保存RDB文件并发送给从库，发送期间会使用缓存区`（replication buffer)`记录后续的所有写操作，从库收到数据后，会先**清空**当前数据库，然后加载从主库获取的RDB文件。
- 当主库完成RDB文件发送后，也会把将保存发送RDB文件期间期间写操作的`replication buffer`发送给从库，从库再重新执行这些操作，这样主从库就实现同步了。

另外，为了分担主库生成 RDB 文件和传输 RDB 文件压力，提高效率，可以使用 **“主 - 从 - 从”模式将主库生成 RDB 和传输 RDB 的压力，以级联的方式分散到从库上。**

<img src="E:\实习\Java_docs\pics\image-20211118232056333.png" alt="image-20211118232056333" style="zoom:67%;" />

##### 增量同步

增量同步，基于环形缓冲区`repl_backlog_buffer`缓存区实现

在环形缓冲区，主库会记录自己写到的位置`master_repl_offset`，从库会记录自己已经读到的位置`slave_repl_offset`,主库并通过`master_repl_offset`和`slave_repl_offset`的差值的数据同步到从库。

==主从库间网络断了，主从库会采用增量复制的方式继续同步==，主库会把断连期间收到的写操作命令，写入`replication_buffer`，同时也会把这些操作命令也写入 `repl_backlog_buffer` 这个缓冲区，然后主库并通过`master_repl_offset` 和 `slave_repl_offset`的差值数据同步到从库。

因为`repl_backlog_buffer` 是一个环形缓冲区，当在缓冲区写满后，主库会继续写入，此时，会出现什么情况呢？

覆盖掉之前写入的操作。如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致。因此需要关注 `repl_backlog_size`参数，调整合适的缓冲空间大小，避免数据覆盖，主从数据不一致。

#### 9.9.2 Sentinel 哨兵模式

哨兵机制是实现主从库自动切换的关键机制，其主要分为三个阶段:

- 监控：哨兵进程会周期性地给所有的主从库发送PING命令，检测它们是否仍然在线运行。
- 选主（选择主库）：主库挂了以后，哨兵基于一定规则评分选举出来一个从库实例新的主库。
- 通知：哨兵会将新主库的信息发送给其他从库，让它们和新主库建立连接，并进行数据复制。**同时，哨兵会把新主库的信息广播通知给客户端，让它们把请求操作发到新主库上。**

![image-20211118232900067](E:\实习\Java_docs\pics\image-20211118232900067.png)

**其中，在监控中如何判断主库是否处于下线状态？**

- **主观下线**： **哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况，用来判断实例的状态，** 如果单哨兵发现主库或从库对 PING 命令的响应超时了，那么，哨兵就会先把它标记为“主观下线”
- 客观下线：在哨兵集群中，基于少数服从多数，多数实例都判定主库已“主观下线”，则认为主库“客观下线”。

==哨兵之间时如何互相通信的呢==

哨兵集群中哨兵实例之间可以相互发现，**基于Redis提供的发布/订阅机制（pub/sub机制）**

哨兵可以在主库中发布/订阅消息，在主库上有一个名为`\_sentinel_:hello`的频道，不同哨兵就是通过它来相互发现，实现互相通信，而且只有订阅了同一个频道的应用，才能通过发布的消息进行信息交换。

==如何选举新的主库==

- 从库的当前在线状态
- 判断它之前的网络连接状态，通过`down-after-milliseconds * num`（断开连接次数），当断开连接次数超过阈值，不适合作为新的主库。

==如何选举leader哨兵==

基于少数服从多数原则“投票仲裁”选举出来

- 当任何一个从库判定主库“主观下线”后，发送命令`s-master-down-by-addr`命令发送想要称为leader的信号。
- 其他哨兵根据于主机连接情况作出相应的相应，而且如果有多个哨兵发起请求，每个哨兵的赞成票只能投给其中一个，其他只能为反对票。

想要成为Leader 的哨兵，要满足两个条件：

- 第一，获得半数以上的赞成票；
- 第二，获得的票数同时还需要大于等于哨兵配置文件中的`quorum`值。

==选举完leader哨兵并新主库切换完毕之后，那么leader哨兵怎么通知客户端？==

基于哨兵自身的 pub/sub 功能，实现了客户端和哨兵之间的事件通知，客户端订阅哨兵自身消息频道 

##### 数据问题

###### 数据丢失-主从异步复制

因为`master` 将数据复制给`slave`是异步实现的，在复制过程中，这可能存在master有部分数据还没复制到slave，master就宕机了，此时这些部分数据就丢失了。

###### 数据丢失-脑裂

何为脑裂？当一个集群中的 master 恰好网络故障，导致与 sentinal 通信不上了，sentinal会认为master下线，且sentinal选举出一个slave 作为新的 master，此时就存在两个 master了。

可能存在client还没来得及切换到新的master，还继续写向旧master的数据，当master再次恢复的时候，会被作为一个slave挂到新的master 上去，自己的数据将会清空，重新从新的master 复制数据，这样就会导致数据缺失。

**总结：主库的数据还没有同步到从库，结果主库发生了故障，等从库升级为主库后，未同步的数据就丢失了。**

#### 数据丢失解决方案

数据丢失可以通过合理地配置参数 min-slaves-to-write 和 min-slaves-max-lag 解决，比如

- `min-slaves-to-write` 1
- `min-slaves-max-lag` 10

如上两个配置：要求至少有 1 个 slave，数据复制和同步的延迟不能超过 10 秒，如果超过 1 个 slave，数据复制和同步的延迟都超过了 10 秒钟，那么这个时候，master 就不会再接收任何请求了。

###### 数据不一致

在主从异步复制过程，当从库因为网络延迟或执行复杂度高命令阻塞导致滞后执行同步命令，这样就会导致数据不一致

解决方案： 可以开发一个外部程序来监控主从库间的复制进度（`master_repl_offset` 和 `slave_repl_offset` ），通过监控 `master_repl_offset` 与`slave_repl_offset`差值得知复制进度，当复制进度不符合预期设置的Client不再从该从库读取数据。

### 9.10 Redis分布式锁 *****************

[redis系列：基于redis的分布式锁 - 云枭zd - 博客园 (cnblogs.com)](https://www.cnblogs.com/fixzd/p/9479970.html)

#### **最基础的版本1**

假设有两个客户端A和B，A获取到分布式的锁。A执行了一会，突然A所在的服务器断电了（或者其他什么的），也就是客户端A挂了。这时出现一个问题，这个锁一直存在，且不会被释放，其他客户端永远获取不到锁。

![image-20211118202121288](E:\实习\Java_docs\pics\image-20211118202121288.png)

#### **设置锁的过期时间**

1. 客户端A获取锁成功，过期时间30秒。
2. 客户端A在某个操作上阻塞了50秒。
3. 30秒时间到了，锁自动释放了。
4. 客户端B获取到了对应同一个资源的锁。
5. 客户端A从阻塞中恢复过来，释放掉了客户端B持有的锁。

![image-20211118202316538](E:\实习\Java_docs\pics\image-20211118202316538.png)

==这时会有两个问题==

1. 过期时间如何保证大于业务执行时间?
2. 如何保证锁不会被误删除?

#### **设置锁的value**

#### **具有原子性的释放锁**

#### **确保过期时间大于业务执行时间**

### 9.11 Redis为什么这么快

- **基于内存**：Redis是使用内存存储的，没有磁盘IO上的开销，数据存在内存中，读写速度快。
- **单线程实现**（Redis6.0以前）：Redis使用单个线程处理请求，避免了多个线程之间线程切换和锁资源争用的开销。
- **IO多路复用模型**： Redis采用IO多路复用技术，Redis使用单线程来轮询描述符，将数据库的操作都转换成了事件，不在网络I/O上浪费太多的事件
- **高效的数据结构**： Redis为每种数据结构底层都做了优化，目的就是为了追求更快的速度。

### 9.12 布谷鸟过滤器

#### 布谷鸟哈希

两个不同的hash算法将新来的元素映射到数组的两个位置。如果两个位置中有一个位置为空，那么就可以直接将元素放进去。**但是**如果两个位置都满了， ==随机踢走一个==，然后自己霸占这个位置。

```java
p1 = hash1(x) % l
p2 = hash2(x) % l
```

被挤走的那个元素去查看自己的另一个位置，如果为空，就自己挪过去。如果这个位置还被别人占了，那就再来一次【鸠占鹊巢】。

布谷鸟哈希会设置一个阈值，当连续占巢行为超出了某个阈值，就认为这个数组已经几乎满了。这时候就需要对它进行扩容，重新放置所有元素。

**挤兑循环**。比如两个不同的元素，hash 之后的两个位置正好相同，这时候它们一人一个位置没有问题。但是这时候来了第三个元素，它 hash 之后的位置也和它们一样，很明显，这时候会出现挤兑的循环。

#### 优化

原始的布谷鸟哈希算法的平均空间利用率大概只有50%，改良的方案之一是增加hash函数，这样可以大大降低碰撞的概率。

另一种方案是在数组的每个位置上挂上多个多个座位，这样即使两个元素被hash在了同一个位置，也不必立即【鸠占鹊巢】，这种方案的空间利用率只有85%，但是查询效率会很高，同一个位置上的多个座位在内存空间上是连续的，可以有效利用 CPU 高速缓存。

#### 布谷鸟过滤器

它也是一维数组，但是布谷鸟哈希会存储整个元素，而布谷鸟过滤器中只会存储元素的指纹信息（几个bit，类似于布隆过滤器）。

首先布谷鸟过滤器还是只会选用两个hash函数，但是每个位置可以放置多个座位。这两个hash函数选择的比较特殊，因为过滤器只能存储指纹信息。当这个位置上的指纹被挤兑之后，它需要计算出另一个对偶位置。而计算这个对偶位置是需要元素本身的。

```java
fp = fingerprint(x)
p1 = hash1(x) % l
p2 = hash2(x) % l
//我们知道了p1和x的指纹，是没有办法直接算出p2的    
```

#### 特殊的hash函数

使得可以根据p1和元素指纹直接计算出p2，而不需要完整的x元素。

```java
fp = fingerprint(x)
p1 = hash(x)
p2 = p1 ^ hash(fp)  // 异或
p1 = p2 ^ hash(fp)  
```

**数据结构**

```java
type bucket [4]byte  // 一个桶，4个座位
type cuckoo_filter struct {
  buckets [size]bucket // 一维数组
  nums int  // 容纳的元素的个数
  kick_max  // 最大挤兑次数
}
```

```java
class Solution {
    public int integerBreak(int n) {
        int[] dp = new int[n + 1];
        for (int i = 2; i <= n; i++) {
            int curMax = 0;
            for (int j = 1; j < i; j++) {
                curMax = Math.max(curMax, Math.max(j * (i - j), j * dp[i - j]));
            }
            dp[i] = curMax;
        }
        return dp[n];
    }
}

```

## 9.13 Redis 实现分布式锁

```c++
SETNX key value
```

`setnx`是  *SET if Not eXists*的简写。

如果不存在set成功返回int的1，这个key存在了返回0。

![image-20211129153218866](E:\实习\Java_docs\pics\image-20211129153218866.png)

```c++
SETEX key seconds value
```

将值`value`关联到`key`, 并将`key`的生存时间设为`seconds`

如果`key`已经存在，`setex`命令将覆写旧值。

![image-20211129153707935](E:\实习\Java_docs\pics\image-20211129153707935.png)

## 9.14 Redis、DB数据一致性

一致性就是数据保持一致，在分布式系统中，可以理解为多个节点中数据的值是一致的。

- **强一致性**：这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响大
- **弱一致性**：这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态
- **最终一致性**：最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型

### 9.14.1 集中式redis缓存的三个经典的缓存模式

缓存可以提升性能、缓解数据库压力

- Cache-Aside Pattern
- Read-Through/Write through
- Write behind

#### 1. Cache-Aside Pattern

旁路缓存模式，它的提出是为了尽可能地解决缓存与数据库的数据不一致问题。

**Cache-Aside的读流程**

![image-20211129160302076](E:\实习\Java_docs\pics\image-20211129160302076.png)

==读的时候，先读缓存，缓存命中的话，直接返回数据==

==缓存没有的命中的话，就去读数据库，从数据库取出数据，放入缓存后，同时返回响应。==

**Cache-Aside的写流程**

<img src="E:\实习\Java_docs\pics\image-20211129160826046.png" alt="image-20211129160826046" style="zoom:67%;" />

==更新的时候，先更新数据库，然后再删除缓存==

#### 2. Read-Through/Write-Through(读写穿透)

Read/Write Through模式中，服务端把缓存作为主要数据存储。应用程序跟数据库缓存交互，都是通过抽象缓存层完成的。

**Read-Through读流程**

![image-20211129161149318](E:\实习\Java_docs\pics\image-20211129161149318.png)

==从缓存读取数据，读到直接返回==

==如果读取不到的话，从数据库加载，写入缓存后，再返回响应==

这个简要流程是不是跟Cache-Aside很像呢？

其实Read-Through就是多了一层Cache-Provider，流程如下：

<img src="E:\实习\Java_docs\pics\image-20211129161555560.png" alt="image-20211129161555560" style="zoom:65%;" />

**Write-Through写流程**

<img src="E:\实习\Java_docs\pics\image-20211129161859318.png" alt="image-20211129161859318" style="zoom:67%;" />

#### 3. Write behind(异步缓存写入)

Write behind跟Read-Through/Write-Through有相似的地方，都是由Cache Provider来负责缓存和数据库的读写。它两又有个很大的不同：Read/Write Through是同步更新缓存和数据的，Write Behind则是只更新缓存，不直接更新数据库，通过批量异步的方式来更新数据库。

<img src="E:\实习\Java_docs\pics\image-20211129162118163.png" alt="image-20211129162118163" style="zoom:67%;" />

这种方式下，缓存和数据库的一致性不强，对一致性要求高的系统要谨慎使用。

> 但是它适合频繁写的场景，MySQL的InnoDB Buffer Pool机制就使用到这种模式。

### 9.14.2 三种模式的比较

Cache Aside 更新模式实现起来比较简单，但是需要维护两个数据存储：

- 一个是缓存（Cache）
- 一个是数据库（Repository）

Read/Write Through 的写模式需要维护一个数据存储（缓存），实现起来复杂

Write Behind  更新模式和Read/Write Through 更新模式类似，区别是Write Behind  更新模式的数据持久化操作是**异步的**，但是Read/Write Through 更新模式的数据持久化操作是**同步的**。

Write Behind Caching 的优点是直接**操作内存速度快**，多次操作可以合并持久化到数据库。缺点是数据可能会丢失，例如系统断电等。

### 9.14.3 Cache-Aside的问题

##### ==更新数据的时候，是删除缓存呢，还是更新缓存==

<img src="E:\实习\Java_docs\pics\image-20211129162957074.png" alt="image-20211129162957074" style="zoom:67%;" />

操作的次序如下：

> 线程A先发起一个写操作，第一步先更新数据库
> 线程B再发起一个写操作，第二步更新了数据库

**现在，由于网络等原因，线程B先更新了缓存, 线程A再更新缓存。**

这时候，缓存保存的是A的数据（老数据），数据库保存的是B的数据（新数据），数据不一致了，脏数据出现啦。如果是删除缓存取代更新缓存则不会出现这个脏数据问题。

> 更新缓存相对于删除缓存，还有两点劣势：

> 1 如果你写入的缓存值，是经过复杂计算才得到的话。 更新缓存频率高的话，就浪费性能啦。

> 2 在写多读少的情况下，数据很多时候还没被读取到，又被更新了，这也浪费了性能呢(实际上，写多的场景，用缓存也不是很划算了)

##### ==双写的情况下，先操作数据库还是先操作缓存==

> 美团二面：Redis与MySQL双写一致性如何保证？

Cache-Aside缓存模式中，在写入请求的时候，为什么是先操作数据库呢？为什么不先操作缓存呢？
假设有A、B两个请求，请求A做更新操作，请求B做查询读取操作。

<img src="E:\实习\Java_docs\pics\image-20211129163727439.png" alt="image-20211129163727439" style="zoom:67%;" />

A、B两个请求的操作流程如下：

1. 线程A发起一个写操作，第一步del cache
2. 此时线程B发起一个读操作，cache miss
3. 线程B继续读DB，读出来一个老数据
4. 然后线程B把老数据设置入cache
5. 线程A写入DB最新的数据

> 缓存保存的是老数据，数据库保存的是新数据。因此，Cache-Aside缓存模式，选择了先操作数据库而不是先操作缓存。

### 9.14.4 redis分布式缓存与数据库的数据一致性

**重要：\*缓存是通过牺牲强一致性来提高性能的\*。**

这是由**CAP理论**决定的。缓存系统适用的场景就是非强一致性的场景，它属于CAP中的AP。

#### 3种保证数据库与缓存的一致性的方案

- 延迟双删策略
- 删除缓存重试机制
- 读取biglog异步删除缓存

#### 延迟双删

步骤：

1. 先删除缓存
2. 再更新数据库
3. 休眠一会（比如1秒），再次删除缓存

#### 删除缓存重试机制

如果第二步删除缓存失败？

删除失败就多删除几次呀,保证删除缓存成功呀~  所以可以引入删除缓存重试机制

<img src="E:\实习\Java_docs\pics\image-20211129182749426.png" alt="image-20211129182749426" style="zoom:67%;" />、

**删除缓存重试机制的大致步骤：**

- 写请求更新数据库
- 缓存因为某些原因，删除失败
- 把删除失败的key放到消息队列
- 消费消息队列的消息，获取要删除的key
- 重试删除缓存操作

#### 同步biglog异步删除缓存

[Redis与DB的数据一致性解决方案（史上最全） - 疯狂创客圈 - 博客园 (cnblogs.com)](https://www.cnblogs.com/crazymakercircle/p/14853622.html)

重试删除缓存机制会造成好多业务代码入侵。

<img src="E:\实习\Java_docs\pics\image-20211129183757654.png" alt="image-20211129183757654" style="zoom:67%;" />

# JVM虚拟机

## JVM内存区域

JVM内存区域包括PC计数器、Java虚拟机栈、本地方法栈、堆、方法区、运行时常量池、直接内存。

程序计数器、虚拟机栈、本地方法栈属于每个**线程私有的**；堆和方法区属于**线程共享访问的**。

### PC计数器

程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看作是**当前线程**所执行的**字节码行号指示器**。

### Java虚拟机栈

线程**私有**内存空间，它的生命周期和线程相同。线程执行期间，每个方法执行时都会创建一个**栈帧（Stack Frame）**，用于存储**局部变量表、操作数栈、动态链接、方法出口**等信息。

==每一个方法从调用直到执行完成的过程，就对应着一个**栈帧**在虚拟机栈中的**入栈**和**出栈**的全过程。==

####  局部变量表

**局部变量表**是一组**变量值**的存储空间，用于存储**方法参数**和**局部变量**。 在 `Class` 文件的方法表的 `Code` 属性的 `max_locals` 指定了该方法所需局部变量表的**最大容量**。

在编译期间分配内存空间，可以存放编译期的各种变量类型：

1. **基本数据类型** ：`boolean`, `byte`, `char`, `short`, `int`, `float`, `long`, `double`等`8`种；

2. **对象引用类型** ：`reference`，指向对象**起始地址**的**引用指针**；

3. **返回地址类型** ：`returnAddress`，返回地址的类型。

#### 操作数栈

**操作数栈**（Operand Stack)也称为操作栈，是一个后入先出栈。在`Class`文件的`Code`属性中的max_stacks指定了执行过程中最大的栈深度。

1. 和**局部变量表**一样，**操作数栈**也是一个以`32`**字长**为单位的数组。

2. 虚拟机在操作数栈中可存储的**数据类型**：`int`、`long`、`float`、`double`、`reference`和`returnType`等类型 (对于`byte`、`short`以及`char`类型的值在压入到操作数栈之前，也会被转换为`int`)。

3. 和**局部变量表**不同的是，它不是通过**索引**来访问，而是通过标准的**栈操作** — **压栈**和**出栈**来访问。比如，如果某个指令把一个值压入到操作数栈中，稍后另一个指令就可以弹出这个值来使用。

#### 动态链接

每个**栈帧**都包含一个指向运行时**常量池**中所属的**方法引用**，持有这个引用是为了支持方法调用过程中的动态链接。

`Class`文件的**常量池**中存在有大量的**符号引用**，字节码中的**方法调用指令**就以常量池中指向方法的**符号引用**为参数。这些符号引用：

1. 静态解析：一部分会在**类加载阶段**或第一次使用的时候转化为**直接引用**（如`final`、`static`域等），称为**静态解析**，
2. 动态解析：另一部分将在每一次的**运行期间**转化为**直接引用**，称为**动态链接**。

#### 方法返回地址

1. **正常返回**：
2. **异常返回**

### 本地方法栈

本地方法栈和Java虚拟机栈发挥的作用非常相似，主要区别是Java虚拟机栈执行的是Java方法服务，而本地方法栈执行`Native`方法服务（通常用C编写）

### 堆

Java堆是被所有**线程共享**的**最大**的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是**存放对象实例**，几乎所有的对象实例都在这里分配内存。

在Java中，堆被划分为两个不同的区域：**新生代（Young Generation）**、**老年代（Old Generation）**。新生代又被划分为3个区域： 一个Eden区和两个Survivor区 

新的对象分配是首先放在年轻代 (`Young Generation`) 的`Eden`区，`Survivor`区作为`Eden`区和`Old`区的缓冲，在`Survivor`区的对象经历若干次收集仍然存活的，就会被转移到老年代`Old`中。

### 方法区

方法区和Java堆一样，为多个线程郭襄，它用于存储**类信息、常量、静态常量、即时编译后的代码**等数据。

## 常见内存溢出异常

除了程序计数器外，Java虚拟机的其他运行时区域都有可能发生`OutOfMemoryError`的异常

### Java堆溢出

`Java`堆能够存储对象实例。通过不断地创建对象，并保证`GC Roots`到对象有可达路径来避免垃圾回收机制清除这些对象。 当对象数量到达最大堆的容量限制时就会产生`OutOfMemoryError`异常。

### 虚拟机和本地方法栈溢出

关于虚拟机栈和本地方法栈，分析内存异常类型可能存在以下两种：

- 如果现场请求的**栈深度**大于虚拟机所允许的**最大深度**，将抛出`StackOverflowError`异常。
- 如果虚拟机在**扩展栈**时无法申请到足够的**内存**空间，可能会抛出`OutOfMemoryError`异常。

