# 面经

## 1.ArrayList

### 1.1ArrayList底层实现

ArrayList是基于数组实现的，是一个动态数组。它实现了List接口，底层使用数组保存所有元素；其操作基本上是对数组的操作。

1）私有属性：

```java
//ArrayList只定义类的两个私有属性
//elementData存储ArrayList内的元素
private transient Object[] elementData;
//size表示它包含的元素的数量
private int size;
```

2）构造方法：

- 构造一个默认初始容量为10的空列表
- 构造一个指定初始容量的空列表
- 构造一个包含指定collection的元素的列表

```java
// ArrayList带容量大小的构造函数。 
public ArrayList(int initiaCapacity){
    super();
    if(initiaCapacity < 0)
       throw new IllegalArgumentException("Illegal Capacity: "+    
                                               initialCapacity);
    //新建一个数组
    this.elementData = new Object[initialCapacity];
}
//ArrayList无参构造方法，默认容量是10。
public ArrayList(){
    this(10);
}
// 创建一个包含collection的ArrayList    
public ArrayList(Collection<? extends E> c){
    elementData = c.toArray();
    size = elementData.length;
    if (elementData.getClass() != Object[].class)    
            elementData = Arrays.copyOf(elementData, size, Object[].class);  
}
```

3）元素存储

`set(int index, E element)、add(E e)、add(int index, E element)、addAll(Collection<? extends E> c)、addAll(int index, Collection<? extends E> c)`

```java
// 用指定的元素替代此列表中指定位置上的元素，并返回以前位于该位置上的元素。  
public E set(int index, E element){
    RangeCheck(index);
    
    E oldValue = (E)elementData[index];
    elementData[index] = element;
    return oldValue;
}

//将指定的元素添加到此列表的尾部
public boolean add(E e){
    ensureCapacity(size + 1);
    elementData[size++] = e;
    return true;
}

//将指定的元素插入此列表中的指定位置
//如果当前位置有元素，则向右移动当前位于该位置的元素以及所有后续元素
public void add(int index, E element){
    rangeCheckForAdd(index);
    //如果数组长度不足，将进行扩容
    ensureCapacity(size + 1); //自增+1
    System.arraycopy(elementData, index, elementData, index + 1, size - index);
    elementData[index] = element;
}
//按照指定collection的迭代器锁返回的元素顺序，将该collection中的所有元素添加到此列表的尾部
public boolean addAll(Collection<? extends E> c){
    Object[] a = c.toArray();
    int numNew = a.length;
    ensureCapacityInternal(size + numNew);  //自增+1
    System.arraycopy(a, 0, elementData, size, numNew);
    size += numNew;
    return numNew != 0;
}
```



4）元素读取

```java
public E get(int index){
    RangeCheck(index);
    return (E) elementData[index];
}
```

5）元素删除

```java
//两种方式
/*移除指定位置的元素
*/
public E remove(int index){
    RangeCheck(index);
    
    modCount++;
    E oldValue = (E)elementData[index];
    
    int numMoved = size - index - 1;
    if (numMoved > 0)
        System.arraycopy(elementData, index + 1, elementData, index, numMoved);
    elementData[--size] = null;
    
    return oldValue;
}
/*移除列表种首次出现的指定元素（如果存在）
*/
public boolean remove(Object o){
    //由于Array List种允许存放null，因此有两种情况
    // 1.
    if (o == null){
        for (int index = 0; index < size; index++)
            if(elementData[index] == null){
                //类似remove(int index)，移除列表中指定位置的元素
                fastRemove(index);
                return true;
            }
    }
    else{
        for(int index = 0; index < size; index++)
            if(o.equals(elementData[index])){
                fastRemove(index);
                return true;
            }
    }
    return false;
}
```

**fastRemove不会判断边界，因为找到元素就相当于确定了index不会超过边界，而且fastRemove并不返回被移除的元素。**



### 1.2.ArrayList底层是怎么排序的

重写sort方法

```java
@Override
public void sort(Comparator<? super E> c){
    final int expectedCount = modCount;
    Arrays.sort((E[]) elementData, 0 , size, c);
    if(modCount != expectedModCount){
        throw new ConcurrentModificationException();
    }
    modCount++;
}
//使用

arrayList.sort(Comparator.naturalOrder());
arrayList.sort(Comparator.reverseOrder());
```

### 1.3.ArrayList扩容机制

```java
private void grow(int minCapacity){
    //考虑溢出
    int oldCapacity = elementData.length;
    int newCapacity = oldCapacity + (oldCapacity >> 1); //扩大1.5倍
    if(newCapacity - minCapacity < 0)
        newCapacity = minCapacity;
    if(newCapacity - MAX_ARRAY_SIZE > 0)
        newCapacity = hugeCapacity(minCapacity);
    elementData = Arrays.copyOf(elementData, newCapacity);
}
```

## 2. LinkedList

### 2.1LinkedList的底层实现

**Node类**

```java
private static class Node<E>{
    E item;
    Node<E> next;
    Node<E> prev;
    
    Node(Node<E> prev, E element, Node<E> next){
        this.item = element;
        this.next = next;
        this.prev = prev;
    }
}
```

**属性**

```java
//节点数  transient关键字：不参与对象的序列化
transient int size = 0;
//指向第一个节点的引用
transient Node<E> first;
//指向最后一个节点的引用
transient Node<E> last;
```

**构造方法**

```java
//无参构造
public LinkedList(){
}
//集合构造
public LinkedList(Collection<? extends E> c){
    this();
    addAll(c);
}
```

**核心方法**

```java
//返回列表中指定位置的元素
public E get(int index){
    checkElementIndex(index);
    return node(index).item;
}
```

```java
//返回指定元素索引处的节点
Node<E> node(int index){
    //二分法，选择从前向后遍历还是从后向前遍历
    if (index < (size >> 1)){
        Node<E> x = first;
        for(int i = 0; i < index; i++)
            x = x.next;
    }
    else{
        Node<E> x = last;
            for (int i = size - 1; i > index; i--)
                x = x.prev;
            return x;
    }
}
```

```java
//向指定位置添加元素
public void add(int index, E element){
    checkPostionIndex(index);
    
    if(index == size)
        linkLast(element);
    else
        linkBefore(element, node(index));
}

//添加的元素是在末尾
void linkLast(E e){
    final Node<E> l = last;
    final Node<E> newNode = new Node<>(l, e, null);
    last = newNode;
    if(l == null)
        first = newNode;
    else
        l.next = newNode;
    size++;
    modCount++;
}

//正常添加
void linkBefore(E e, Node<E> succ){
    final Node<E> pred = succ.prev;
        final Node<E> newNode = new Node<>(pred, e, succ);
        succ.prev = newNode;
        if (pred == null)
            first = newNode;
        else
            pred.next = newNode;
        size++;
        modCount++;
}
```

```java
//删除元素
public E remove(int index){
    checkElementIndex(index);
    return unlink(node(index));
}

//真正的删除方法
E unlink(Node<E> x){
    final E element = x.item;
        final Node<E> next = x.next;
        final Node<E> prev = x.prev;

        if (prev == null) {
            first = next;
        } else {
            prev.next = next;
            x.prev = null;
        }

        if (next == null) {
            last = prev;
        } else {
            next.prev = prev;
            x.next = null;
        }

        x.item = null;
        size--;
        modCount++;
        return element;
}
```

## 3. HashMap

### 3.1HashMap底层实现

**关键变量**

```java
//初始容量大小  必须是2的幂
static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; //  = 16
//最大容量
static final int MAXIMUM_CAPACITY = 1 << 30;

//负载因子，因为统计学中hash冲突符合泊松分布，7-8之间冲突最小
static final float DEFAULT_LOAD_FACTOR = 0.75f;

//链表大于这个值就会树化
static final int TREEIFY_THRESHOLD = 8;

//小于这个值就会反树化
static final int UNTREEIFY_THRESHOLD = 6;
```

**构造方法**

- 指定初始容量大小，负载因子

```java
public HashMap(int initialCapacity, float loadFactor){
    if(initialCapacity < 0)
        throw new IllegalArgumentException("Illegal initial capacity: " +
                                               initialCapacity);
    if (initialCapacity > MAXIMUM_CAPACITY) //1<<30 最大容量是 Integer.MAX_VALUE;
            initialCapacity = MAXIMUM_CAPACITY;
        if (loadFactor <= 0 || Float.isNaN(loadFactor))
            throw new IllegalArgumentException("Illegal load factor: " +
                                               loadFactor);
	this.loadFactor = loadFactor;
    //tableSizeFor 可以找到最小的2的幂
    this.threshold = tableSizeFor(initialCapacity);  
}
```

- *负载因子给的默认值0.75*

```java
public HashMap(int initialCapacity) {
        this(initialCapacity, DEFAULT_LOAD_FACTOR);
    }
```

- 空参构造，均使用默认值

```java
public HashMap() {
        this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted
    }
```

- 初始化

```java
public HashMap<Map<? extends K, ? extends V> m){
    this.loadFactor = DEFAULT_LOAD_FACTOR;
    putMapEntries(m, false);
}
```

### 3.2HashMap计算Hash值

**hash方法**

```java
static final int hash(Object key){
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

`h = key.hashCode()`表示`h`是key对象的hashCode返回值；

`h >>> 16`是h又移16位，因为int是4字节，32位，所以右移16位后变成：左边16个0 + 右边原h的高16位；

**异或**：二进制位运算。如果一样返回 0，不一样则返回 1。

例：两个二进制 110 和 100 进行异或

​        	  110

​          ^ 100

结果     010

**putVal（）中寻址部分**

`tab[i = (n - 1) & hash]`

**为什么不直接用hashCode（）%length?**

**寻址为什么不用取模**

HashMap中规定了哈希表长度为2的幂，而2 ^k - 1转为二进制就是k个连续的1，那么`hash & (k个连续的1)返回的就是hash的低k个位，该计算结果就是0到2^k-1。

为什么不直接用 hashCode() 而是用它的高 16 位进行异或计算新 hash 值？

int 类型占 32 位，可以表示 2^32 种数（范围：-2^31 到 2^31-1），而哈希表长度一般不大，在 HashMap 中哈希表的初始化长度是 16（HashMap 中的 DEFAULT_INITIAL_CAPACITY），如果直接用 hashCode 来寻址，那么相当于只有低 4 位有效，其他高位不会有影响。这样假如几个 hashCode 分别是 210、220、2^30，那么寻址结果 index 就会一样而发生冲突，所以哈希表就不均匀分布了。

为了减少这种冲突，HashMap 中让 hashCode 的高位也参与了寻址计算（进行扰动），即把 hashCode 高 16 位与 hashCode 进行异或算出 hash，然后根据 hash 来做寻址。

### 3.3HashMap的put操作

![image-20211027210936780](E:\实习\Java_docs\pics\image-20211027210936780.png)

```java
public V put(K key, V value){
    //把key先hash得到hash值
    return putVal(hash(key), key, value, false, true);
}

final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict){
    //数组+链表+红黑树，链表型（Node泛型)数组，每一个元素代表一条链表，则每个元素称为桶
    //HashMap 的每一个元素，都是链表的一个节点（Entry<K,V>）这里也就是Node<K,V>
    //tab:桶 p:桶 n:哈希表数组大小 i:数组下标(桶的位置)
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    //1.判断当前是否为0，空就调用resize()方法（resize中会判断是否进行初始化）
    if((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    //2.判断是否有hash冲突，根据入参key和key的hash值找到具体的桶并且判空，
    if((p = tab[i = (n - 1) & hash]) == null)
		tab[i] = newNode(hash, key, value, null);
    //3.以下表示有冲突，处理hash冲突
    else{
        Node<K,V> e; K k; //均为临时变量
        //4.判断当前桶的key是否与入参key一致，一致则存在，把当前桶p赋值给e,覆盖原 value 在步骤10进行
        if(p.hash == hash && ((k = p.key) == key ||(key != null && key.equals(k))))
            e = p;
        //5.如果当前的桶为红黑树，用putTreeVal方法写入
        else if(p instanceof TreeNode)
            e = ((TreeNode<K,V>) p).putTreeVal(this, tab, hash, key, value);
        //当前的桶是链表 遍历链表
        else{
            for(int binCount = 0; ; ++binCount){
                if((e = p.next) == null){
                    //7.尾插法 链表下一个节点是null
                    p.next = newNode(hash, key, value, null);
                    //8.判断是否大于阈值，是否需要转成红黑树
                    if(binCount >= TreeIFY_THRESHOLD - 1)
                        treeifyBin(tab, hash);
                    break;
                }
                //9. 如果链表中key存在，则直接跳出
                if(e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        //存在相同的key的Node节点，覆盖原来的value
        if(e != null){
            V oldValue = e.value;
            if(!onlyIfAbsent || oldValue == null)
                e.value = value;
            //LinkedHashMap用到的回调方法
                    afterNodeAccess(e);
                    return oldValue;
        }
    }
    ++modCount;
    if(++size > threshold)
        resize();
    //LinkedHashMap用到的回调方法
            afterNodeInsertion(evict);
            return null;
}

/**
1、开始，入参key、value
2、判断当前table是否为空或者length=0？
    是，去扩容，（resize()方法中有判断是否初始化）
    否，根据key算出hash值并得到插入的数组的索引
        判断找到的这个table[i]是否为空？
            是，直接插入，再到步骤
            否，判断key是否存在？
                是，直接覆盖对应的value,再到步骤3
                否，去判断当前这个table[i]是不是treeNode？
                    是，使用红黑树的方式插入key、value
                    否，开始遍历链表准备插入
                        判断链表长度是不是大于8？
                            是，链表转红黑树插入key、value
                            否，以链表的方式插入key、value如果key存在就直接覆盖对应value

3、判断map的size()是否大于阈值？
	    是 就去扩容resize()
4、结束
**/

```

### 3.4HashMap的get操作

```java
public V get(Object key){
    Node<K,V> e;
    return (e = getNode(hash(key), key)) == null ? null : e.value;
}


final Node<K,V> getNode(int hash, Object key){
    Node<K,V>[] tab; Node<K,V> first, e, int n; K k;
    //1.判断当前数组不为空并且长度大于0 && 由key的hash值找到对应数组下的桶
    if((tab = table) != null &&  (n = table.length) > 0 &&
      (first = tab[(n - 1)& hash]) != null){
        //2.先判断桶的第一个节点 如果key一致 返回
        if(first.hash == hash &&
          ((k = first,key) == key || (key != null && key.equals(k))))
            return first;
        //3.再判空下一个节点不为空 && 判断是红黑树还是链表
        if((e = first.next) != null){
            //4.如果是红黑树 则按红黑树方式取值
            if(first instanceof TreeNode)
                return ((TreeNode<K,V> first).getTreeNode(hash,key));
            //否则就是链表
            do{
                if(e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k))))
                    return e;
            }
            while((e = e.next) != null);
        }
        
    }
    return null;
}
```

```java
/**1、开始，入参key
2、判断当前的数组长度不为空&&length>0
    是 return null;
    否，去判断第一个节点，如果key符合，返回
        再去判断下一个节点是否为空
            是，return null
            否，判断否是红黑树？
                是，按红黑树的方式取值
                否，遍历链表取值

3、结束
**/
```

### 3.5HashMap的resize（）

```java
final Node<K,V>[] resize(){
    Node<K,V>[] oldtab = table;
    int oldCap = (oldTab == null) ? 0 : oldTab.length;
    int OldThr = threshold;
    int newCap, newThr = 0;
    //1.原数组扩容
    if(oldCap > 0){
        //如果原数组长度大于最大容量，把阈值调最大，return
        if(oldCap >= MAXIMUM_CAPACITY){
            threshold = Integer.MAX_VALUE;
            return oldTab;
        }
        //把原数组大小、阈值都扩大一倍
        else if((newCap = oldCap << 1) < MAXIMUM_CAPACITY && 
               oldCap >= DEFAULT_INITIAL_CAPACITY)
            newThr = oldThr << 1;   现阈值 = 原阈值的两倍
    }
    //使用指定initialCapacity的构造方法，则用原阈值作为新容量
    else if(oldThr > 0)
        newCap = oldThr;
    //使用空参构造，用默认值
    else{
        newCap = DEFAULT_INITIAL_CAPACITY; //16
        newThr = (int)(DEFAULT_INITIAL_CAPACITY * DEFAULT_INITIAL_LOAD_FACTOR); //0.75*16 = 12
    }
    //使用指定了initialcapacity的构造方法，新阈值为0，则计算新的阈值
    if(newThr == 0){
        float ft = (float)newCap * loadFactor;
        newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE); 
    }
    threshold = newThr;
    //2.用新的数组容量大小初始化数组
    Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
    //如果仅仅是初始化过程，到此结束
    table = newTab;
    //3.开始扩容的主要工作，数据迁移
    if(oldTab != null){
        //遍历原数组开始复制旧数据
        for(int j = 0; j < oldCap; j++){
            Node<K,V> e;
            if((e = oldTab[j]) != null){
                oldTab[j] = null;
                //原数组中单个元素，直接复制到新表
                if(e.next == null)
                    newTab[e.hash & (newCap - 1)] = e;
                //如果该元素类型是红黑树，则按红黑树处理
                else if(e instanceof TreeNode)
                    ((TreeNode<K,V>)e.split(this, newTab,j, oldCap));
                else{
                    //先定义了两种类型的链表，以及头尾节点，高位链表和低位链表
                    Node<K,V> loHead = null, loTail = null;
                    Node<K,V> hiHead = null, hiTail = null;
                    Node<K,V> next;
                    //按顺序遍历原链表的节点
                    do{
                        next = e.next;
                        //核心判断条件，
                        // =0 则放到低位链表
                        if(e.hash & oldCap) == 0){
                            if(loTail == null)
                                loHead = e;
                            else
                                loTail.next = e;
                            loTail = e;
                        }
                        else {
                                if (hiTail == null)
                                    hiHead = e;
                                else
                                    hiTail.next = e;
                                hiTail = e;
                            }
                    }
                    while((e = next) != null);
                    //把整个低位链表放到新数组的j位置
                        if (loTail != null) {
                            loTail.next = null;
                            newTab[j] = loHead;
                        }
                        //把整个高位链表放到新数组的j+oldCap位置上
                        if (hiTail != null) {
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead;
                }
            }
        }
    }
        return newTab;
}
```

```java
/**1、开始，拿到原数组
2、对原数组扩容
	  2.1如果原数组中的容量到最大，不再扩容，return原数组
	  2.2把原数组容量大小与阈值都扩大一倍
3、如初始化用的指定initialCapacity的构造方法，则用原阈值作为新容量
4、如初始化时候用的空参构造，用默认容量与默认阈值
5、如初始化用的指定initialCapacity的构造方法，阈值=0，计算新的阈值
6、用新的容量初始化数组，如果是初始化，结束返回新数组
7、开始扩容，做数据迁移
	  7.1遍历原数组copy数据到新数组
		    7.1.1如数组中只有一个元素，则直接复制
		    7.1.2如元素是红黑树数类型，则按红黑树的方式处理
		    7.1.3对原数组的链表进行处理
				      定义一个高位链表、一个低位链表（对原链表拆分）
				      开启一个循环，遍历原链表
					        判断条件e.hash & oldCap == 0?
						          是，把这些链表节点放到低位链表
						          否，放到高位链表
				      循环结束，遍历链表完成
				      把整个低位链表放到新数组j位置
				      把整个高位链表放到新数组j+oldCap位置
	  7.2循环结束，遍历旧数组完成
8、返回新数组
**/
```



### 3.6HashMap扩容时(e.hash & oldCap) == 0推导

[让星星⭐月亮告诉你，HashMap扩容时的rehash方法中(e.hash & oldCap) == 0算法推导_Dylaniou的博客-CSDN博客](https://blog.csdn.net/u010425839/article/details/106620440)

HashMap在扩容时，需要先创建一个新数组，然后再将旧数组中的数据转移到新数组

此时，旧数组上的数据就会根据`e.hash & oldCap)`是否等于0，将数据分为2类。

① 等于0时，将该头节点放到新数组时的索引位置等于其在旧数组时的索引位置。

② 不等于0时，则将该头节点放到新数组时的索引位置等于其在旧数组时的索引位置加上旧数组长度。

**算法：** `**(e.hash & oldCap) =0**`

**前提：**

- `e.hash`代表的时旧数组中节点或元素e的hash值，该hash值时根据key确定的 `key == null ? 0 :(h = key.hashCode()) ^ (h >>> 16)`
- oldCap为旧数组的数组长度，是2的n次幂的整数。 即 `e.hash & 2 ^ n = 0`

**推导过程：**

1. 因为oldCap是2的n次幂的整数， 其二进制表达为1个1后面跟着n个0： 1000...0。若要想`e.hash & oldCap`的结果为0，则e.hash的二进制形式中与对应oldCap的二进制中的1的位置一定为0，也就是说额e.hash的**最高位为0**
2. 假设： oldCap = 2^3 = 8 = 1000

则  e.hash可以是                         0101

e.hash & oldCap                        0000   = 0

3. (2oldCap - 1) = 2^4 - 1 = 01111,其二进制位数比oldCap多一位，但多的这一位是0,其余都是1(其低三位肯定也是1);(oldCap-1)=2 ^ 3-1=0111，其二进制位数与oldCap相同,且其低3位的值都是1。故(2oldCap-1)和(oldCap -1)两者与只有4位且首位为0的e.hash=0101计算时，其实只有低3位真正能影响计算结果，而两者的低3位相同，都是111；

### 3.7HashMap为什么要使用红黑树而不使用AVL树

**红黑树**

- 根节点是黑色的
- 每个叶子节点都是黑色的空节点（NIL），叶子节点不存储数据
- 任何相邻的节点都不能同时为红色
- 每个节点，从该节点到其可达叶子节点的所有路径，都包含相同数目的黑色节点

AVL树和红黑树有几点比较和区别：
（1）AVL树是更加严格的平衡，因此可以提供更快的查找速度，一般读取查找密集型任务，适用AVL树。
（2）红黑树更适合于插入修改密集型任务。
（3）通常，AVL树的旋转比红黑树的旋转更加难以平衡和调试。

### 3.8 HashMap 的 tableSizeFor

```java
static final int tableSizeFor(int cap){
    //part1
    int n = cap - 1;
    //part2
    n |= n >>> 1;
    n |= n >>> 2;
    n |= n >>> 4;
    n |= n >>> 8;
    n |= n >>> 16;
    //part3
    return (n < 0) ? 1: (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
}
```

| **操作** | **原始值**           | **结果值** |
| -------- | -------------------- | ---------- |
| n >>> 1  | 1xxxxxxx             | 01xxxxxx   |
| 位或操作 | 1xxxxxxx \| 01xxxxxx | 11xxxxxx   |
| n >>> 2  | 11xxxxxx             | 0011xxxx   |
| 位或操作 | 11xxxxxx \| 0011xxxx | 1111xxxx   |
| n >>> 4  | 1111xxxx             | 00001111   |
| 位或操作 | 1111xxxx \| 00001111 | 11111111   |
| n >>> 8  | 11111111             | 00000000   |
| 位或操作 | 11111111 \| 00000000 | 11111111   |
| n >>> 16 | 11111111             | 00000000   |
| 位或操作 | 11111111 \| 00000000 | 11111111   |

## 4. ConcurrentHashMap

### 4.1ConcurrentHashMap底层实现

数组 + 链表 + 红黑树

- table： 默认为null，初始化发生在第一次插入操作，默认为16的数组。
- nextTable：默认为null，扩容时新生成的数组，其大小为原数组的两倍。
- sizeCtl： 默认为0，用来控制table的初始化和扩容操作
  - **-1** 代表table正在初始化
  - **-N** 表示有N - 1个线程正在进行扩容操作
- Node：保存key，value及key的hash值

其中value和next都用volatile修饰，保证并发的可见性

```java
static class Node<K,V> implements Map.Entry<K,V>{
    final int hash;
    final K key;
    volatile V val;
    volatile Node<K,V> next;
}
```

- ForwardingNode: 一个特殊的Node节点，hash值为-1，其中存储nextTable的引用。

只有table发生扩容的时候，ForwardingNode才会发挥作用，作为一个占位符放在table中表示当前节点为null或则已经被移动。

```java
final class ForwardingNode<K,V> extends Node<K,V>{
    final Node<K,V>[] nextTable;
    ForwardingNode(Node<K,V>[] tab){
        super(MOVED, null, null, null);
        this.nextTable = tab;
    }
}
```

**实例初始化**

实例化ConcurrentHashMap时倘若声明了table的容量，在初始化时会根据参数调整table大小，==确保table的大小总是2的幂次方==。默认的table大小为16.

table的初始化操作回延缓到第一put操作再进行，并且初始化只会执行一次。

```java
private final Node<K,V>[] initTable(){
    Node<K,V>[] tab; int sc;
    while((tab = table) == null || tab.length == 0){
         // 别的线程已经初始化好了或者正在初始化 sizeCtl 为 -1
        if((sc = sizeCtl) < 0)
            Thread.yield   // 让出线程的执行权
		// CAS 一下，将 sizeCtl 设置为 -1，代表抢到了锁 基本在这就相当于同步代码块
        else if(U.compareAndSwapInt(this, SIZECTL, sc, -1)){
            try{
                if((tab = table) == null || tab.length == 0){
                    // DEFAULT_CAPACITY 默认初始容量是 16
                    int n = (sc > 0) ? sc : DEFAULT_CAPACTIY;
                    // 初始化数组，长度为16或者初始化时提供的长度
                    Node<K,V>[] nt = (Node<K,V>[]) new Node<?,?>[n];
                    //将这个数组赋值给table，table时volatile的，它的写发生在别人的读之前
                    table = tab = nt;
                    //如果n为16的化，那么这里sc其实就是0.75 * n
                    sc = n -(n >>> 2);
                }
            }
            finally{
                设置下次扩容时的阈值
                sizeCtl = sc;
            }
            break;
        }
    }
    return tab;
}
```

### 4.2table扩容

什么时候会触发扩容

- 如果新增节点后，所在的链表的元素个数大于等于8，则会调用treeifyBin把链表转换为红黑树。在转换结构时，若tab的长度小于==MIN_TREEIFY_CAPACITY==，默认值为64，则会将数组长度扩大到原来的两倍，并触发`transfer`,重新调整节点位置。（只有当`tab.length >= 64, ConcurrentHashMap`才会使用红黑树。）

- 新增节点后，`addCount`统计tab中的节点个数大于阈值（sizeCtl），会触发`transfer`，重新调整节点位置。



**TreeifyBin**

```java
```

### 4.3如何在扩容时，并发地复制与插入？

1. 遍历整个table，当前节点为空，则采用CAS的方式在当前位置放入fwd
2. 当前节点已经为fwd(with hash field “MOVED”)，则已经有有线程处理完了了，直接跳过 ，这里是控制并发扩容的核心
3. 当前节点为链表节点或红黑树，重新计算链表节点的hash值，移动到nextTable相应的位置（构建了一个反序链表和顺序链表，分别放置在i和i+n的位置上）。移动完成后，用`Unsafe.putObjectVolatile`在tab的原位置赋为为fwd, 表示当前节点已经完成扩容。

### 4.4 transfer

1. 如果当前的 `nextTab` 是空，也就是说需要进行扩容的数组还没有初始化，那么初始化一个大小是原来两倍的数组出来，作为扩容后的新数组。 
2. 我们分配几个变量，来把原来的数组拆分成几个完全相同的段，你可以把他们想象成一个个大小相同的短数组，每个短数组的长度是 `stride` 。 
3. 我们先取最后一个短数组，用 `i` 表示一个可变的指针，可指向短数组的任意一个位置，最开始指向的是短数组的结尾。`bound` 表示短数组的下界，也就是开始的位置。也就是我们在短数组选择的时候是采用从后往前进行的。 
4. 然后使用了一个全局的属性 `transferIndex`（线程共享），来记录当前已经选择过的短数组和还没有被选择的短数组之间的分隔。
5. 那么当前的线程选择的这个短数组其实就是当前线程应该进行的数据迁移任务，也就是说当前线程就负责完成这一个小数组的迁移任务就行了。那么很显然在 `transferIndex` 之前的，没有被线程处理过的短数组就需要其他线程来帮忙进行数据迁移，其他线程来的时候看到的是 `transferIndex` 那么他们就会从 `transferIndex` 往前数 `stride` 个元素作为一个小数组当做自己的迁移任务。

### 4.5 ConcurrentHashMap怎么保证写数据安全

通过自旋锁 + CAS + sychronized + 分段锁

Ⅰ. 先判断散链表是否已经初始化，如果没有初始化则先初始化数组

```java
if(tab == null || (n = tab.length) == 0)
    tab = initTable();
```

Ⅱ. 向桶中加入数据，需要先判断桶中是否为空，如果为空就通过CAS算法将新增数据加入到桶中。

如果写入失败，说明其他线程已经在当前桶位写入了数据，当前线程竞争失败，回到自旋位置，进行等待。

```java
// 如果对应key的哈希值对应table数组下标的位置没有node，则通过cas操作创建一个node放入table
else if((f = tabAt(tab, i = (n - 1) & hash) == null)){
    if(casTabAt(tab,i, null, new Node<K,V>(hash, key, value,null)))
        break;
}
```

Ⅲ. 如果桶中不为空，就需要判断当前桶中头节点的类型：如果桶中头节点值为- 1则表示当前桶位的头节点为fed节点，目前散链表正处于扩容状态，这时候当前线程需要协助扩容。

```java
// 如果table正在扩容，则得到扩容后的table，然后再重新开始一个循环
else if((fh == f.hash) == MOVED) //MOVED = -1
    tab = helpTransfer(tab, f)
```

### ***很难 4.6 ConcurrentHashMap怎么获取size

[ConcurrentHashMap 1.8 计算 size 的方式 - 简书 (jianshu.com)](https://www.jianshu.com/p/971ee45597ac)

```java
//方法一 但不推荐
public int size(){
    long n = sumCount();
    return ((n < 0L) ? 0 : (n > (long)Integer.MAX_VALUE)? Integer.MAX_VALUE):
    (int)n);
}
//更推荐使用这个方法计算
public long mappingCount() {
        long n = sumCount();
        return (n < 0L) ? 0L : n; // ignore transient negative values
    }


//核心方法 sumCount
final long sumCount() {
        CounterCell[] as = counterCells; CounterCell a;
        long sum = baseCount;
        if (as != null) {
            for (int i = 0; i < as.length; ++i) {
                if ((a = as[i]) != null)
                    sum += a.value;
            }
        }
        return sum;
    }
```

==baseCount就是记录容器数量，为什么sumCount()还要遍历counterCells数组，累加对象的值呢==

```java
//counterCells是个全局的变量，表示的是CounterCell类数组。CounterCell是ConcurrentHashmap的内部类，它就是存储一个值。
private transient volatile CounterCell[] counterCells;
```

JDK1.8中使用一个volatile类型的变量baseCount记录元素的个数，当插入新数据put()或则删除数据remove()时，**会通过addCount()方法更新baseCount:**

```java
```

### 4.7 为什么 ConcurrentHashMap 的读操作不需要加锁？

volatile关键字来保证可见性、有序性。但不保证原子性。

```java
static class Node<K,V> implements Map.Entry<K,V> {
        final int hash;
        final K key;
    
    //Node的值 用了volatile修饰
        volatile V val;
        volatile Node<K,V> next;
```

- 数组用volatile修饰主要是保证在数组扩容的时候保证可见性。

## 5.并发

### 5.1volatile



Java线程安全（volatile & synchronized）

- volatile 不能保证线程安全而 synchronized 可以保证线程安全。

- volatile 只能保证被其修 饰变量的内存可见性，但如果对该变量执行的是非原子操作线程依旧是不安全的。
- synchronized 既可以保证其修饰范围内存可见性和操作的原子性，所以 synchronized 是线程安全的

![image-20211028210722121](E:\实习\Java_docs\pics\image-20211028210722121.png)

保证了共享变量的可见性，可见性 就是在一个线程修改一个共享变量的时候，另一个线程可以看到修改后的值

线程对 volatile 变量的修改会立刻被其他 线程所感知，即不会出现数据脏读的现象，从而保证数据的“可见性”

### 5.2 进程、线程的区别

**进程** 

一个在内存中运行的应用程序。每个进程都有自己独立的一块内存空间，一个进程可以有多个线程，比如在Windows系统中，一个运行的xx.exe就是一个进程。

**线程**

进程中的一个执行任务（控制单元），负责当前进程中程序的执行。一个进程至少有一个线程，一个进程可以运行多个线程，多个线程可共享数据。

与进程不同的是同类的多个线程共享进程的**堆**和**方法区**资源，但每个线程有自己的**程序计数器**、**虚拟机栈**和**本地方法栈**，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。



==根本区别==：进程是操作系统资源分配的基本单位，而线程是处理器任务调度和执行的基本单位

![image-20211028223308797](E:\实习\Java_docs\pics\image-20211028223308797.png)

### 5.3 死锁出现的原因和如何避免

是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象。

==产生的四个必要条件==

**互斥条件：**资源是独占的且排他使用，进程互斥使用资源，即任意时刻一个资源只能给 一个进程使用，其他进程若申请一个资源，而该资源被另一进程占有时，则申请者等待 直到资源被占有者释放。

**不可剥夺条件：**进程所获得的资源在未使用完毕之前，不被其他进程强行剥夺，而只能 由获得该资源的进程资源释放

**请求和保持条件：**进程每次申请它所需要的一部分资源，在申请新的资源的同时，继续 占用已分配到的资源

**环路等待条件：**在发生死锁时必然存在一个进程等待队列{P1,P2,…,Pn},其中 P1 等待 P2 占有的资源，P2 等待 P3 占有的资源，…，Pn 等待 P1 占有的资源，形成一个进程等待 环路，环路中每一个进程所占有的资源同时被另一个申请，也就是前一个进程占有后一 个进程所深情地资源

==如何解决死锁==

*预防死锁*

*避免死锁*

*检测死锁*

*解除死锁*

### 5.4 sychronized

synchronized 是 Java 的关键字，是一种同步锁，==被 synchronized 修饰的代码块及 方法，在同一时间，只能被单个线程访问==。

三种使用场景

- 修饰普通同步方法： 锁是当前实例对象
- 修饰静态同步方法： 锁是当前的类Class对象
- 修饰同步代码块： 锁是synchronized后面括号里配置的对象，



**锁升级**

==偏向锁==:认为锁不存在多线程竞争，总是由同一线程获得。。所以当一个线程获取锁的时候，会在对象头和栈帧中的锁记录里存放锁偏向的线程 ID，之后该线程进入和退出该同步 代码块不需要通过 cas 操作获得锁和释放锁，只需要比较对象头内存储的线程 ID 是不是当 前线程，是的话就获得了锁。

**当多个线程竞争锁时，偏向锁就会撤销，偏向锁撤销之后会升级为轻量级锁**

==轻量级锁==：线程在执行同步块之前，JVM 会先在当前线程的栈桢中创建用于存储锁记录的空间，并 将对象头中的 Mark Word 复制到锁记录中，官方称为 Displaced Mark Word。

然后线程尝试 使用 CAS 将对象头中的 Mark Word 替换为指向锁记录的指针。

==重量级锁==：如果经过多次 cas 还是获得不了锁，那么就升级为重量级锁，线程阻塞，将 线程在操作系统层面挂起。从用户态进入内核态。

### 5.5 AQS源码

[AQS源码详细解读 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/65349219)

```
AbstractQueuedSynchronizer  //队列同步器
```

AQS是用来构建锁和其他同步组件的基础框架

####  5.5.1 成员变量

```java
private volatile int state;

//设置期望值，想修改的值，通过CAS操作实现
protected final boolean compareAndSetState(int expect, int update){
    return unsafe.compareAndSwapInt(this, stateOffset,expect, update);
}
//维护了等待队列（也叫CHL队列，同步队列）的头节点和尾节点
private transient volatile Node head;
private transient volatile Node tail;
//CHL队列由链表实现，以自旋的方式获取资源，是可阻塞的先进先出的双向队列，通过自旋和CAS操作保证节点插入和移除的原子性，当有线程获取锁失败，就被添加到队列==末尾==
```

#### 5.5.2 内部类Node

AQS的工作模式分为独占模式和共享模式，记录在节点的信息中。

一般地，它的实现类只实现一种模式，ReentrantLock就实现了独占模式；但也有例外，ReentrantReadAndWriteLock实现了独占模式和共享模式。

```java
//当前节点处于共享模式的标记
static final Node SHARED = new Node();
//当前节点处于独占模式的标记
static final Node EXCLUSIVE  = null;
//线程被取消了
static final int CANCELLED =  1;
//释放资源后需唤醒后继节点
static final int SIGNAL    = -1;
//等待condition唤醒
static final int CONDITION = -2;
//工作于共享锁状态，需要向后传播，
//比如根据资源是否剩余，唤醒后继节点
static final int PROPAGATE = -3;

//等待状态，有1,0,-1,-2,-3五个值。分别对应上面的值
volatile int waitStatus;

//前驱节点
volatile Node prev;

//后继节点
volatile Node next;

//等待锁的线程
volatile Thread thread;

//等待条件的下一个节点，ConditonObject中用到
Node nextWaiter;
```

#### 5.5.3 获取资源（锁）

获取释放资源是对state变量的修改，

获取锁的方法有**acquire(),acquiredShared()**.

- acquire()独占模式获取资源，忽略中断

```java
public final void acquire(int arg){
    if(!tryAcquire(arg) && 
      //让线程处于一种自旋状态
      //尝试让该线程重新获取锁！ 当条件满足获取到了锁可以从自旋过程中退出，否则继续
      acquireQueued(addWait(Node.EXCULSIVE), arg))
      selfInterrrupt();
}
```

addWaiter()：将当前线程插入至队尾，返回在等待队列中的节点（就是处理了它的前驱后继）

```java
  private Node addWaiter(Node mode) {
        //把当前线程封装为node,指定资源访问模式
        Node node = new Node(Thread.currentThread(), mode);
        // Try the fast path of enq; backup to full enq on failure
        Node pred = tail;
        //如果tail不为空,把node插入末尾
        if (pred != null) {
            node.prev = pred;
            //此时可能有其他线程插入,所以使用CAS重新判断tail
            if (compareAndSetTail(pred, node)) {
                pred.next = node;
                return node;
            }
        }
        //如果tail为空，说明队列还没有初始化，执行enq()
        enq(node);
        return node;
    }
```

### 5.6 Reentrantlock

ReentrantLock重入锁，是实现Lock接口的一个类，也是在实际编程中使用频率很高的一个锁，**支持重入性，表示能够对共享资源能够重复加锁，即当前线程获取该锁再次获取不会被阻塞**。

#### 5.6.1 重入的实现原理

要解决的问题 

1. 在线程获取锁的时候，如果已经获取锁的线程是当前线程的话则直接再去获取成功。
2. 由于锁会被获取N次，那么只有在锁在被释放同样的N次之后，该锁才算是完全释放成功。

以非公平锁为例，判断当前线程是否能获得锁为例，核心方法为nofiarTryAcquire：

```java
final boolean nofairTryAcquire(int acquires){
    //获取当前线程
    final Thread current = Thread.currentThread;
    int c = getState;
    //1.如果该锁未被任何线程占用，该锁直接被当前线程获取
    if(c == 0){
        if(compareAndSetState(0, acquires)){
            setExclusiveOwnerThread(current);
            return true;
        }
    }   
    //若被占用，检查占用线程是否是当前线程
    else if(current == getExclusiveOwnerThread)){
        //如果还是当前线程，再次获取，计数加一
        int nextc = c + acquires;
        if(nextc < 0){
            throw new Error();
        }
        setState(nextc);
        return true;
    }
    return false;
}
```

释放锁：

```java
protected final boolean tryRelease(int releases){
    //1.同步状态减1
    int c = getState() - releases;
    if(Thread.currentThread() != getExclusiveOwnerThread())
        throw Exception();
    boolean free = false;
    if(c == 0){
        //2.只有当同步状态为0时，锁成功被释放，返回true
        free = true;
        setExclusiveOwnerThread(null);
    }
    //锁未被完全释放，返回false;
    setState(c);
    return free;
}
```

#### 5.6.1 公平锁和非公平锁

公平性是针对获取锁而言的，如果一个锁是公平的，那么锁的获取顺序就应该符合请求上的绝对时间顺序，满足FIFO。ReentrantLock的构造方法无参时构造非公平锁：

```java
public ReentrantLock(){
    sync = new NofairSync();
}
//第二种，传入一个boolean类型，true为公平锁
public ReentrantLock(boolean fair){
    sync = fair ? new FairSync(): new NofairSync();
}
```

**公平锁 VS 非公平锁**

1. 公平锁每次获取到锁为同步队列中的第一个节点，**保证请求资源时间上的绝对顺序**，而非公平锁有可能刚释放锁的线程下次继续获取该锁，则有可能导致其他线程永远无法获取到锁，**造成“饥饿”现象**。

1. 公平锁为了保证时间上的绝对顺序，需要频繁的上下文切换，而非公平锁会降低一定的上下文切换，降低性能开销。因此，ReentrantLock默认选择的是非公平锁，则是为了减少一部分上下文切换，**保证了系统更大的吞吐量**。

```java
//非公平锁
static final class NonfairSync extends Sync{
    private static final long serialVersionUID = ;
    final void lock(){
        //CAS设置state值为1
        if(compareAndSetState0(0, 1))
            //CAS成功说明获取到锁，此时将当前线程设为独占模式下锁对象的持有者
            setExclusiveOwnerThread(Thread.currentThread());
        else
           //CAS失败
            //可能是同一线程再去获取锁
            //
            acquire(1);
    }
}
```

通过 CAS 操作， 设置 state = 1

若 CAS 操作成功，则将当前线程设置为独占模式锁对象的持有者

若 CAS 操作失败, 最终会调用 sync 的方法 nonfairTryAcquire; 此时说明可能是同一线程再次尝试获取锁，也有可能是其他线程尝试获取锁

若当前 state == 0, 继续执行前两步操作

若当前 state != 0, 则判断当前线程是否为锁的持有者；若判断成立，则对 state + 1

### 5.7 Lock接口

实现类： ReentrantLock，ReentrantReadWriteLock

简单使用：

```java
Lock Lock = new ReentrantLock();
lock.lock();
try{
    
}finally{
    lock.unlock();
}
//最好不要把获取锁的过程写在try语句块中，因为如果在获取锁时发生了异常，异常抛出的同时也会导致锁无法被释放
```

Condition newCondition()

==Condition接口==

synchronized关键字与wait()和notify/notifyAll()方法相结合可以实现等待/通知机制

ReentrantLock需要借助于Condition接口与newCondition（）方法，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。

```java
//使用Condition实现等待、通知机制
//使用单个Condition实例实现等待/通知机制

```

## 6. 线程池

### 6.1 线程池的几个主要参数

```java
public ThreadPoolExecutor(int corePoolSize,
                         int maxmumPoolsize,
                         long keepAliveTime,
                         TimeUnit unit,
                         BlockingQueue<Runnable workQueue,
                         ThreadFactory threadFactory,
                         RejectedExecutionHandler handler) 
```

corePoolSize: 线程池中核心线程的数量

maximumPoolSize: 线程池中最大线程数量

keepAliveTime：非核心线程的存活时间 

TimeUnit unit：存活时间单位 

workQueue：任务队列 

threadFactory：线程工厂，用于创建线程，一般用默认的即可 

handler：拒绝策略，当队列满了并且工作线程大于等于线程池的最大线程数

### 6.2 线程池的执行流程

![image-20211108220501033](E:\实习\Java_docs\pics\image-20211108220501033.png)

1） 在创建线程池后，等待提交过来的任务请求。

2） 当调用execute()方法添加一个请求任务时，线程池会做如下判断：

- 如果正在运行的线程数量小于corePoolSize，那么马上创建核心线程运行这个线程。
- 如果正在运行的线程数量大于或者等于corePoolSize，那么将这个任务放入任务队列中
- 如果任务队列满了且正在运行的线程数量小于maximumPoolSize(最大线程数)，那么创建一个非核心线程立刻运行这个任务；

- 如果任务队列满了且正在运行的线程数量大于或等于maximumPoolSize，线程池会执行拒绝策略；

3） 当一个线程完成任务时，会在队列中取下一个任务来执行；

### 6.3 任务拒绝策略

线程池有一个最大的容量，当线程池的任务缓存队列已满，并且线程池中的线程数目达到maximumPoolSize时，就需要拒绝该任务。

拒绝策略是一个接口：

```java
public interface RejectedExecutionHandler{
    void rejectedExecution(Runnable r, ThreadPoolExecutor, executor);
}
//可以通过实现这个接口去定制拒绝策略，也可以选择JDK提供的4种已有策略

```

![image-20211109195944246](E:\实习\Java_docs\pics\image-20211109195944246.png)

### 6.4 线程池的创建方式

```java
ThreadPoolExecutor//：最原始的创建线程池的方式，它包含了 7 个参数可供设置
//1. 创建一个固定大小的线程池
newFixedThreadPool(int nThreads)
//2. 创建单个线程的线程池
newSingleThreadExecutor()
//3. 创建一个可缓存的线程池，若线程数超过处理所需，缓存一段时间后会回收，若线程数不够，则新建线程。
newCachedThreadPool()
//4. 创建一个可以执行延迟任务的线程池。
newScheduledThreadPool(int corePoolSize)
```

## 7. 垃圾回收GC

### 7.1 收集对象

**识别垃圾**

- 引用计数法（reference counting）：对每个对象的引用进行计数，每当有一个地方引用它时计数器 +1、引用失效则 -1，引用的计数放到对象头中，大于 0 的对象被认为是存活对象。
- **可达性分析，又称引用链法（Tracing GC）：** 从 GC Root 开始进行对象搜索，可以被搜索到的对象即为可达对象，此时还不足以判断对象是否存活/死亡，需要经过多次标记才能更加准确地确定，整个连通图之外的对象便可以作为垃圾被回收掉。



**收集算法**

- **Mark-Sweep（标记-清除）：** 回收过程主要分为两个阶段，第一阶段为追踪（Tracing）阶段，即从 GC Root 开始遍历对象图，并标记（Mark）所遇到的每个对象，第二阶段为清除（Sweep）阶段，即回收器检查堆中每一个对象，并将所有未被标记的对象进行回收，整个过程不会发生对象移动。
- **Mark-Compact （标记-整理）：** 这个算法的主要目的就是解决在非移动式回收器中都会存在的碎片化问题，也分为两个阶段，第一阶段与 Mark-Sweep 类似，第二阶段则会对存活对象按照整理顺序（Compaction Order）进行整理。
- **Copying（复制）：** 将空间分为两个大小相同的 From 和 To 两个半区，同一时间只会使用其中一个，每次进行回收时将一个半区的存活对象通过复制的方式转移到另一个半区。

### 7.2 收集器

![image-20211109204207001](E:\实习\Java_docs\pics\image-20211109204207001.png)

#### 7.2.1 ParNew



#### 7.2.2 CMS

以获取最短回收停顿时间为目标，采用**“标记-清除”**算法，分 4 大步进行垃圾收集，其中初始标记和重新标记会 STW ，多数应用于互联网站或者 B/S 系统的服务器端上，JDK9 被标记弃用，JDK14 被删除。



1. 初始标记（initial mark)

2) 并发标记（concurrent mark）
3) 重新标记（remark）
4) 并发清除（concurrent sweep）



**初始标记、重新标记**这两个步骤仍然需要“Stop the World"，初始标记仅仅只是标记一下GC Roots能直接关联到的对象。

**并发标记**阶段就是从GC Roots的直接关联对象开始遍历整个对象图的过程，可以并发进行。

**重新标记**阶段则是为了修正并发标记期间，产生变动的那一部分对象的标记记录。

**并发清除**，清理删除掉标记阶段判断的已经死亡的对象，由于不需要移动存放对象，也是可以并发的。

==CMS存在的问题==

1. CMS收集器对处理器资源非常敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程而导致应用程序变慢，降低总吞吐量。
2. **CMS无法处理浮动垃圾**。有可能出现”Con-current Mode Failure“而导致Full GC的产生。
3. 由于是基于”标记-清除“算法实现的收集器，可能会有大量的空间碎片产生。



```java
//参数
//并发执行初始标记
-XX:+CMSParallelInitialMarkEnabled
//当此阶段耗时较长的时候，可以加入参数,在重新标记之前，先执行一次young GC，回收掉年轻代的对象无用的对象。
-XX:CMSScavengeBeforeRemark
```



==使用时的注意点==

- **减少remark阶段的停顿**

一般CMS的GC耗时80%都在remark阶段，如果发现remark阶段停顿事件很长，可以先进行一下Young GC，目的在于减少年轻代对老年代的无效引用，降低remark时的开销。

- **内存碎片问题**

CMS是基于标记-清除算法的，所以会产生内存碎片，这时候需要参数 `-XX:CMSFullGCsBeforeCompaction=n`意思是说上一次CMS并发GC执行过后，还要再执行多少次Full GC才会做压缩。

- **concurrent mode failure**

```java
-XX:+UseCMSInitiatingOccupancyOnly
-XX:CMSInitiatingOccupancyFraction=70 //是指设定CMS在对内存占用率达到70%的时候开始GC
```





#### 7.2.3 G1收集器

一种服务器端的垃圾收集器，应用在多处理器和大容量内存环境中，在实现高吞吐量的同时，尽可能地满足垃圾收集暂停时间的要求。

### 7.3 新生代和老年代

新生代又被称为Eden、from survivor、to survior

JVM每次只会使用Eden和其中一块survivor来为对象服务，所以无论什么时候，都会有一块survivor空间，因此新生代实际可用空间为90%。

**新生代GC**（minor gc）：指发生在新生代的垃圾回收动作，因为Java对象大多数都是“朝生夕死”的特性，所以minor GC非常频繁，使用复制算法快速的回收。

## 8. Mysql

### 8.1 四大隔离级别

**事务**： 由一个有限的数据库操作序列构成，这些操作要么全部执行，要么全部不执行，是一个不可分割的工作单位。

**事务的特性**

- **原子性**： 事务作为一个整体被执行，包含在其中的对数据库的操作要么全部都执行，要么都不执行。
- **一致性**： 指在事务开始之前和事务结束以后，数据不会被破坏，假如A账户给B账户转10块钱，不管成功与否，A和B的总金额是不变的。
- **隔离性**：多个事务并发访问时，事务之间是相互隔离的，一个事务不应该被其他事务干扰，多个并发事务之间要相互隔离。
- **持久性**：表示事务完成提交后，该事务对数据库所作的操作更改，将持久地保存在数据库中。

### 8.2 存在的问题

==**脏读 （dirty read）**==

假设现在有两个事务A、B：

- 假设现在A的余额是100，事务A正在准备查询Jay的余额
- 这时候，事务B先扣减Jay的余额，扣了10
- 最后A 读到的是扣减后的余额



==**不可重复读（unrepeatable read）**==

假设现在有两个事务A和B：

- 事务A先查询Jay的余额，查到结果是100
- 这时候事务B 对Jay的账户余额进行扣减，扣去10后，提交事务
- 事务A再去查询Jay的账户余额发现变成了90

==**幻读**==

假设现在有两个事务A、B：

- 事务A先查询id大于2的账户记录，得到记录id=2和id=3的两条记录
- 这时候，事务B开启，插入一条id=4的记录，并且提交了
- 事务A再去执行相同的查询，却得到了id=2,3,4的3条记录了。

### 8.3 事务的隔离级别

- 读未提交（Read Uncommitted）
- 读已提交（Read Committed）
- 可重复读（Repeatable Read）
- 串行化（Serializable）

[一文彻底读懂MySQL事务的四大隔离级别 - 掘金 (juejin.cn)](https://juejin.cn/post/6844904115353436174)

#### 8.3.1 读未提交

```sql
set session transaction isolation level read uncommitted;
//开启事务A
begin;
select * from account where id = 1;
//开启事务B
begin;
update account set balance = labance + 20 where id = 1;
//回到事务A
select * from account where id = 1;
//会看到 balance + 20
```



不能解决脏读问题。

#### 8.3.2 已提交读

```sql
set session transaction isolation level read committed;
//开启事务A
begin;
select * from account where id = 1;
//开启事务B
begin;
update account set balance = balance + 20 where id = 1;
//回到事务A，数据没有改变
select * from account where id = 1;
//到事务B执行事务
commit;
// 再回到事务A查询，发现数据变成balance + 20
```

可以解决脏读问题，但是不能解决**不可重复读**

#### 8.3.3 可重复读

![image-20211111203331561](E:\实习\Java_docs\pics\image-20211111203331561.png)

#### 8.3.4串行化（Serializable）

### 8.4 隔离级别的实现原理

- 读写锁
- 一致性快照， MVCC

Repeatable Read、Read Committed的实现与MVCC有关， Read Uncommitted、串行化与锁有关。



**读未提交**

==SELECT statements are performed in a nonlocking fashion, but a possible earlier version of a row might be used. Thus, using this isolation level, such reads are not consistent.==

读未提交，采取的是读不加锁原理。

- 事务读不加锁，不阻塞其他事务的读和写
- 事务写阻塞其他事务写，但不阻塞其他事务读；

**串行化（Serializable)**

- 所有SELECT语句会隐式转化为`SELECT ... FOR SHARE`，即加共享锁。
- 读加共享锁，写加排他锁，读写互斥。如果有未提交的事务正在修改某些行，所有select这些行的语句都会阻塞。

##### MVCC的实现原理

MVCC，中文叫**多版本并发控制**，它是通过读取历史版本的数据，来降低并发事务冲突，从而提高并发性能的一种机制。它的实现依赖于**隐式字段、undo日志、快照读&当前读、Read View**

###### **隐式字段**

对于InnoDB存储引擎，每一行记录都有两个隐藏列**DB_TRX_ID、DB_ROLL_PTR**，如果表中没有主键和非NULL唯一键时，则还会有第三个隐藏的主键列**DB_ROW_ID**。

- DB_TRX_ID，记录每一行最近一次修改（修改/更新）它的事务ID，大小为6字节；
- DB_ROLL_PTR，这个隐藏列就相当于一个指针，指向回滚段的undo日志，大小为7字节；
- DB_ROW_ID，单调递增的行ID，大小为6字节；

###### undo日志

- 事务未提交的时候，修改数据的镜像（修改前的旧版本），存到undo日志里。以便事务回滚时，恢复旧版本数据，撤销未提交事务数据对数据库的影响。

- undo日志是逻辑日志。可以这样认为，当delete一条记录时，undo log中会记录一条对应的insert记录，当update一条记录时，它记录一条对应相反的update记录。

- 存储undo日志的地方，就是**回滚段**。

多个事务并行操作某一行数据时，不同事务对该行数据的修改会产生多个版本，然后通过回滚指针（DB_ROLL_PTR）连一条**Undo日志链**。

==举例==

- 假设表accout现在只有一条记录，插入该该记录的事务Id为100
- 如果事务B（事务Id为200），对id=1的该行记录进行更新，把balance值修改为90

![image-20211111223840302](E:\实习\Java_docs\pics\image-20211111223840302.png)

###### 快照读&当前读

**快照读：**

读取的是记录数据的可见版本（有旧的版本），不加锁,普通的select语句都是快照读,如：

```sql
select * from account where id > 2;
```

**当前读：**

读取的是记录数据的最新版本，显示加锁的都是当前读

```sql
select * from account where id>2 lock in share mode;
select * from  account where id>2 for update;
```

###### Read View

- Read View就是事务执行**快照读**时，产生的读视图。
- 事务执行快照读时，会生成数据库系统当前的一个快照，记录当前系统中还有哪些活跃的读写事务，把它们放到一个列表里。
- Read View主要是用来做可见性判断的，即判断当前事务可见哪个版本的数据



- **m_ids:当前系统中那些活跃的读写事务ID,它数据结构为一个List。**
- **min_limit_id:m_ids事务列表中，最小的事务ID**
- **max_limit_id:m_ids事务列表中，最大的事务ID**

如果DB_TRX_ID < min_limit_id，表明生成该版本的事务在生成ReadView前已经提交(因为事务ID是递增的)，所以该版本可以被当前事务访问。

如果DB_TRX_ID > m_ids列表中最大的事务id，表明生成该版本的事务在生成ReadView后才生成，所以该版本不可以被当前事务访问。

如果 min_limit_id =<DB_TRX_ID<= max_limit_id,需要判断m_ids.contains(DB_TRX_ID)，如果在，则代表Read View生成时刻，这个事务还在活跃，还没有Commit，你修改的数据，当前事务也是看不见的；如果不在，则说明，你这个事务在Read View生成之前就已经Commit了，修改的结果，当前事务是能看见的。

##### 锁相关

###### 共享锁与排他锁

InnoDB 实现了标准的行级锁，包括两种：共享锁（简称 s 锁）、排它锁（简称 x 锁）。

- 共享锁（S锁）：允许持锁事务读取一行。
- 排他锁（X锁）：允许持锁事务更新或者删除一行。

如果事务 T1 持有行 r 的 s 锁，那么另一个事务 T2 请求 r 的锁时，会做如下处理：

- T2 请求 s 锁立即被允许，结果 T1 T2 都持有 r 行的 s 锁
- T2 请求 x 锁不能被立即允许

如果 T1 持有 r 的 x 锁，那么 T2 请求 r 的 x、s 锁都不能被立即允许，T2 必须等待T1释放 x 锁才可以，因为X锁与任何的锁都不兼容。

###### 记录锁（Record Locks）

- 记录锁是最简单的行锁，**仅仅锁住一行**。如：`SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE`

c1 为 10 的记录行会被锁住。

==需要注意的是：`id` 列必须为`唯一索引列`或`主键列`，否则上述语句加的锁就会变成`临键锁`。==

==同时查询语句必须为`精准匹配`（`=`），不能为 `>`、`<`、`like`等，否则也会退化成`临键锁`==

- 记录锁**永远都是加在索引上**的，即使一个表没有索引，InnoDB也会隐式的创建一个索引，并使用这个索引实施记录锁。
- 会阻塞其他事务对其插入、更新、删除

在通过 `主键索引` 与 `唯一索引` 对数据行进行 UPDATE 操作时，也会对该行数据加`记录锁`：

```sql
-- id 列为主键列或唯一索引列
update set age = 50 where id = 1;
```



###### 间隙锁（Gap Locks）

- 间隙锁是一种**加在两个索引之间的锁，或者加在第一个索引之前，或最后一个索引之后的间隙**。

- 使用**间隙锁锁住的是一个区间**，而不仅仅是这个区间中的每一条数据。

- 间隙锁只阻止其他事务插入到间隙中，他们不阻止其他事务在同一个间隙上获得间隙锁，所以 gap x lock 和 gap s lock 有相同的作用。

```sql
select * from table where id between 1 and 10 for update;
```

即所有在`（1，10）`区间内的记录行都会被锁住，所有id 为 2、3、4、5、6、7、8、9 的数据行的插入会被阻塞，但是 1 和 10 两条记录行并不会被锁住。



###### 临键锁（Next-Key Locks）

 每个数据行上的`非唯一索引列`上都会存在一把**临键锁**，当某个事务持有该数据行的**临键锁**时，会锁住一段**左开右闭区间**的数据。

`InnoDB` 中`行级锁`是基于索引实现的，**临键锁**只与`非唯一索引列`有关，在`唯一索引列`（包括`主键列`）上不存在**临键锁**。

| id   | age  |  name  |
| ---- | ---- | :----: |
| 1    | 10   |  Lee   |
| 3    | 24   | Soraka |
| 5    | 32   |  Zed   |
| 7    | 45   | Talon  |

该表中 `age` 列潜在的`临键锁`有：

| (-∞, 10] |
| :------: |
| (10, 24] |
| (24, 32] |
| (32, 45] |
| (45, +∞] |

在事务A中执行：

```sql
-- 根据非唯一索引列update某条记录
update table set name = Vladimir where age = 24;
-- 根据非唯一索引列 锁住某条记录
select * from table where age = 24 for update;
```

事务B中执行：

```sql
-- 命令会被阻塞
insert into table values(100,26, 'Ezreal');
```

### 8.5 主从复制

主从复制是指将主数据库的DDL和DML操作通过二进制日志传到从数据库上，然后再从数据库上对这些日志进行重新执行，从而使从数据库和主数据库的数据保持一致。

#### 8.5.1 主从复制的原理

- MySql主库在事务提交时会把数据变更作为事件记录在二进制日志Binlog中；
- 主库推送二进制日志文件Binlog中的事件到从库的中继日志Relay Log中，之后从库根据中继日志重做数据变更操作，通过逻辑复制来达到主库和从库数据的一致性。
- MySql通过三个线程完成主从库间的数据复制，其中BinLog Dump线程跑在主库上，I/O线程和SQL线程

跑在从库上。

- 当在从库上启动复制时，首先创建I/O线程连接主库，主库随后创建Binlog Dump线程读取数据库事件并发送给I/O线程，I/O线程获取到事件数据后更新到从库的中继日志Relay Log中去，之后从库上的SQL线程读取中继日志Relay Log中更新的数据库事件并应用。

![image-20211112202602539](E:\实习\Java_docs\pics\image-20211112202602539.png)

### 8.6 临时表

MySql在执行SQL语句的时候会临时创建一些存储中间结果集的表，这种表被称为**临时表**，临时表只对当前连接可见，在连接关闭后，临时表会被删除并释放空间。

临时表主要分为内存临时表和磁盘临时表两种。内存临时表使用的是MEMORY存储引擎，磁盘临时表使用的是MyISAM存储引擎。

会产生临时表的情况：

- From中的子查询
- Distinct 查询并加上Order by
- Order by 和 Group by 的子句不一样时
- 使用Union查询

### 8.7 慢查询

#### 8.7.1 慢查询日志

1. MySql的慢查询日志是MySql提供的一种日志记录，它用来记录MySql中查询时间超过设置阈值（long_query_time)的语句，记录到慢查询日志中。
2. long_query_time的默认值是10

#### 8.7.2 开启慢查询日志

**默认情况下，MySql没有开启慢查询日志**。需要手动开启。

```sql
-- 查看慢查询日志是否开启
show variables like '%slow_query_log%'
-- 开启慢查询日志，只对当前数据库生效，并且重启数据库后失效
set global slow_query_log = 1;
-- 查看慢查询日志的阈值，默认为10s
shown variables like '%long_query_time%';
-- 设置阈值
set long_query_time = 3;

```



#### 8.7.3 对慢查询优化

- 分析语句的执行情况，查询SQL语句的索引是否命中
- 优化数据库的结构，将字段很多的表分解成多个表，或者考虑建立中间表
- 优化LIMIT 分页

### 8.8 SQL的执行顺序

```sql
SELECT DISTINCT     select_list FROM     left_table LEFT JOIN     right_table ON join_condition WHERE     where_condition GROUP BY     group_by_list HAVING     having_condition ORDER BY     order_by_condition
```

![image-20211113202004093](E:\实习\Java_docs\pics\image-20211113202004093.png)

### ThreadLocal

**基本使用**

```java
//创建一个ThreadLocal对象
private ThreadLocal<Integer> localInt = new ThreadLocal<>();
// 上述代码创建一个localInt变量，由于ThreadLocal是一个泛型类，这里指定了localInt的类型为整数。

//如何设置和获取这个变量的值
public int setAndGet(){
    localInt.set(8);
    return localInt.get();
}
/***
上述代码设置变量的值为8，接着取得这个值。
由于ThreadLocal里设置的值，只有当前线程自己看得见，这意味着你不可能通过其他线程为它初始化值。
ThreadLocal提供了一个withInitial()方法统一初始化所有线程的ThreadLocal的值
***/
private ThreadLocal<Integer> localInt = ThreadLocal.withInitial(() -> 6);
//上述代码将ThreadLocal的初始值设置为6，这对全体线程都是可见的。
```

#### 实现原理

```java
public T get(){
    //获取当前线程
    Thread t = Thread.currentThread();
    //每个线程 都有一个自己的ThreadLocalMap
    //ThreadLocalMap里就保存着所有的ThreadLocal变量
    ThreadLocalMap map = getMap(t);
    if(map != null){
        //ThreadLocalMap的key就是当前ThreadLocal对象实例，
        //多个ThreadLocal变量都是放在这个map中
        ThreadLocalMap.Entry e = map.getEntry(this);
        if(e != null){
            //从map 中取出来的值就是我们需要的这个ThreadLocal变量
            T result = (T)e.value;
            return result;
        }
    }
    //如果map没有初始化
    return setInitialValue();
}
```

可以看到，所谓的ThreadLocal变量就是保存在每个线程的map中的。这个map就是Thread对象中的threadLocals字段。如下：

```java
ThreadLocal.ThreadLocalMap threadLocals = null;
```

ThreadLocal.ThreadLocalMap是一个比较特殊的Map，它的每个Entry的key都是一个弱引用：

```java
static class Entry extends WeakReference<ThreadLocal<?>> {
    /** The value associated with this ThreadLocal. */
    Object value;
    //key就是一个弱引用
    Entry(ThreadLocal<?> k, Object v) {
        super(k);
        value = v;
    }
}
```

#### 内存泄漏的原因

严格来说，ThreadLocal没有内存泄漏问题。有的话，那就是你忘记执行remove方法。这是不正确使用引起的。

如果你不调用remove方法的话，ThreadLocal所对应的值，就会存在，一直到当前线程的销毁。

众所周知，线程的生命周期都比较长，加上现在普遍使用的线程池，会让线程的生命更加长。不remove，当然不会释放。这和Key，到底是不是弱引用，关系不大。

#### 使用场景

- ThreadLocal **用作保存每个线程独享的对象**，为每个线程都创建一个**副本**，这样每个线程都可以修改自己所拥有的副本, 而不会影响其他线程的副本，确保了线程安全。

- **用作每个线程内需要独立保存信息**，以便供其他方法更方便地获取该信息的场景。每个线程获取到的信息可能都是不一样的，前面执行的方法保存了信息后，后续方法可以通过 ThreadLocal 直接获取到，**避免了传参，类似于全局变量的概念**。

代码演示：

```java
public class LocalContext{
    //初始化ThreadLocal 将用户信息装进Local
    private static final ThreadLocal<Node> LOCAL = new InheritbaleThreadLocal<Node>(){
        @Override
        protected Node initialValue(){
            return new Node();
        }
    };
    /**
    将userID装进本地缓存
    **/
    public static void setUserId(Integer userId){
        LOCAL.get().setUserId(userId);
    }
    
    /**获取用户
    **/
    public static Integer getUserId(){
        return LOCAL.get().getUserId();
    }
    //清除本地变量
    public static void remove(){
        LOCAL.remove();
    }
    
    //定义用户
    @Data
    public static class Node{
        private Integer userId;
    	private String openId;
    	private String mobile;
    	private String appId;

    	public Node(){
      		this.userId = 0;
     		this.openId = "";
      		this.mobile = "";
      		this.appId = "";
    }    
}
```

## 9. Redis

### 9.1 五种基本数据类型

#### 9.1.1 String 字符串

```
set number 520
get number
```

- 应用场景： 共享session、分布式锁、计数器、限流
- 内部编码：`int（8字节长整型）/embstr（小于等于39字节字符串）/raw（大于39个字节字符串）`

##### SDS(simple dynamic string)

```c++
struct sdshdr{
    //记录buf数组已使用字节的数量
    //等于SDS锁保存字符串的长度
    int len;
    //记录buf数组未使用字节的数量
    int free;
    //字节数组，用于保存字符串
    char buf[];
}
```

![image-20211115190014649](E:\实习\Java_docs\pics\image-20211115190014649.png)

**SDS的好处**

- **常数复杂度获取字符串长度**
- 杜绝缓冲区溢出

==API会先检查SDS的空间是否满足修改所需的要求，如果不满足的话，会自动将SDS的空间扩展至实行修改所需的大小。==

- 减少修改字符串时带来的内存重分配次数

#### 9.1.2 Hash 哈希

- 哈希类型是指v（值）本身又是一个键值对（k-v)结构
- 举例： `hset key field value`,`hget key field`
- 内部编码： `ziplist(压缩列表)`,`hashtable(哈希表)`
- 应用场景： 缓存用户信息

#### 9.1.3 List 链表

简介：列表（list）类型是用来存储多个有序的字符串，一个列表最多可以存储2^32-1个元素。

简单实用举例：` lpush  key  value [value ...]` 、`lrange key start end`

内部编码：ziplist（压缩列表）、linkedlist（链表）

应用场景： 消息队列，文章列表

- lpush+lpop=Stack（栈）
- lpush+rpop=Queue（队列）
- lpsh+ltrim=Capped Collection（有限集合）
- lpush+brpop=Message Queue（消息队列）

```c++
typedef struct listNode{
    //前置节点
    struct listNode *prev;
    //后置节点
    struct listNode *next;
    void *value;
}listNode;
```

![image-20211115191406712](E:\实习\Java_docs\pics\image-20211115191406712.png)

- 链表被用于实现Redis的各种功能，比如列表键、发布与订阅、慢查询、监视器等。

#### 9.1.4 Set（集合）

![image-20211117201357931](E:\实习\Java_docs\pics\image-20211117201357931.png)

- 简介：集合（set）类型也是用来保存多个的字符串元素，但是不允许重复元素
- 简单使用举例：`sadd key element [element ...]`、`smembers key`
- 内部编码：`intset（整数集合）`、`hashtable（哈希表）`
- 应用场景： 用户标签、生成随机数抽奖、社交需求。

#### 9.1.5 Zset 有序集合

简介：已排序的字符串集合，同时元素不能重复

简单格式举例：`zadd key score member [score member ...]`，`zrank key member`

底层内部编码：`ziplist（压缩列表）`、`skiplist（跳跃表）`

应用场景：排行榜，社交需求（如用户点赞）。

### 9.2 数据结构

#### SDS

#### 字典

用于保存键值对（key-value pair)

```
set msg 'hello world'
//在数据库中创建一个键为"msg",值为"hello world"
```

当一个哈希键包含的键值对比较多，又或者键值对中的元素都是比较长的字符串时，Redis就会使用字典作为哈希键的底层实现。

==字典使用哈希表作为底层实现，一个哈希表里面又多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。==

**哈希表**

```c++
typedef struct dictht{
    //哈希表数组
    dictEntry **table;
    //哈希表大小
    unsigned long size;
    //哈希表大小掩码，用于计算索引值
    //总是等于size - 1
    unsigned long sizemask;
    //已有节点数量
    unsigned long used;
}dictht;
```

**哈希表节点**

```c++
typedef struct dictEntry{
    //键
    void *key;
    //值
    union{
        void *val;
        uint64_tu64;
        int64_ts64;
    }v;
    //指向下一个哈希表节点，形成链表
    struct dictEntry *next;
}dictEntry;
```

![image-20211115192436628](E:\实习\Java_docs\pics\image-20211115192436628.png)

**字典实现**

```c++
typedef struct dict{
    //类型指定
    dictType * type;
    //私有属性
    void * privdata;
    //哈希表
    dictht ht[2];
    //rehash索引
    //当不在rehash时，值为-1
    int trehashidx;
}dict;
```

- ht属性是一个包含两个项的数组，数组中的每个项都是一个dictht哈希表，一般情况下，字典只使用ht [0]哈希表,ht [1]哈希表只会在对ht[0]哈希表进行rehash时使用。
- 除了ht[1]之外，另一个和rehash有关的属性就是rehashidx，它记录了rehash目前的进度，如果目前没有在进行rehash，那么它的值为-1。

##### Rehash过程

1. 为字典的ht[1]分配空间，取决于ht[0].used属性的值

- 如果执行的是扩展操作，那么ht[1]的大小为第一个大于等于ht[0].used * 2的 2^n
- 如果执行的收缩操作，那么ht[1]的大小为第一个大于等于ht[0].used 的2^n

2. 将保存在ht[0]中的所有键值对rehash到ht[1]上面：rehash是指重新计算哈希值和索引值。
3. 当ht[0]包含的所有键值对都迁移到了ht[1]之后（ht[0]为空表时), 释放ht[0]， 将ht[1]设置为ht[0]，并且ht[1]新创建一个空白哈希表。

==扩展和收缩的条件==

当以下条件中的任意一个被满足时，程序会自动开始对哈希表执行扩展操作：

1. 服务器目前没有执行BGSAVE命令或者BGREWRITEAOF命令，并且哈希表的**负载因子大于等于1**
2. 服务器目前执行BGSAVE命令或者BGREWRITEAOF命令，并且哈希表的**负载因子大于等于5.**

收缩： **负载因子小于0.1**



#### 跳跃表

![image-20211117201845992](E:\实习\Java_docs\pics\image-20211117201845992.png)

### 9.3 持久化之RDB、AOF

#### RDB快照持久化

RDB持久化是通过**快照**的方式，即在指定的时间间隔内将内存中的数据集快照写入磁盘。在创建快照之后，用户可以备份该快照，可以将快照复制到其他服务器以创建相同数据的服务器副本，或者在重启服务器后恢复数据。RDB是Redis**默认的持久化方式**

RDB文件默认为当前工作目录下的`dump.rdb`，可以根据配置文件中的`dbfilename`和`dir`设置RDB的文件名和文件位置

==触发快照的时机==

- 执行save和bgsave命令
- 配置文件设置`save<seconds><changes>`规则，自动间隔性执行bgsave命令
- 主从复制时，从库全量复制同步主库数据，主库会执行bgsave
- 执行flushall命令清空服务器数据
- 执行shutdown命令关闭Redis时，会执行save命令



#### save和bgsave命令

执行save和bgsave命令，可以手动触发快照，生成RDB文件

使用save命令会**阻塞Redis服务器进程**，服务器进程在RDB文件创建完成之前是不能处理任何请求

而bgsave命令，会**fork**一个子进程，然后该子进程会负责创建RDB文件。

```c++
127.0.0.1:6379>bgsave
Background saving started
```

`fork`一个子进程，子进程会把数据集先写入临时文件，写入成功之后，再替换之前的RDB文件，用二进制压缩存储，这样可以保证RDB文件始终存储的是完整的持久化内容

#### AOF持久化

AOF持久化会把被执行的写命令写到AOF文件的末尾，记录数据的变化。默认情况下，Redis是没有开启AOF持久化的，开启后，每执行一条更改Redis数据的命令，都会把该命令追加到AOF文件中，

==AOF需要记录Redis的每个写命令，步骤为：命令追加（append）、文件写入（write）和文件同步（sync）==

**命令追加**：

开启AOF持久化功能后，服务器每执行一个写命令，都会把该命令以协议格式先追加到`aof_buf`缓存区的末尾，而不是直接写入文件，减少硬盘IO次数。

**文件写入(write)和文件同步(sync)**

对于何时把`aof_buf`缓冲区的内容写入保存在AOF文件中，Redis提供了多种策略

- `appenfsync always`: 将`aof_buf`缓冲区的所有内容写入并同步到AOF文件，每个写命令同步写入磁盘
- `appendfsync everysec`: 将`aof_buf`缓冲区的内容写入AOF文件，每秒同步一次，该操作由一个线程单独完成。
- `appendfsync no`：将`aof_buf`缓存区的内容写入AOF文件，什么时候同步由操作系统来决定

`appendfsync`选项的默认配置为`everysec`，即每秒执行一次同步

#### AOF重写（rewrite）

```sql
127.0.0.1:6379> set s1 hello
OK
127.0.0.1:6379> set s2 world
OK
```

==appendonly.aof==

```
*3
$3
set
$2
s1
$5
hello
*3
$3
set
$2
s2
$5
world
```

该命令格式为Redis的序列化协议（RESP）。`*3`代表这个命令有三个参数，`$3`表示该参数长度为3

AOF重写的目的就是减小AOF文件的体积，不过值得注意的是：**AOF文件重写并不需要对现有的AOF文件进行任何读取、分享和写入操作，而是通过读取服务器当前的数据库状态来实现的**

文件重写可分为手动触发和自动触发，手动触发执行`bgrewriteaof`命令，该命令的执行跟`bgsave`触发快照时类似的，都是先`fork`一个子进程做具体的工作

自动触发会根据`auto-aof-rewrite-percentage`和`auto-aof-rewrite-min-size 64mb`配置来自动执行`bgrewriteaof`命令

- 重写会有大量的写入操作，所以服务器进程会`fork`一个子进程来创建一个新的AOF文件

- 在重写期间，服务器进程继续处理命令请求，如果有写入的命令，追加到`aof_buf`的同时，还会追加到`aof_rewrite_buf`AOF重写缓冲区

- 当子进程完成重写之后，会给父进程一个信号，然后父进程会把AOF重写缓冲区的内容写进新的AOF临时文件中，再对新的AOF文件改名完成替换，这样可以保证新的AOF文件与当前数据库数据的一致性

#### RDB和AOF优缺点

**RDB**

优点：

- RDB快照是一个压缩过的非常紧凑的文件，保存着某个时间点的数据集，适合做数据的备份，灾难恢复
- 可以最大化Redis性能，在保存RDB文件，服务器进程只需fork一个子进程来完成RDB文件的创建，父进程不需要做IO操作
- 与AOF相比，恢复大数据集的时候会更快

缺点：

- RDB的数据安全性不如AOF，保存整个数据集的过程比较繁重，如果服务器宕机，可能丢失几分钟的数据
- Redis数据集比较大时，fork的子进程会比较消耗CPU、耗时



**AOF**

优点：

- 数据更完整，安全性更高，（取决于fsync策略）

- AOF文件只是一个追加的日志文件，内容是可读的，适合误删紧急恢复

缺点：

- 对于相同的数据集，AOF文件体积要大于RDB文件

- 根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB。 不过在一般情况下， 每秒 fsync 的性能依然非常高

### 9.4 内存回收

1. 在Redis中，set指令可以指定key的过期时间，当过期时间到达后，key就失效了
2. Redis是基于内存操作的，所有数据都是保存在内存中的。

#### 9.4.1 内存回收机制

Redis的内存回收主要分为过期删除策略和内存淘汰策略两部分。

##### 过期删除策略

删除达到过期时间的key

**1.定时删除**

对于每一个设置了过期时间的key都会创建一个定时器，一旦到达过期时间就会立即删除。该策略可以立即清除过期的数据，对内存较友好，但是缺点是占用了大量的CPU资源去处理过期的数据，会影响Redis的吞吐量和响应时间。

**2.惰性删除**

当访问一个key时，才判断该key是否过期，过期则删除。该策略能最大限度地节省CPU资源。有一种极端的情况是可能出现大量的过期key没有被再次访问，因为不会被清除，导致占用了大量的内存。

**3.定期删除**

每隔一段时间，扫描Redis中过期key字典，并且清除部分过期的key。这是**折中方案。**

*redisDb结构体定义*

```c++
typedef struct redisDb{
    dict *dict; 	//键空间，保存所有键值对
    dict *expires;	//保存所有过期的key
    dict * blocking_keys;	
    dict *ready_keys;
    dict *watched_keys;
    int id; 	//数据库ID字段，代表不同的数据库
    long long avg_ttl; 
}
```

**expires属性**

它的类型也是字典，Redis会把所有的过期键值对加入到expires，之后再通过定期删除清理expires里面的值，加入expires的场景有：

1. set指定过期时间expire

如果设置key的时候指定了过期时间，Redis会将这个key直接加入到expires字典中并将超时时间设置到该字典元素。

2. 调用expire命令

显式指定某个key的过期时间

3. 恢复或者修改数据

从Redis持久化文件中恢复文件或者修改key，如果数据中的key已经设置过期时间，那么就将他加入expires

**Redis清理过期key的时机**

1. Redis在启动的时候，会注册两种事件，一种是时间事件，另一种是文件事件。

在时间事件中，redis注册的回调函数是`serverCron`，在定时任务回调函数中，通过调用databasesCron清理部分过期key（*定期删除的实现*）

```c++
int serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData)
{
    databasesCron();
}
```

2. 每次访问key的时候，都会调用`expireIfNeeded`函数判断key是否过期，如果是，清理key*（惰性删除的实现）*

```c++
robj *lookupKeyRead(redisDb *db, robj *key){
    robj *val;
    expireIfNeeded(db, key);
    val = lookupKey(db, key);
    
    return val;
}
```

**删除key**

Redis4.0以前，使用`del`指令删除，del会直接释放对象的内存。

Redis4.0版本引入了`unlink`指令，能对删除操作进行‘懒’处理，将删除操作丢给后台线程，由后台线程来异步回收内存。

==实际上，在判断key需要过期之后，真正删除key 的过程是先广播expire事件到从库和AOF文件中，然后根据Redis的配置决定立即删除还是异步删除。==

如果是立即删除，Redis会立即释放key和value占用的的内存空间，否则，Redis会在另一个bio线程中释放需要延迟删除的空间。

##### 内存淘汰策略

是指内存达到**maxmemory**极限时，使用某种算法来决定清理掉哪些数据，以保证新数据的存入。

- noeviction：当内存不足以容纳新写入数据时，新写入操作会报错
- allkeys-lru：当内存不足以容纳新写入数据时，在键空间`(server.db[i].dict)`中，移除最近最少使用的key**（最常用）**

- allkeys-random：当内存不足以容纳新写入数据时，在键空间（`server.db[i].dict`）中，随机移除某个 key。

- volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（`server.db[i].expires`）中，移除最近最少使用的 key。

- volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（`server.db[i].expires`）中，随机移除某个 key。

- volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（`server.db[i].expires`）中，有更早过期时间的 key 优先移除。

==在配置文件中，通过maxmemory-policy可以配置要使用哪一个淘汰机制。==

```c++
int processCommand(client *c)
{
    ...
    if (server.maxmemory) {
        int retval = freeMemoryIfNeeded();  
    }
    ...
}
```

###### LRU实现原理

在淘汰key时，Redis默认最常用的是LRU算法（Latest Recently Used）。Redis通过在每一个redisObject保存lru属性来保存key最近的访问时间，在实现LRU算法时直接读取key的lru属性。

具体实现时，Redis遍历每一个db，从每一个db中随机抽取一批样本key，默认是3个key，再从这3个key中，删除最近最少使用的key。

### 9.5 三大缓存问题

#### 9.5.1 缓存穿透

缓存穿透是指查询一条数据库和缓存都没有的一条数据，就会一直查询数据库，对数据库的访问压力就会增大。

解决方案：

1. 缓存空对象：代码维护比较简单，但是效果不好
2. 布隆过滤器：代码维护复杂，但是效果很好

##### 缓存空对象

缓存空对象是指当一个请求过来缓存和数据库中都不存在该请求的数据，第一次请求就会掉过缓存进行数据库的访问，并且访问数据库后返回为空，此时也将该对象进行缓存。

若是再次进行访问该空对象的时候，就会直接**击中缓存**，而不是再次**数据库**，缓存空对象实现的原理图如下：

![image-20211116213555543](E:\实习\Java_docs\pics\image-20211116213555543.png)

缓存空对象的实现代码很简单，但是缓存空对象会带来比较大的问题，就是缓存中会存在很多空对象，占用**内存的空间**，浪费资源，一个解决的办法就是设置空对象的**较短的过期时间**

##### 布隆过滤器

布隆过滤器是一种基于**概率**的**数据结构**，主要用来判断某个元素是否在集合内，它具有**运行速度快**（时间效率），**占用内存小**的优点（空间效率），但是有一定的**误识别率**和**删除困难**的问题。它只能告诉你某个元素一定不在集合内或可能在集合内。

在实际项目中会启动一个**系统任务**或者**定时任务**，来初始化布隆过滤器，将热点查询数据的id放进布隆过滤器里面，当用户再次请求的时候，使用布隆过滤器进行判断，该订单的id是否在布隆过滤器中存在，不存在直接返回null，具体操作代码

#### 9.5.2 缓存击穿

缓存击穿是指一个key非常热点，在不停的扛着大并发，**大并发**集中对这一个点进行访问，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，瞬间对数据库的访问压力增大。

缓存击穿这里强调的是**并发**，造成缓存击穿的原因有两个：

1. 该数据没有人查询过，第一次就大并发的访问
2. 添加到了缓存，但是**失效**了，大并发访问

对于缓存击穿的解决方案是加锁：

当用户出现**大并发**访问的时候，在查询缓存的时候和查询数据库的过程加锁，只能第一个进来的请求进行执行，当第一个请求把该数据放进缓存中，接下来的访问就会直接集中缓存，防止了**缓存击穿**。

业界比价普遍的一种做法，即根据key获取value值为空时，锁上，从数据库中`load`数据后再释放锁。若其它线程获取锁失败，则等待一段时间后重试。

以一个获取商品库存的案例进行代码的演示，**单机版**的锁实现具体实现的代码如下：

```java
//获取库存数量
public String getProduceNum(String key){
    try{
        synchronized(this){
            //加锁
            //缓存中取数据，并存入缓存中
            int num = Integer.parseInt(redisTemplate.opsForValue().get(key));
            
            if(num &gt; 0){
                //每查一次库存 - 1
                redisTemplate.opsForValue().set(key, (num - 1)+"");
                System.out.println("剩余库存为" + (num - 1));
            }
            else{
                System.out.println("库存为0");
            }
            
        }
    }
}
```

#### 9.5.3 缓存雪崩

缓存雪崩是指在某一个时间段，缓存集中过期失效，刺客无数的请求直接绕开缓存，直接请求数据库。

造成缓存雪崩的原因，有两点：

1. redis宕机
2. 大部分数据失效

比如天猫双11，马上就要到双11零点，很快就会迎来一波抢购，这波商品在23点集中的放入了缓存，假设缓存一个小时，那么到了凌晨24点的时候，这批商品的缓存就都过期了。

而对这批商品的访问查询，都落到了数据库上，对于数据库而言，就会产生周期性的压力波峰，对数据库造成压力，甚至压垮数据库。

**对于缓存雪崩的解决方案有以下两种**：

1. 搭建高可用的集群，防止单机的redis宕机。
2. 设置不同的过期时间，防止同意之间内大量的key失效。

### 9.6 数据一致性

#### KV、DB读写模式

缓存+数据库读写模式，就是**Cache Aside Pattern**

- 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回相应。
- 更新的时候，先**更新数据库，然后再删除缓存。**

==为什么是删除缓存，而不是更新缓存==

很多时候，在复杂的缓存场景，缓存不单单是数据库中直接取出来的值。

比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。

### 9.7 Redis线程模型

Redis内部使用文件事件处理器`file event handler`，这个文件事件处理器是单线程的，所以Redis才叫做单线程的模型。它采用IO多路复用机制同时监听多个**Socket**，根据Socket上的事件来选择对应的事件处理器进行处理。

文件事件处理器的结构包含4个部分：

- 多个Socket
- IO多路复用程序
- 文件事件派发器
- 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）

### 9.8 什么是热Key问题，如何解决热Key问题

在Redis中，我们把访问频率高的Key，称为热点key

如果某一热点key的请求到服务器主机时，由于请求量特别大，可能会导致主机资源不足，甚至宕机，从而影响正常的服务。

**热点key的产生**

- 用户消费的数据远大于生产的数据，如秒杀、热点新闻等读多写少的场景。
- 请求分片集中，超过单Redis服务器的性能，比如固定名称key，Hash落入同一台服务器。

**如何解决热Key问题**

- Redis集群扩容：增加分片副本，均衡读流程。

- 将热key分散到不同的服务器中；
- 使用二级缓存，即JVM本地缓存,减少Redis的读请求。

### 9.9 集群方案

#### 9.9.1 主从复制模式

![image-20211118230939850](E:\实习\Java_docs\pics\image-20211118230939850.png)

`Redis`主从复制，主从库模式一个`Master`主节点多`Slave`从节点的模式，将一份数据保存在多`Slave`个实例中，**增加副本冗余量**，当某些出现宕机后，`Redis`服务还可以使用。

但是这会存在数据不一致问题，那redis的副本集是如何数据一致性？

==`Redis`为了保证数据副本的一致，主从库之间采用读写分离的方式==：

- **读操作：主库、从库都可以执行处理；**
- **写操作：先在主库执行，再由主库将写操作同步给从库。**

使用读写分离方式的好处，可以避免当主从库都可以处理写操作时，主从库处理写操作加锁等一系列巨额的开销。

主从库是同步数据方式有两种：

##### **全量同步**

- 当一次从库启动时，从库给主库发送`psync`命令进行数据同步（`psync` 命令包含：主库的 `runID` 和复制进度 `offset` 两个参数）
- 当主库接收到ysync命令后将会保存RDB文件并发送给从库，发送期间会使用缓存区`（replication buffer)`记录后续的所有写操作，从库收到数据后，会先清空当前数据库，然后加载从主库获取的RDB文件。
- 当主库完成RDB文件发送后，也会把将保存发送RDB文件期间期间写操作的`replication buffer`发送给从库，从库再重新执行这些操作，这样主从库就实现同步了。

另外，为了分担主库生成 RDB 文件和传输 RDB 文件压力，提高效率，可以使用 **“主 - 从 - 从”模式将主库生成 RDB 和传输 RDB 的压力，以级联的方式分散到从库上。**

<img src="E:\实习\Java_docs\pics\image-20211118232056333.png" alt="image-20211118232056333" style="zoom:67%;" />

##### 增量同步

增量同步，基于环形缓冲区`repl_backlog_buffer`缓存区实现

在环形缓冲区，主库会记录自己写到的位置`master_repl_offset`，从库会记录自己已经读到的位置`slave_repl_offset`,主库并通过`master_repl_offset`和`slave_repl_offset`的差值的数据同步到从库。

==主从库间网络断了，主从库会采用增量复制的方式继续同步==，主库会把断连期间收到的写操作命令，写入`replication_buffer`，同时也会把这些操作命令也写入 `repl_backlog_buffer` 这个缓冲区，然后主库并通过`master_repl_offset` 和 `slave_repl_offset`的差值数据同步到从库。

因为`repl_backlog_buffer` 是一个环形缓冲区，当在缓冲区写满后，主库会继续写入，此时，会出现什么情况呢？

覆盖掉之前写入的操作。如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致。因此需要关注 `repl_backlog_size`参数，调整合适的缓冲空间大小，避免数据覆盖，主从数据不一致。

#### 9.9.2 Sentinel 哨兵模式

哨兵机制是实现主从库自动切换的关键机制，其主要分为三个阶段:

- 监控：哨兵进程会周期性地给所有的主从库发送PING命令，检测它们是否仍然在线运行。
- 选主（选择主库）：主库挂了以后，哨兵基于一定规则评分选举出来一个从库实例新的主库。
- 通知：哨兵会将新主库的信息发送给其他从库，让它们和新主库建立连接，并进行数据复制。**同时，哨兵会把新主库的信息广播通知给客户端，让它们把请求操作发到新主库上。**

![image-20211118232900067](E:\实习\Java_docs\pics\image-20211118232900067.png)

**其中，在监控中如何判断主库是否处于下线状态？**

- **主观下线**： **哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况，用来判断实例的状态，** 如果单哨兵发现主库或从库对 PING 命令的响应超时了，那么，哨兵就会先把它标记为“主观下线”
- 客观下线：在哨兵集群中，基于少数服从多数，多数实例都判定主库已“主观下线”，则认为主库“客观下线”。

==哨兵之间时如何互相通信的呢==

哨兵集群中哨兵实例之间可以相互发现，**基于Redis提供的发布/订阅机制（pub/sub机制）**

哨兵可以在主库中发布/订阅消息，在主库上有一个名为`\_sentinel_:hello`的频道，不同哨兵就是通过它来相互发现，实现互相通信，而且只有订阅了同一个频道的应用，才能通过发布的消息进行信息交换。

==如何选举新的主库==

- 从库的当前在线状态
- 判断它之前的网络连接状态，通过`down-after-milliseconds * num`（断开连接次数），当断开连接次数超过阈值，不适合作为新的主库。

==如何选举leader哨兵==

基于少数服从多数原则“投票仲裁”选举出来

- 当任何一个从库判定主库“主观下线”后，发送命令`s-master-down-by-addr`命令发送想要称为leader的信号。
- 其他哨兵根据于主机连接情况作出相应的相应，而且如果有多个哨兵发起请求，每个哨兵的赞成票只能投给其中一个，其他只能为反对票。

想要成为Leader 的哨兵，要满足两个条件：

- 第一，获得半数以上的赞成票；
- 第二，获得的票数同时还需要大于等于哨兵配置文件中的`quorum`值。

==选举完leader哨兵并新主库切换完毕之后，那么leader哨兵怎么通知客户端？==

基于哨兵自身的 pub/sub 功能，实现了客户端和哨兵之间的事件通知，客户端订阅哨兵自身消息频道 

##### 数据问题

###### 数据丢失-主从异步复制

因为`master` 将数据复制给`slave`是异步实现的，在复制过程中，这可能存在master有部分数据还没复制到slave，master就宕机了，此时这些部分数据就丢失了。

###### 数据丢失-脑裂

何为脑裂？当一个集群中的 master 恰好网络故障，导致与 sentinal 通信不上了，sentinal会认为master下线，且sentinal选举出一个slave 作为新的 master，此时就存在两个 master了。

可能存在client还没来得及切换到新的master，还继续写向旧master的数据，当master再次恢复的时候，会被作为一个slave挂到新的master 上去，自己的数据将会清空，重新从新的master 复制数据，这样就会导致数据缺失。

**总结：主库的数据还没有同步到从库，结果主库发生了故障，等从库升级为主库后，未同步的数据就丢失了。**

#### 数据丢失解决方案

数据丢失可以通过合理地配置参数 min-slaves-to-write 和 min-slaves-max-lag 解决，比如

- `min-slaves-to-write` 1
- `min-slaves-max-lag` 10

如上两个配置：要求至少有 1 个 slave，数据复制和同步的延迟不能超过 10 秒，如果超过 1 个 slave，数据复制和同步的延迟都超过了 10 秒钟，那么这个时候，master 就不会再接收任何请求了。

###### 数据不一致

在主从异步复制过程，当从库因为网络延迟或执行复杂度高命令阻塞导致滞后执行同步命令，这样就会导致数据不一致

解决方案： 可以开发一个外部程序来监控主从库间的复制进度（`master_repl_offset` 和 `slave_repl_offset` ），通过监控 `master_repl_offset` 与`slave_repl_offset`差值得知复制进度，当复制进度不符合预期设置的Client不再从该从库读取数据。


作者：Ccww
链接：https://juejin.cn/post/6920457759393742862
来源：稀土掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

### 9.10 Redis分布式锁 *****************

[redis系列：基于redis的分布式锁 - 云枭zd - 博客园 (cnblogs.com)](https://www.cnblogs.com/fixzd/p/9479970.html)

#### **最基础的版本1**

假设有两个客户端A和B，A获取到分布式的锁。A执行了一会，突然A所在的服务器断电了（或者其他什么的），也就是客户端A挂了。这时出现一个问题，这个锁一直存在，且不会被释放，其他客户端永远获取不到锁。

![image-20211118202121288](E:\实习\Java_docs\pics\image-20211118202121288.png)

#### **设置锁的过期时间**

1. 客户端A获取锁成功，过期时间30秒。
2. 客户端A在某个操作上阻塞了50秒。
3. 30秒时间到了，锁自动释放了。
4. 客户端B获取到了对应同一个资源的锁。
5. 客户端A从阻塞中恢复过来，释放掉了客户端B持有的锁。

![image-20211118202316538](E:\实习\Java_docs\pics\image-20211118202316538.png)

==这时会有两个问题==

1. 过期时间如何保证大于业务执行时间?
2. 如何保证锁不会被误删除?

#### **设置锁的value**

#### **具有原子性的释放锁**

#### **确保过期时间大于业务执行时间**





### 9.11 Redis为什么这么快

- **基于内存**：Redis是使用内存存储的，没有磁盘IO上的开销，数据存在内存中，读写速度快。
- **单线程实现**（Redis6.0以前）：Redis使用单个线程处理请求，避免了多个线程之间线程切换和锁资源争用的开销。
- **IO多路复用模型**： Redis采用IO多路复用技术，Redis使用单线程来轮询描述符，将数据库的操作都转换成了事件，不在网络I/O上浪费太多的事件
- **高效的数据结构**： Redis为每种数据结构底层都做了优化，目的就是为了追求更快的速度。

### 9.12 布谷鸟过滤器

#### 布谷鸟哈希

两个不同的hash算法将新来的元素映射到数组的两个位置。如果两个位置中有一个位置为空，那么就可以直接将元素放进去。**但是**如果两个位置都满了， ==随机踢走一个==，然后自己霸占这个位置。

```java
p1 = hash1(x) % l
p2 = hash2(x) % l
```

被挤走的那个元素去查看自己的另一个位置，如果为空，就自己挪过去。如果这个位置还被别人占了，那就再来一次【鸠占鹊巢】。

布谷鸟哈希会设置一个阈值，当连续占巢行为超出了某个阈值，就认为这个数组已经几乎满了。这时候就需要对它进行扩容，重新放置所有元素。

**挤兑循环**。比如两个不同的元素，hash 之后的两个位置正好相同，这时候它们一人一个位置没有问题。但是这时候来了第三个元素，它 hash 之后的位置也和它们一样，很明显，这时候会出现挤兑的循环。

#### 优化

原始的布谷鸟哈希算法的平均空间利用率大概只有50%，改良的方案之一是增加hash函数，这样可以大大降低碰撞的概率。

另一种方案是在数组的每个位置上挂上多个多个座位，这样即使两个元素被hash在了同一个位置，也不必立即【鸠占鹊巢】，这种方案的空间利用率只有85%，但是查询效率会很高，同一个位置上的多个座位在内存空间上是连续的，可以有效利用 CPU 高速缓存。

#### 布谷鸟过滤器

它也是一维数组，但是布谷鸟哈希会存储整个元素，而布谷鸟过滤器中只会存储元素的指纹信息（几个bit，类似于布隆过滤器）。

首先布谷鸟过滤器还是只会选用两个hash函数，但是每个位置可以放置多个座位。这两个hash函数选择的比较特殊，因为过滤器只能存储指纹信息。当这个位置上的指纹被挤兑之后，它需要计算出另一个对偶位置。而计算这个对偶位置是需要元素本身的。

```java
fp = fingerprint(x)
p1 = hash1(x) % l
p2 = hash2(x) % l
//我们知道了p1和x的指纹，是没有办法直接算出p2的    
```

#### 特殊的hash函数

使得可以根据p1和元素指纹直接计算出p2，而不需要完整的x元素。

```java
fp = fingerprint(x)
p1 = hash(x)
p2 = p1 ^ hash(fp)  // 异或
p1 = p2 ^ hash(fp)  
```

**数据结构**

```java
type bucket [4]byte  // 一个桶，4个座位
type cuckoo_filter struct {
  buckets [size]bucket // 一维数组
  nums int  // 容纳的元素的个数
  kick_max  // 最大挤兑次数
}
```

```java
class Solution {
    public int integerBreak(int n) {
        int[] dp = new int[n + 1];
        for (int i = 2; i <= n; i++) {
            int curMax = 0;
            for (int j = 1; j < i; j++) {
                curMax = Math.max(curMax, Math.max(j * (i - j), j * dp[i - j]));
            }
            dp[i] = curMax;
        }
        return dp[n];
    }
}

```

